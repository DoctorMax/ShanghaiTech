\documentclass[english,onecolumn]{IEEEtran}
\usepackage[T1]{fontenc}
\usepackage[latin9]{luainputenc}
\usepackage[letterpaper]{geometry}
\geometry{verbose}
\usepackage{amsfonts}
\usepackage{babel}

\usepackage{extarrows}
\usepackage[colorlinks]{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage[ruled,linesnumbered]{algorithm2e}

\usepackage{amsmath,graphicx}
\usepackage{subfigure} 
\usepackage{cite}
\usepackage{amsthm,amssymb,amsfonts}
\usepackage{textcomp}
\usepackage{bm}
\usepackage{booktabs}
\usepackage{listings}
\definecolor{salmon}{rgb}{1, 0.5020, 0.4471}
\usepackage{xparse}

\NewDocumentCommand{\codeword}{v}{%
\texttt{\textcolor{blue}{#1}}%
}

\lstdefinestyle{mystyle}{
    numberstyle=\color{green},
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

\providecommand{\U}[1]{\protect\rule{.1in}{.1in}}
\topmargin            -18.0mm
\textheight           226.0mm
\oddsidemargin      -4.0mm
\textwidth            166.0mm
\def\baselinestretch{1.5}


\newcommand{\Rbb}{\mathbb{R}}
\newcommand{\Pb}{\mathbf{P}}
\newcommand{\Ib}{\mathbf{I}}
\newcommand{\vb}{\mathbf{v}}
\newcommand{\Ucal}{\mathcal{U}}
\newcommand{\Wcal}{\mathcal{W}}
\newcommand{\Vcal}{\mathcal{V}}
\newcommand{\Rcal}{\mathcal{R}}
\newcommand{\Ncal}{\mathcal{N}} 


\def\Q{\mathbf{Q}}
\def\A{\mathbf{A}}
\def\R{\mathbf{R}}
\def\I{\mathbf{I}}


\begin{document}

\begin{center}
	\textbf{\LARGE{SI231 - Matrix Computations, Fall 2020-21}}\\
	{\Large Homework Set \#3}\\
	\texttt{Prof. Yue Qiu and Prof. Ziping Zhao}\\
	\texttt{\textbf{Name:}}   	\texttt{ GongChenyu }  		\hspace{1bp}
	\texttt{\textbf{Major:}}  	\texttt{ Master in IE } 	\\
	\texttt{\textbf{Student No.:}} 	\texttt{ 2020232133}     \hspace{1bp}
	\texttt{\textbf{E-mail:}} 	\texttt{ gongchy@shanghaitech.edu.cn}
\par\end{center}



\noindent
\rule{\linewidth}{0.4pt}
{\bf {\large Acknowledgements:}}
\begin{enumerate}
    \item Deadline: \textbf{2020-11-01 23:59:00}
    \item Submit your homework at \textbf{Gradescope}. Entry Code: \textbf{MY3XBJ}. 
    Homework \#3 contains two parts, the theoretical part the and the programming part.
    \item About the the theoretical part:
    \begin{enumerate}
            \item[(a)] Submit your homework in \textbf{Homework 3} in gradescope. Make sure that you have correctly select pages for each problem. If not, you probably will get 0 point.
            \item[(b)] Your homework should be uploaded in the \textbf{PDF} format, and the naming format of the file is not specified.
            \item[(c)] You need to use \LaTeX $\,$ in principle.
            \item[(d)] Use the given template and give your solution in English. Solution in Chinese is not allowed. 
        \end{enumerate}
  \item About the programming part:
  \begin{enumerate}
      \item[(a)] Submit your codes in \textbf{Homework 3 Programming part} in gradescope.
      \item[(b)] Detailed requirements see in Problem 2 and Probelm 3.
  \end{enumerate}
  \item \textbf{No late submission is allowed.}
\end{enumerate}
\rule{\linewidth}{0.4pt}
\newpage 

\section{Understanding projection}
\noindent\textbf{Problem 1}. \textcolor{blue}{(5 points $\times$ 3)}

Suppose that $\Pb\in \Rbb^{n\times n}$ is a projector onto a subspace $\mathcal{U}$ along its orthogonal complement $\mathcal{U}^{\perp}$, then it is called the \textbf{orthogonal projector} onto $\Ucal$.
\begin{enumerate}
    \item Prove that an orthogonal projector must be singular if it is not an identity matrix.
	\item What is the orthogonal projector onto $\mathcal{U}^{\perp}$ along the subspace $\mathcal{U}$?
    \item Let $\Ucal$ and $\Wcal$ be two subspaces of a vector space $\mathcal{V}$, and denote $\Pb_{\Ucal}$ and $\Pb_{\Wcal}$ as the corresponding orthogonal projectors, respectively. Prove that $\Pb_{\Ucal} \Pb_{\Wcal} = 0$ if and only if $\Ucal \perp \Wcal$.
\end{enumerate}

\noindent
\textbf{Solution.}
Please insert your solution here ... 
\begin{enumerate}
    \item
    Set $ \bf{u} \in \mathcal{U}, \bf{w} \in \mathcal{U}^{\perp}, \bf{v = u + w} \in \mathbb{R}^n$.\\
    $\because \Pb\in \Rbb^{n\times n}$ is a projector onto a subspace $\mathcal{U}$ along its orthogonal complement $\mathcal{U}^{\perp}$\\
    $\therefore \bf{P^2 v= P \cdot P v = P u = u = P v }  $ \\ 
    $\therefore \bf{P^2 = P} \Rightarrow \bf{P} $ is a idempotent matrix. And the eigenvalue of $\bf{P}$ must be 0 or 1.\\
    $\therefore $  det($\bf{P}$) = 0 or det($\bf{P}$) = 1
    If det($\bf{P}$) = 1, in other words, $\bf{P^{-1}}$ exists.\\
    $\because \bf{P^2 = P}$, then left and right both multiply $\bf{P^{-1}}$, we can get $\bf{P^{2} P^{-1} = P P^{-1}}$ , $ \Rightarrow \bf{P = I}$.\\
    $\therefore$ If $\bf{P} $ is not $\bf{I}$, $\bf{P}$ must be singular.\\
    
    \item
    $\because \Pb\in \Rbb^{n\times n}$ is a orthogonal projector onto a subspace $\mathcal{U}$ along its orthogonal complement $\mathcal{U}^{\perp}$\\
    $\therefore$ for $\bf{x} \in \mathcal{U}$ and $\bf{y} \in \mathcal{U}^{\perp}$, $\bf{v = x + y} \in \mathbb{R}^n$, then $\bf{P v = x}$.\\
    Then $\bf{v = P v + y } \Rightarrow \bf{(I-P)v = y}$, so orthogonal projector onto a subspace $\mathcal{U}^{\perp}$ along its orthogonal complement $\mathcal{U}$ is $\bf{(I-P)}$.    \\
    
    \item
    Firstly, \\ 
    If $\mathcal{U} \perp \mathcal{W}, \bf{P_{\mathcal{W}} = I - P_{\mathcal{U}}}$, then $ \bf{P_{\mathcal{U}} P_{\mathcal{W}} = P_{\mathcal{U}} ( I - P_{\mathcal{U}}) = P_{\mathcal{U}} - P_{\mathcal{U}} P_{\mathcal{U}} }$,\\
    $\because \bf{P_{\mathcal{U}}}  $ is a idempotent matrix,\\
    $\therefore \bf{P_{\mathcal{U}} = P_{\mathcal{U}} P_{\mathcal{U}}} $\\
    so if $\mathcal{U} \perp \mathcal{W}$, $\bf{P_{\mathcal{U}} P_{\mathcal{W}}} = 0 $.\\
    Secondly, \\
    $\because \bf{P_{\mathcal{U}} P_{\mathcal{W}}} = 0,  $\\
    $\therefore $ all the column vectors of $\bf{P_{\mathcal{W}} }$ must belong to $\mathcal{N} (\bf{P_{\mathcal{U}
    }})$\\
    $\therefore \mathcal{R} (\bf{P_{\mathcal{W}}}) \subseteq \mathcal{N} (\bf{P_{\mathcal{U}}})$ \\
    $\because \mathcal{R}(\bf{P_{\mathcal{W}}}) = \mathcal{W}$ and $\mathcal{N}(\bf{P_{\mathcal{U}}}) = \mathcal{U}^{\perp}$\\
    $\therefore \bf{P_{\mathcal{U}} P_{\mathcal{W}}} = 0 \Rightarrow \mathcal{W} \subseteq \mathcal{U}^{\perp} \Rightarrow \mathcal{U} \perp \mathcal{W} $.
    
\end{enumerate}

\newpage
\section{Least Square (LS) programming.}
\noindent\textbf{Problem 2}. \textcolor{blue}{(10 points + 10 points + 5 points)}

Write programs to solve the least square problem with specified methods, any programming language is suitable.
$$
\mathbf{x} = \mathop{\arg\min}_{\mathbf{x} \in \Rbb^n} f(\mathbf{x}), \quad f(\mathbf{x}) = ||\mathbf{y}-\mathbf{A}\mathbf{x}||_2^2
$$
where $\mathbf{A} \in \Rbb^{m \times n}$ is a matrix representing the predefined data set with $m$ data samples of $n$ dimensions ($m$=1000, $n$=210), and $\mathbf{y} \in \Rbb^m$ represents the labels. The data samples are provided in the "data.txt" file, and the labels are provided in the "label.txt" file, you are supposed to load the data before solving the problem.

\begin{enumerate}
    \item Solve the LS with gradient decent method.\\
    The gradient descent method for solving problem updates $ {\bf x}$ as
    $$
        {\bf x} = {\bf x} - \gamma \cdot \nabla_{{\bf x}} f(\mathbf{x}),
    $$
    where $\gamma$ is the step size of the gradient decent methods. We suggest that you can set $\gamma=1e-5$.
    \item Solve the LS by the method of normal equation with Cholesky decomposition and forward/backward substitution.
    \item Compare two methods above. 
    \begin{enumerate}
        \item[(a)] Basing on the true running results from the program, count the number of "flops"*;
        \item[(b)] Compare gradient norm and loss $f(\mathbf{x})$ for results $\mathbf{x}=\mathbf{x_{LS}}$ of above two algorithms.
    \end{enumerate}
\end{enumerate}
    \textbf{Notation*:} "flop": one flop means one floating point operation, i.e., one addition, subtraction, multiplication, or division of two floating-point numbers, in this problem each floating points operation $+,-,\times, \div, \sqrt{\cdot}$ counts as one "flop". \\
    \textbf{Hint for gradient decent programming:} 
    \begin{enumerate}
        \item \textbf{Step size selection}: to ensure the convergence of the method, $\gamma$ is supposed to be selected properly (large step size may accelerate the convergence rate but also may lead to instability, A sufficiently small compensation always ensures that the algorithm converges). 
        \item \textbf{Terminal condition}: the gradient decent is an iteration algorithm that need a terminal condition. In this problem, the algorithm can stop when the gradient of the loss function $f(\mathbf{x})$ at current $\mathbf{x}$ is small enough.
    \end{enumerate}
    \noindent\textbf{Remarks: }
   \begin{itemize}
    \item The solution of the two methods should be printed in files named "sol1.txt" and "sol2.txt" and submitted in gradescope.  The format should be same as the input file (210 rows plain text, each rows is a dimension of the final solution).
    \item Make sure that your codes are executable and are consistent with your solutions.
   \end{itemize}
\noindent

\newpage

\textbf{Solution.}
Please insert your solution here ...\\

\begin{enumerate}
    \item 
    The code is in the file : 'GradientDescent.m' \\
    The keys are computing loss and updating $\bf{x}$ to $\bf{x_{LS}}$. \\ The loss is the distance between label '$\bf{y}$' and predict '$\bf{Ax}$':
    $$loss = ||\bf{y - Ax} ||^2_2$$
    We need a $\Delta \bf{x}$ to update $\bf{x}$ :
    $$\Delta \bf{x} = \nabla_x f(x) = 2 \times (\bf{A^{T} A x - A^T y}) $$ 
    $$ \bf{x} = \bf{x} - \lambda \Delta \bf{x}  $$ \\
    
    \item
    The code is in the file : 'NormalEquation.m'
    
    Solving LS via the normal equations
    $$ \mathbf{A}^{T} \mathbf{A} \mathbf{x}_{\mathrm{L} \mathrm{S}}=\mathbf{A}^{T} \mathbf{y} $$
    The solving process is below:\\
    compute the lower triangular portion of $\mathbf{C}=\mathbf{A}^{T} \mathbf{A}$\\
    form the matrix-vector product $\mathbf{d}=\mathbf{A}^{T} \mathbf{y}$\\
    compute the Cholesky factorization $\mathbf{C}=\mathbf{G}^{T} \mathbf{G}$\\
    solve $\mathbf{G^{T} z}=\mathbf{d}$ and $\mathbf{G} \mathbf{x}_{\mathrm{LS}}=\mathbf{z}$\\
    
    
    \item
    
    \begin{enumerate}
        \item
        count flops ($\bf{A} \in \mathcal{R}^{\bf{m} \times \bf{n}}$):\\
        \textbf{As for gradient decent}:\\
        set iteration = $epoch$, $N = $ length($\bf{y}$), for every iteration:\\
        compute $loss = ||\bf{y - Ax} ||^2_2 $ $ \Rightarrow $ $cost_1 = m \times (n+n-1)+m+m+m-1+1 = 2m(n+1)$ \\
        compute $\Delta \bf{x} =  2 \times (\bf{A^{T} A x - A^T y})$ $\Rightarrow$ $cost_2=n^2 (2m-1)+n(n+n-1)+n = 2m n^2+2m n +n^2-n$ \\ 
        update $\bf{x} = \bf{x} - \lambda \Delta \bf{x}$ $\Rightarrow$ $cost_3=n$ \\
        total flops = $epoch \times (cost_1 + cost_2 + cost_3) = epoch \times (2m n^2 + n^2 + 4m n +2m )$
        
        \textbf{As for normal equation}:\\
        \begin{align*}
            \mathbf{C}=\mathbf{A}^{T} \mathbf{A} & \Rightarrow \text{one output point need m times multiplication and m-1 times addition}  \\  & \Rightarrow cost_1 = n \times n \times (m+m-1) = n^2 (2m-1) \Rightarrow \mathcal{O} (m n^{2})
        \end{align*}
        \begin{align*}
            \mathbf{d}=\mathbf{A}^{T} \mathbf{y} & \Rightarrow cost_2 = n \times (m+m-1) = n (2m-1) \Rightarrow \mathcal{O}(m n)
        \end{align*}

        \begin{align*}
            \mathbf{C}=\mathbf{G}^{T} \mathbf{G} \Rightarrow
            cost_3 &= 1 \cdot n + 3(n-1) + 5(n-2) + \cdots +(2n-3)2+(2n-1) \cdot 1 \\
            & = \sum_i^n (2i-1)(n+1-i) = \sum_i^n [(n+1)(2i-1)-i(2i-1)]\\
            &= (n+1)(2\sum_i^n i -\sum_i^n 1) - (2\sum_i^n i^2 - \sum_i^n i)\\
            &= (n+1)(2\cdot \frac{n(n+1)}{2} - n) - (2 \cdot \frac{n(n+1)(2n+1)}{6} - \frac{n(n+1)}{2} )\\
            &=n(n+1)(\frac{2n+1}{6}) \Rightarrow \mathcal{O}\left(n^{3} / 3\right)
        \end{align*}
        
        $$\mathbf{G^{T} z}=\mathbf{d}\quad \text{and}\quad \mathbf{G} \mathbf{x}_{\mathrm{LS}}=\mathbf{z}  \Rightarrow  cost_4 = 2\cdot (1+3+5+\cdots +2n-1) = 2\sum_i^n (2i-1) = 2n^2 \Rightarrow \mathcal{O}\left(n^{2}\right) $$
        
        total flops = $cost_1+cost_2+cost_3+cost_4 = n^2(2m-1)+ n(2m-1)+\frac{n(n+1)(2n+1)}{6}+2n^2$\\
        complexity: $\mathcal{O}\left(m n^{2}+n^{3} / 3\right)$\\
        
        \item
        \textbf{gradient decent}:\\
        gradient norm: 6.8872e-09 \\
        loss:26.6426\\
        \textbf{normal equation}:\\
        gradient norm: 8.2084e-18 \\
        loss:26.6426\\
        
        In the condition of same loss, the gradient norm of normal equation is much smaller.


    \end{enumerate}
    
\end{enumerate}

\newpage
\section{Understanding the QR Factorization}
\noindent\textbf{Problem 3 [Understanding the Gram-Schmidt algorithm.]}. \textcolor{blue}{(5 points + 7 points + 6 points + 7 points)}
\begin{enumerate}
	\item 
	Consider the subspace $\mathcal{S}$ spaned by $\{ {\bf a}_1,\ldots, {\bf a}_4\}$, where
	\[
	{\bf a}_1 = \begin{bmatrix} 1 \\ 2 \\ 3 \\ 4\end{bmatrix}\,,\quad 
	{\bf a}_2 =  \begin{bmatrix}2 \\ 3 \\ 4 \\ 5 \end{bmatrix}\,,\quad 
	{\bf a}_3 =  \begin{bmatrix}3 \\ 4 \\5 \\ 6 \end{bmatrix}\,,\quad
	{\bf a}_4 =  \begin{bmatrix}3 \\ 5 \\7 \\ 11 \end{bmatrix}\,.
	\] 
	Use the \textbf{classical} Gram-Schimidt algorithm (See Algorithm \ref{alg:classical_gs}), find a set of orthonormal basis $\{{\bf q}_i\}$ for $\mathcal{S}$ by hand (derivation is expected). \textcolor{black}{
	Do not use decimals in your answers, fraction and $n$-th roots of numbers are accepted.}
	Verify the orthonormality of the found basis.
	\begin{algorithm}[htbp]
 \label{alg:classical_gs}
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\caption{Classical Gram-Schmidt algorithm}
\SetAlgoLined
\Input{A collection of linearly independent vectors ${\bf a}_1,\ldots, {\bf a}_n$.}
\textbf{Initilization:} $\widetilde{{\bf q}}_1 = {\bf a}_1, {\bf q}_1 = \widetilde{{\bf q}}_1/\|\widetilde{{\bf q}}_1\|_2$\\
 \For{$i= 2,\ldots, n$}{
  $\widetilde{{\bf q}}_i = {\bf a}_i - \sum_{j=1}^{i-1} ({\bf q}_j^T{\bf a}_i){\bf q}_j$\\
  ${\bf q}_i = \widetilde{{\bf q}}_i/\|\widetilde{{\bf q}}_i\|_2$ 
 }
 \Output{${\bf q}_1,\ldots, {\bf q}_n$}
\end{algorithm}
	\item 
	Orthogonal projection of vector ${\bf a}$ onto a nonzero vector ${\bf b}$ is defined as
	\[
	\text{proj}_\mathbf{b}(\bf a)=\frac{\langle{\bf a},{\bf b}\rangle}{\langle{\bf b},{\bf b}\rangle}{\bf b},
	\]
	where $\langle,\rangle$ denotes the inner product of vectors.
	And for subspace $\mathcal{M}$ with 
	orthonormal basis $\{ {\bf u}_1,\ldots, {\bf u}_k \}$, the orthogonal projector onto subspace $\mathcal{M}$ is given by 
	\[
	{\bf P} = {\bf UU}^T\,,\quad {\bf U} = [{\bf u}_1|\cdots|{\bf u}_k]\,.
	\]
	In the context of \textbf{projection of vector} and \textbf{projection onto subspace} respectively, can you give another two understandings of the classical Gram-Schmidt algorithm?
    %Try to understand Gram-Schmidt algorithm in the context of \textbf{projection onto subspace} and give a new expression of ${\bf q}_k$ based on your understanding.
	%It can b \|\,,\\_2e written as projection of subspace $\bf Pa$ and $\bf P$ is an orthogonal projector, where $\bf P$ is a projection matrix.
	\item Consider the subspace $\mathcal{S}$ spaned by $\{{\bf a}_1, {\bf a}_2, {\bf a}_3\}$,
	\[
	{\bf a}_1 = \begin{bmatrix} 1 \\ \epsilon \\ \epsilon \\ \end{bmatrix}\,,\quad 
	{\bf a}_2 =  \begin{bmatrix}1 \\ \epsilon \\ 0\end{bmatrix}\,,\quad 
	{\bf a}_3 =  \begin{bmatrix}1 \\ 0 \\ \epsilon\end{bmatrix}\,,
	\]
	where $\epsilon$ is a small real number such that $1+k\epsilon^2 =1$ $(k\in\mathbb{N}^+)$. 
	First complete the pseudo algorithm in Algorithm \ref{alg:modified_gs}.
	Then use the \textbf{classical} Gram-Schimidt algorithm and the \textbf{modified} Gram-Schimidt algorithm respectively, find two sets of basis for $\mathcal{S}$ by hand (derivation is expected). Are the two sets of basis the same? If not, which one is the desired orthonormal basis? Report what you have found.
	\begin{algorithm}[htbp]
	\label{alg:modified_gs}
\SetKwInOut{Input}{Input}\SetKwInOut{Output}{Output}
\caption{Modified Gram-Schmidt algorithm}
\SetAlgoLined
\Input{A collection of linearly independent vectors ${\bf a}_1,\ldots, {\bf a}_n$.}
\textbf{Initilization:$\bf{Q_1=0,R_1=0,A=[a_1,\cdots,a_n]}$}\\
\textbf{for $ i=1,\cdots,n$}\\
\textbf{$\quad\quad \bf{z} = \bf{A}(:,i)$}\\
\textbf{$\quad\quad \bf{R_1}(i,i) = ||\bf{z}||_2$}\\
\textbf{$\quad\quad \bf{Q_1}(:,i) = \bf{z/R_1}(i,i)$}\\
\textbf{$\quad\quad \bf{R_1}(i,i+1:n) = \bf{Q_1}(:,i)^T \bf{A}(:,i+1:n)$}\\
\textbf{$\quad\quad  \bf{A}(:,i+1:n) =  \bf{A}(:,i+1:n) - \bf{Q_1}(:,i)\bf{R_1}(i,i+1:n)$}\\
\textbf{end}\\
\textbf{$\bf{q_i} = \bf{Q_1}(:,i) $}\\

 \Output{${\bf q}_1,\ldots, {\bf q}_n$}
\end{algorithm}
	\item \textbf{Programming part:}
	In this part, you are required to code both the \textbf{ classical Gram-Schmidt} and \textbf{the modified Gram-Schmidt} algorithms.
	For $\epsilon=1\text{e}-4$ and $\epsilon=1\text{e}-9$ in sub-problem 2), give the outputs of two algorithms and calculate $\|{\bf Q}^T {\bf Q} - {\bf I}\|_{\text{F}}$, where ${\bf Q} = [{\bf q}_1,{\bf q}_2,{\bf q}_3]$.
\end{enumerate}
\noindent\textbf{Remarks: }
\begin{itemize}
    \item Coding languages are not restricted, but do not use built-in function such as \codeword{qr}.
    \item When handing in your homework in gradescope, package all your codes into {\sf your\_student\_id+hw3\_code.zip} and upload. In the package, you also need to include a file named {\sf README.txt/md} to clearly identify the function of each file.
     \item Make sure that your codes can run and are consistent with your solutions.
\end{itemize}

\noindent
\textbf{Solution.}
Please insert your solution here ...
\begin{enumerate}
    \item
    $$
    \begin{bmatrix}
    \bf{a_1} & \bf{a_2} & \bf{a_3} & \bf{a_4} \\ 
    \end{bmatrix}   = 
    \begin{bmatrix}
    1 & 2 & 3 & 3\\
    2 & 3 & 4 & 5\\
    3 & 4 & 5 & 7\\
    4 & 5 & 6 & 11
    \end{bmatrix}
    \Rightarrow
    \begin{bmatrix}
    1 & 1 & 1 & 0\\
    0 & 1 & 2 & 0\\
    0 & 0 & 0 & 1\\
    0 & 0 & 0 & 0
    \end{bmatrix}
    \Rightarrow
    \{\bf{a_1,a_2,a_4} \} \text{are linear independent.}
    $$
    $\bf{\hat{q}_1} = \bf{a_1} \Rightarrow \bf{q_1} = \bf{\hat{q}_1 / ||\hat{q}_1 ||_2} = \frac{1}{\sqrt{30}} (1,2,3,4)^T$\\
    $\bf{\hat{q}_2} = \bf{a_2} - (\bf{q^T_1 a_2})q_1 = (2,3,4,5)^T- \frac{40}{30} (1,2,3,4)^T =\frac{1}{3} (2,1,0,-1) \Rightarrow \bf{q_2} =\frac{1}{\sqrt{6}} (2,1,0,-1)^T$\\
    $\bf{\hat{q}_3} = \bf{a_4} - (\bf{q^T_1 a_4})q_1 -(\bf{q^T_2 a_4})q_2=\frac{1}{5}(2,-1,-4,3)^T\Rightarrow  \bf{q_3} = 1/\sqrt{30} (2,-1,-4,3)^T$\\
    $$
    \bf{Q} = (\bf{q_1,q_2,q_3}) = 
    \begin{bmatrix}
    \frac{\sqrt{30}}{30} & \frac{\sqrt{6}}{3} & \frac{\sqrt{30}}{15}\\
    \frac{\sqrt{30}}{15} & \frac{\sqrt{6}}{6} & -\frac{\sqrt{30}}{30}\\
    \frac{\sqrt{30}}{10} & 0 & -\frac{2\sqrt{30}}{15}\\
    \frac{2\sqrt{30}}{15} & -\frac{\sqrt{6}}{6} & \frac{\sqrt{30}}{10}\
    \end{bmatrix}
    $$\\
    
    
    \item
    Set the basis of $\mathbb{R}^n$ are $\{\bf{a_1},\bf{a_2},\cdots,\bf{a_n} \}$, the classical Gram-Schmidt algorithm is $$
    \bf{\hat{q}_i = a_i} - \sum_j^{i-1} (\bf{q_j^T a_i})\bf{q_j}, \quad \bf{q_i = \hat{q}_i/ ||\hat{q}_i||_2 }
    $$\\
    In the context of \textbf{projection of vector}:\\
    we can rewrite the equation to that
    $$
    \bf{\hat{q}_i} = \bf{a_i} - \sum_j^{i-1} \frac{\langle \bf{\hat{q}_j}, \bf{a_i} \rangle}{||\bf{\hat{q}_j}||_2^2} \bf{\hat{q}_j} =  \bf{a_i} - \sum_j^{i-1} \frac{\langle \bf{\hat{q}_j}, \bf{a_i} \rangle}{\langle \bf{\hat{q}_j}, \bf{\hat{q}_j} \rangle} \bf{\hat{q}_j} = \bf{a_i} - \sum_j^{i-1} \text{proj}_{\bf{\hat{q}_j}}(\bf{a_i})
    $$\\
    $\because  \text{proj}_\mathbf{b}(\bf a)=\frac{\langle{\bf a},{\bf b}\rangle}{\langle{\bf b},{\bf b}\rangle}{\bf b} $ is orthogonal projection of vector ${\bf a}$ onto vector ${\bf b}$ \\
    $\therefore \bf{\hat{q}_i} $ is the rest vector subtracted by $\{ \bf{\bf{\hat{q}_1},\bf{\hat{q}_2},\cdots, \bf{\hat{q}_{i-1}}} \}$. In other words, $\bf{\hat{q}_i}$ is orthogonal to the vectors $\{ \bf{\bf{\hat{q}_1},\bf{\hat{q}_2},\cdots, \bf{\hat{q}_{i-1}}} \}$.\\
    In the context of \textbf{projection onto subspace}:\\
    Set the orthonormal basis matrix $\bf{Q} = (\bf{q_1},\bf{q_2},\cdots,\bf{q_n})$, so $\bf{Q Q^T} = \sum_{j}^{n} \bf{q_j} \bf{q_j^T} $ \\
    $$\bf{\hat{q}_i} = \bf{a_i} - \sum_{j}^{i-1} (\bf{q_j^T a_i})\bf{q_j} = \bf{a_i} - \sum_{j}^{i-1} \bf{q_j} \bf{q_j^T a_i} = \bf{a_i} -  \bf{Q Q^T a_i} = (\bf{I - Q Q^T}) \bf{a_i} = (\bf{I - P}) \bf{a_i} $$ \\
    So we can get that $\bf{\hat{q}_i}$ is the orthogonal projection onto $(\text{span}(Q))^{\perp}$ along $\text{span}(Q)$.\\
    
    
    \item
    $$(\mathbf{a_1,a_2,a_3}) = \mathbf{A = Q R} = \mathbf{Q}  
    \begin{bmatrix}
    r_{11} & r_{12} & r_{13} \\
    0 & r_{22} & r_{23} \\
    0 & 0 & r_{33} 
    \end{bmatrix} = 
    [\mathbf{q_1,q_2,q_3}]
    \begin{bmatrix}
    \mathbf{q_1^T a_1} & \mathbf{q_1^T a_2} & \mathbf{q_1^T a_3} \\
    0 &\mathbf{q_2^T a_2} & \mathbf{q_2^T a_3} \\
    0 & 0 & \mathbf{q_3^T a_3} 
    \end{bmatrix}
    $$ 
    {\bf *******************The method below is not using $1+k\epsilon^2 =1$ $(k\in\mathbb{N}^+)$******************}\\
    \textbf{Classical GS} (could be said that deal with columns):\\
    compute order: $\mathbf{\hat{q}_1 \Rightarrow r_{11} \Rightarrow q_1 \Rightarrow r_{12} \Rightarrow \hat{q}_2 \Rightarrow r_{22} - q_2 \Rightarrow r_{13},r_{23} \Rightarrow \hat{q}_3 \Rightarrow r_{33} \Rightarrow q_3 } $ \\
    compute process:\\
    $\mathbf{\hat{q}_1 = a_1} = \begin{bmatrix}
    1 \\ \epsilon \\ \epsilon 
    \end{bmatrix}  \Rightarrow
    \mathbf{q_1 = \frac{\hat{q}_1}{||\hat{q}_1||_2}} = 
    \frac{1}{\sqrt{1+2 \epsilon^2}}
    \begin{bmatrix}
    1 \\ \epsilon \\ \epsilon 
    \end{bmatrix}
    $ \\
    $
    \mathbf{\hat{q}_2 = a_2 - (q_1^T a_2)q_1  } =  
    \begin{bmatrix}
    1 \\ \epsilon \\ 0 
    \end{bmatrix} - 
    \frac{1+ \epsilon^2}{1+2 \epsilon^2}
    \begin{bmatrix}
    1 \\ \epsilon \\ \epsilon 
    \end{bmatrix} = 
    \begin{bmatrix}
    \frac{\epsilon^2}{1+2\epsilon^2} \\ \frac{\epsilon^3}{1+2\epsilon^2} \\ -\frac{\epsilon (1+\epsilon^2)}{1+2\epsilon^2} 
    \end{bmatrix} \Rightarrow
    \begin{bmatrix}
    \epsilon \\ \epsilon^2 \\ -(1+\epsilon^2)  
    \end{bmatrix} \Rightarrow
    \mathbf{q_2} = \frac{1}{\sqrt{(1+2\epsilon^2)(1+\epsilon^2)}}
    \begin{bmatrix}
    \epsilon \\ \epsilon^2 \\ -(1+\epsilon^2) 
    \end{bmatrix}
    $\\
    $
    \mathbf{\hat{q}_3 = a_3 - (q_1^T a_3)q_1 - (q_2^T a_3)q_2 } = 
    \begin{bmatrix}
    1 \\ 0 \\ \epsilon 
    \end{bmatrix} - \frac{1+\epsilon^2}{1+2\epsilon^2}
    \begin{bmatrix}
    1 \\ \epsilon \\ \epsilon 
    \end{bmatrix} - \frac{-\epsilon^3}{(1+2\epsilon^2)(1+\epsilon^2)}
    \begin{bmatrix}
    \epsilon \\ \epsilon^2 \\ -(1+\epsilon^2) 
    \end{bmatrix} = 
    \begin{bmatrix}
    \frac{\epsilon^2}{1+\epsilon^2} \\ -\frac{\epsilon}{1+\epsilon^2} \\ 0 
    \end{bmatrix} \Rightarrow
    \begin{bmatrix}
    \epsilon^2 \\ -\epsilon \\ 0 
    \end{bmatrix} \Rightarrow
    \mathbf{q_3} = \frac{1}{\sqrt{\epsilon^4 + \epsilon^2}}
    \begin{bmatrix}
    \epsilon^2 \\ -\epsilon \\ 0 
    \end{bmatrix}
    $
    $$
    \mathbf{Q} = [\mathbf{q_1,q_2,q_3}] = 
    \begin{bmatrix}
    \frac{1}{\sqrt{1+2 \epsilon^2}} & \frac{\epsilon}{\sqrt{(1+2\epsilon^2)(1+\epsilon^2)}} & \frac{\epsilon^2}{\sqrt{\epsilon^4 + \epsilon^2}} \\
    \frac{\epsilon}{\sqrt{1+2 \epsilon^2}} &\frac{\epsilon^2}{\sqrt{(1+2\epsilon^2)(1+\epsilon^2)}} & \frac{-\epsilon}{\sqrt{\epsilon^4 + \epsilon^2}} \\
    \frac{\epsilon}{\sqrt{1+2 \epsilon^2}} & \frac{-(1+\epsilon^2)}{\sqrt{(1+2\epsilon^2)(1+\epsilon^2)}} & 0 
    \end{bmatrix}
    $$\\
    
    \textbf{MGS} (could be said that deal with rows):\\
    compute order:$\mathbf{ \hat{q}_1 \Rightarrow r_{11} \Rightarrow q_1 \Rightarrow r_{12},r_{13} \Rightarrow A(1,:) \Rightarrow \hat{q}_2 \Rightarrow r_{22} \Rightarrow q_2 \Rightarrow r_{23} \Rightarrow A(2,:) \Rightarrow \hat{q}_3  \Rightarrow r_{33} \Rightarrow q_3 }$\\
    compute process:\\
    $\mathbf{\hat{q}_1 = a_1} =  \Rightarrow
    \mathbf{q_1 = \frac{\hat{q}_1}{||\hat{q}_1||_2}} = 
    \frac{1}{\sqrt{1+2 \epsilon^2}}
    \begin{bmatrix}
    1 \\ \epsilon \\ \epsilon 
    \end{bmatrix}
    $ \\
    $
    r_{12} = \mathbf{q_1^T a_2}  = \frac{1}{\sqrt{1+2 \epsilon^2}}
    \begin{bmatrix}
    1 & \epsilon & \epsilon 
    \end{bmatrix}
    \begin{bmatrix}
    1 \\ \epsilon \\ 0 
    \end{bmatrix} = \frac{1 + \epsilon^2}{\sqrt{1+2 \epsilon^2}}
    $\\
    $
    r_{13} = \mathbf{q_1^T  a_3}  = \frac{1}{\sqrt{1+2 \epsilon^2}}
    \begin{bmatrix}
    1 & \epsilon & \epsilon 
    \end{bmatrix}
    \begin{bmatrix}
    1 \\ 0 \\ \epsilon
    \end{bmatrix} = \frac{1 + \epsilon^2}{\sqrt{1+2 \epsilon^2}}
    $\\
    $
    \mathbf{A}(:,2:3) = \mathbf{A}(:,2:3) - (r_{12} \mathbf{q_1}, r_{13} \mathbf{q_1}) = 
    \begin{bmatrix}
    1 & 1 \\ \epsilon & 0 \\ 0 & \epsilon 
    \end{bmatrix} - 
    \begin{bmatrix}
    \frac{1 + \epsilon^2}{1+2 \epsilon^2} & \frac{1 + \epsilon^2}{1+2 \epsilon^2} \\ 
    \frac{1 + \epsilon^2}{1+2 \epsilon^2} \epsilon & \frac{1 + \epsilon^2}{1+2 \epsilon^2} \epsilon \\ 
    \frac{1 + \epsilon^2}{1+2 \epsilon^2} \epsilon & \frac{1 + \epsilon^2}{1+2 \epsilon^2} \epsilon
    \end{bmatrix} = 
    \begin{bmatrix}
    \frac{ \epsilon^2}{1+2 \epsilon^2} & \frac{ \epsilon^2}{1+2 \epsilon^2} \\ 
    \frac{ \epsilon^3}{1+2 \epsilon^2} & -\frac{1 + \epsilon^2}{1+2 \epsilon^2} \\ 
    -\frac{1 + \epsilon^2}{1+2 \epsilon^2} & \frac{ \epsilon^3}{1+2 \epsilon^2}
    \end{bmatrix} \\ \Rightarrow
    \begin{bmatrix}
    \epsilon^2 & \epsilon^2 \\ 
    \epsilon^3 & -(1 + \epsilon^2) \\ 
    -(1 + \epsilon^2) & \epsilon^3
    \end{bmatrix} = (\mathbf{a_2, a_3})
    $\\
    
    $\mathbf{\hat{q}_2 = a_2} =  \Rightarrow
    \mathbf{q_2 = \frac{\hat{q}_2}{||\hat{q}_2||_2}} = 
    \frac{1}{\sqrt{\epsilon^6 + \epsilon^4 + (1+\epsilon^2)^2}}
    \begin{bmatrix}
    \epsilon^2 \\ \epsilon^3 \\ -(1+\epsilon^2) 
    \end{bmatrix}
    $ \\
    $
    r_{23} = \mathbf{q_2^T a_3} = \frac{1}{\sqrt{\epsilon^6 + \epsilon^4 + (1+\epsilon^2)^2}}
    \begin{bmatrix}
    \epsilon^2 & \epsilon^3 & -(1+\epsilon^2)
    \end{bmatrix}
    \begin{bmatrix}
    \epsilon^2 \\ -(1+\epsilon^2) \\ \epsilon^3 
    \end{bmatrix} = \frac{\epsilon^4 - 2\epsilon^3(1+\epsilon^2)}{\sqrt{\epsilon^6 + \epsilon^4 + (1+\epsilon^2)^2}}
    $\\
    $
    \mathbf{A}(:,3) = \mathbf{a_3} = \mathbf{a_3 - q_2}  r_{23} = 
    \begin{bmatrix}
    \epsilon^2 \\ -(1+\epsilon^2) \\ \epsilon^3 
    \end{bmatrix} - 
    \frac{\epsilon^4 - 2\epsilon^3(1+\epsilon^2)}{\epsilon^6 + \epsilon^4 + (1+\epsilon^2)^2}
    \begin{bmatrix}
    \epsilon^2 \\ \epsilon^3 \\-(1+\epsilon^2) 
    \end{bmatrix} = 
    \begin{bmatrix}
    \frac{\epsilon^2(\epsilon^6 + 2\epsilon^5+\epsilon^4+2\epsilon^3+2\epsilon^2+1)}{\epsilon^6 + \epsilon^4 + (1+\epsilon^2)^2} \\ \frac{(1+\epsilon^2)(\epsilon^6-2\epsilon^4-2\epsilon^2-1)-\epsilon^7}{\epsilon^6 + \epsilon^4 + (1+\epsilon^2)^2} \\  \frac{(1+\epsilon^2)(\epsilon^7-\epsilon^5+\epsilon^4-\epsilon^3)}{\epsilon^6 + \epsilon^4 + (1+\epsilon^2)^2} 
    \end{bmatrix}
    $\\
    $
    \mathbf{\hat{q}_3  = a_3 \Rightarrow q_3 = \frac{\hat{q}_3}{||\hat{q}_3||_2}   } \Rightarrow \\ 
    \frac{1}{\sqrt{(\epsilon^2(\epsilon^6 + 2\epsilon^5+\epsilon^4+2\epsilon^3+2\epsilon^2+1))^2+ ((1+\epsilon^2)(\epsilon^6-2\epsilon^4-2\epsilon^2-1)-\epsilon^7)^2+ ((1+\epsilon^2)(\epsilon^7-\epsilon^5+\epsilon^4-\epsilon^3))^2}}
    \begin{bmatrix}
    \epsilon^2(\epsilon^6 + 2\epsilon^5+\epsilon^4+2\epsilon^3+2\epsilon^2+1) \\ 
    (1+\epsilon^2)(\epsilon^6-2\epsilon^4-2\epsilon^2-1)-\epsilon^7 \\
    (1+\epsilon^2)(\epsilon^7-\epsilon^5+\epsilon^4-\epsilon^3) 
    \end{bmatrix}
    $\\ \\
    The two results above are obviously different. I find that if $\epsilon$ is small even nearly equal to zero , the classical GS algorithm will be more unstable due to computer rounding errors.During classical GS, denominator is more likely to be zero.\\
    
    {\bf *******************The method below is using $1+k\epsilon^2 =1$ $(k\in\mathbb{N}^+)$******************}\\
    \textbf{Classical GS} (could be said that deal with columns):\\
    compute order: $\mathbf{\hat{q}_1 \Rightarrow r_{11} \Rightarrow q_1 \Rightarrow r_{12} \Rightarrow \hat{q}_2 \Rightarrow r_{22} - q_2 \Rightarrow r_{13},r_{23} \Rightarrow \hat{q}_3 \Rightarrow r_{33} \Rightarrow q_3 } $ \\
    compute process:\\
    $\mathbf{\hat{q}_1 = a_1} = \begin{bmatrix}
    1 \\ \epsilon \\ \epsilon 
    \end{bmatrix}  \Rightarrow
    \mathbf{q_1 = \frac{\hat{q}_1}{||\hat{q}_1||_2}} = 
    \frac{1}{\sqrt{1+2 \epsilon^2}}
    \begin{bmatrix}
    1 \\ \epsilon \\ \epsilon 
    \end{bmatrix} = \begin{bmatrix}
    1 \\ \epsilon \\ \epsilon 
    \end{bmatrix}
    $ \\
    $
    \mathbf{\hat{q}_2 = a_2 - (q_1^T a_2)q_1  } =  
    \begin{bmatrix}
    1 \\ \epsilon \\ 0 
    \end{bmatrix} - 
    \frac{1+ \epsilon^2}{1+2 \epsilon^2}
    \begin{bmatrix}
    1 \\ \epsilon \\ \epsilon 
    \end{bmatrix} = 
    \begin{bmatrix}
    0 \\ 0\\ -\epsilon
    \end{bmatrix}  \Rightarrow
    \begin{bmatrix}
    0\\ 0 \\ -1 
    \end{bmatrix}
    $\\
    $
    \mathbf{\hat{q}_3 = a_3 - (q_1^T a_3)q_1 - (q_2^T a_3)q_2 } = 
    \begin{bmatrix}
    1 \\ 0 \\ \epsilon 
    \end{bmatrix} - \frac{1+\epsilon^2}{1+2\epsilon^2}
    \begin{bmatrix}
    1 \\ \epsilon \\ \epsilon 
    \end{bmatrix} - (-\epsilon)
    \begin{bmatrix}
    0 \\ 0 \\ -1 
    \end{bmatrix} = 
    \begin{bmatrix}
    0 \\ -\epsilon \\ -\epsilon 
    \end{bmatrix} \Rightarrow
    \mathbf{q_3} = \frac{1}{\sqrt{2}}
    \begin{bmatrix}
    0 \\ -1 \\ -1 
    \end{bmatrix}
    $
    $$
    \mathbf{Q} = [\mathbf{q_1,q_2,q_3}] = 
    \begin{bmatrix}
    1 & 0 & 0 \\
    \epsilon & 0 & -\frac{1}{\sqrt{2}} \\
    \epsilon & -1 & -\frac{1}{\sqrt{2}} 
    \end{bmatrix}
    $$
    
    \textbf{MGS} (could be said that deal with rows):\\
    compute order:$\mathbf{ \hat{q}_1 \Rightarrow r_{11} \Rightarrow q_1 \Rightarrow r_{12},r_{13} \Rightarrow A(1,:) \Rightarrow \hat{q}_2 \Rightarrow r_{22} \Rightarrow q_2 \Rightarrow r_{23} \Rightarrow A(2,:) \Rightarrow \hat{q}_3  \Rightarrow r_{33} \Rightarrow q_3 }$\\
    compute process:\\
    $\mathbf{\hat{q}_1 = a_1} =  \Rightarrow
    \mathbf{q_1 = \frac{\hat{q}_1}{||\hat{q}_1||_2}} = 
    \begin{bmatrix}
    1 \\ \epsilon \\ \epsilon 
    \end{bmatrix}, \quad
    r_{12} = \mathbf{q_1^T a_2}  = \frac{1}{\sqrt{1+2 \epsilon^2}}
    \begin{bmatrix}
    1 & \epsilon & \epsilon 
    \end{bmatrix}
    \begin{bmatrix}
    1 \\ \epsilon \\ 0 
    \end{bmatrix} =  1 , \quad
    r_{13} = \mathbf{q_1^T  a_3}  = 1$\\
    $
    \mathbf{A}(:,2:3) = \mathbf{A}(:,2:3) - (r_{12} \mathbf{q_1}, r_{13} \mathbf{q_1}) = 
    \begin{bmatrix}
    1 & 1 \\ \epsilon & 0 \\ 0 & \epsilon 
    \end{bmatrix} - 
    \begin{bmatrix}
    1 & 1 \\ 
    \epsilon & \epsilon \\ 
    \epsilon & \epsilon
    \end{bmatrix} = 
    \begin{bmatrix}
    0 & 0 \\ 
    0 & -\epsilon \\ 
    -\epsilon & 0
    \end{bmatrix} = (\mathbf{a_2, a_3})
    $\\
    
    $\mathbf{\hat{q}_2 = a_2} = 
    \begin{bmatrix}
    0 \\ 0 \\ -\epsilon
    \end{bmatrix}
    \Rightarrow
    \mathbf{q_2 = \frac{\hat{q}_2}{||\hat{q}_2||_2}} = 
    \begin{bmatrix}
    0 \\ 0\\ -1
    \end{bmatrix}, \quad
    r_{23} = \mathbf{q_2^T a_3} = 
    \begin{bmatrix}
    0 & 0& -1
    \end{bmatrix}
    \begin{bmatrix}
    0 \\ -\epsilon \\ 0 
    \end{bmatrix} = 0
    $\\
    $
    \mathbf{A}(:,3) = \mathbf{a_3} = \mathbf{a_3 - q_2}  r_{23} = 
    \begin{bmatrix}
    0 \\ -\epsilon \\ 0 
    \end{bmatrix} - {\bf 0} = 
    \begin{bmatrix}
    0 \\ -\epsilon \\ 0 
    \end{bmatrix},\quad
    \mathbf{\hat{q}_3  = a_3 \Rightarrow q_3} = \begin{bmatrix}
    0 \\ -1 \\ 0 
    \end{bmatrix}
    $\\ 
    $$
    \mathbf{Q} = [\mathbf{q_1,q_2,q_3}] = 
    \begin{bmatrix}
    1 & 0 & 0 \\
    \epsilon & 0 & -1 \\
    \epsilon & -1 & 0 
    \end{bmatrix}
    $$\\
    
    
    \item
    \textbf{ Classical GS}:
    $$
    \epsilon = 1e-4 \Rightarrow \bf{Q} = 
    \begin{bmatrix}
    1 & 0.0001 & 0.0001 \\
    0.0001 & 0 & -1 \\
    0.0001 & -1 & 0 
    \end{bmatrix}, 
    \|{\bf Q}^T {\bf Q} - {\bf I}\|_{\text{F}} = \text{3.9885e-9}
    $$\\
    $$
    \epsilon = 1e-9 \Rightarrow \bf{Q} = 
    \begin{bmatrix}
    1 & 0 & 0 \\
    0 & 0 & -0.7071 \\
    0 & -1 & -0.7071
    \end{bmatrix}, 
    \|{\bf Q}^T {\bf Q} - {\bf I}\|_{\text{F}} = \text{1}
    $$\\
    \textbf{ MGS}:
    $$
    \epsilon = 1e-4 \Rightarrow \bf{Q} = 
    \begin{bmatrix}
    1 & 0.0001 & 0.0001 \\
    0.0001 & 0 & -1 \\
    0.0001 & -1 & 0 
    \end{bmatrix}, 
    \|{\bf Q}^T {\bf Q} - {\bf I}\|_{\text{F}} = \text{5.6406e-13}
    $$\\
    $$
    \epsilon = 1e-9 \Rightarrow \bf{Q} = 
    \begin{bmatrix}
    1 & 0 & 0 \\
    0 & 0 & -1 \\
    0 & -1 & 0
    \end{bmatrix}, 
    \|{\bf Q}^T {\bf Q} - {\bf I}\|_{\text{F}} = \text{2e-9}
    $$\\
    The result can support my view in (3).
    
\end{enumerate}




\newpage
\section{SOLVING LS VIA QR FACTORIZATION AND NORMAL EQUATION}
\noindent\textbf{Problem 4 [Understanding the influence of the condition number to the solution.]}. \textcolor{blue}{(4 points + 5 points + 4 points + 4 points + 3 points points) }

Consider such two LS problems:
\begin{align}
    &\min_{{\bf x}\in\mathbb{R}^n} \|{\bf Ax - b}\|_2^2 \label{axb}\\
    &\min_{{\bf x}\in\mathbb{R}^n} \|{\bf A}{\bf x} - ({\bf b}+\delta{\bf b })\|_2^2 \nonumber%\label{withnoise}
\end{align}
with ${\bf A}\in \mathbb{R}^{m\times n}$. For ${\bf b} = \begin{bmatrix}
    1 & 3/2 & 3 & 6
    \end{bmatrix}^T$
    and 
    $\delta{\bf b} = \begin{bmatrix}
    1/10 & 0 & 0 & 0
    \end{bmatrix}^T$,

\begin{enumerate}
    \item Computing solution to the problem (\ref{axb})
    via QR decomposition when \[{\bf A}=
    \begin{bmatrix}
    1 & 2 & 3\\
    2 & 3 & 5\\
    3 & 4 & 7\\
    4 & 5 & 11
    \end{bmatrix}. \]
    
    \item For a full-rank matrix ${\bf A}$, consider the equation ${\bf Ax=b}$, after adding some noise $\delta{\bf b}$ to {\bf b}, we have ${\bf A}({\bf x}+\delta{\bf x}) = {\bf b}+\delta{\bf b}$, 
    and then proof
    $$ \frac{1}{\|{\bf A}\| \|{\bf A}^{\dagger}\|} \frac{\|\delta {\bf b}\|}{\|{\bf b}\|}
    \leq \frac{\|\delta {\bf x}\|}{\|{\bf x}\|} \leq
    \|{\bf A}\| \|{\bf A}^{\dagger}\|\frac{\|\delta {\bf b}\|}{\|{\bf b}\|}, $$
    and give it a plain interpretation.
    
    \item Computing the solutions to the two LS problems via the normal equation $ {\bf A}^T{\bf A}{\bf x}_{LS} = {\bf A}^T {\bf b} $ when \[{\bf A}=\begin{bmatrix}
    1 & 2 & 2\\
    2 & 2 & 2\\
    3 & 3 & 3\\
    1 & 1 & 0
    \end{bmatrix}.  \]
    
    \item Computing the solutions to the two LS problems via the normal equation $ {\bf A}^T{\bf A}{\bf x}_{LS} = {\bf A}^T {\bf b} $ when \[ {\bf A} = \begin{bmatrix}
    1 & 1 & 1\\
    1 & 2 & 4\\
    1 & 3 & 9\\
    1 & 4 & 16
    \end{bmatrix}. \]
    
    \item 
    Compare the 2-norm condition number $\|{\bf A}\|\| {\bf A}^{\dagger
    }\|$ for ${\bf A}$ in 3) and 4) and the influence on the solution to problem (\ref{axb}) 
    resulted by the additional noise $\delta{\bf b}$.
    
    \noindent{\bf Hint:} Show the influence on the solution  by $\frac{\|\delta {\bf x}\|}{\|{\bf x}\|}$.
\end{enumerate}




\noindent{\bf Remarks:} You can use MATLAB for some matrix computations (deviation is expected) in 3), 4), 5).
Do not use decimals in your answers, fraction and $n$-th roots of numbers are accepted.

\noindent
\textbf{Solution.}
Please insert your solution here ...

\begin{enumerate}
    \item
    
    $$\left\|\mathbf{Q}\right\|_2^2 = 1 \Rightarrow  \left\|\mathbf{Q}^{T} \mathbf{z}\right\|_{2}=\|\mathbf{z}\|_{2}$$
    $$
    \begin{aligned}
    \|\mathbf{y}-\mathbf{A} \mathbf{x}\|_{2}^{2} &=\left\|\mathbf{Q}^{T} \mathbf{y}-\mathbf{Q}^{T} \mathbf{A} \mathbf{x}\right\|_{2}^{2}=\left\|\mathbf{Q}^{T} \mathbf{y}-\mathbf{R} \mathbf{x}\right\|_{2}^{2} \\
    &=\left\|\left[\begin{array}{c}
    \mathbf{Q}_{1}^{T} \mathbf{y} \\
    \mathbf{Q}_{2}^{T} \mathbf{y}
    \end{array}\right]-\left[\begin{array}{c}
    \mathbf{R}_{1} \mathbf{x} \\
    \mathbf{0}
    \end{array}\right]\right\|_{2}^{2}=\left\|\mathbf{Q}_{1}^{T} \mathbf{y}-\mathbf{R}_{1} \mathbf{x}\right\|_{2}^{2}+\left\|\mathbf{Q}_{2}^{T} \mathbf{y}\right\|_{2}^{2}
    \end{aligned}
    $$
    Then LS problem $\min _{\mathrm{x} \in \mathbb{R}^{n}}\|\mathbf{A} \mathbf{x}-\mathbf{b}\|_{2}^{2}$ reduces to solve $\mathbf{R}_{1} \mathbf{x}=\mathbf{Q}_{1}^{T} \mathbf{b}$\\
    $
    \mathbf{A = (a_1,a_2,a_3)} = 
    \begin{bmatrix}
    1 & 2 & 3\\
    2 & 3 & 4\\
    3 & 4 & 7\\
    4 & 5 & 11
    \end{bmatrix} = 
    \mathbf{Q_1 R_1} = 
    \begin{bmatrix}
    \mathbf{q_1} & \mathbf{q_2}  & \mathbf{q_3} \\
    \end{bmatrix}
    \begin{bmatrix}
    r_{11} & \mathbf{q_1^T a_2}  & \mathbf{q_1^T a_3} \\
    0 & r_{22} & \mathbf{q_2^T a_3} \\
    0 & 0 & r_{33}\\
    \end{bmatrix}
    $\\
   $
   r_{11} = \| \mathbf{a_1} \|_2 = \sqrt{30}, \mathbf{q_1} = \frac{1}{\sqrt{30}} (1,2,3,4)^T, \\ r_{12} = \mathbf{q_1^T a_2 } = \frac{40}{\sqrt{30}}, \mathbf{\hat{q}_2 = a_2 - (q_1^T a_2 )q_1} = \frac{1}{3} (2,1,0,-1)^T,r_{22} = \frac{\sqrt{6}}{3}, \mathbf{q_2} = \frac{\mathbf{\hat{q}_2}}{r_{22}} = \frac{1}{\sqrt{6}} (2,1,0,-1)^T, \\
   r_{13} = \mathbf{q_1^T a_3} = \frac{78}{\sqrt{30}},r_{23} = \mathbf{q_2^T a_3} = 0,\mathbf{\hat{q}_3 = a_3} - r_{13}\mathbf{q_1} - r_{23}\mathbf{q_2} = \frac{1}{5}(2,-1,-4,3)^T,r_{33} = \frac{\sqrt{30}}{5},\\ \mathbf{q_3} = \frac{1}{\sqrt{30}}(2,-1,-4,3)^T.
   $
   $$
   \mathbf{A = Q_1 R_1} = 
   \begin{bmatrix}
    \frac{1}{\sqrt{30}} & \frac{2}{\sqrt{6}} & \frac{2}{\sqrt{30}}\\
    \frac{2}{\sqrt{30}} & \frac{1}{\sqrt{6}} & -\frac{1}{\sqrt{30}}\\
    \frac{3}{\sqrt{30}} & 0 & -\frac{4}{\sqrt{30}}\\
    \frac{4}{\sqrt{30}} & -\frac{1}{\sqrt{6}} & \frac{3}{\sqrt{30}}
    \end{bmatrix}
    \begin{bmatrix}
    \sqrt{30} & \frac{40}{\sqrt{30}} & \frac{78}{\sqrt{30}}\\
    0 & \frac{\sqrt{6}}{3} & 0\\
    0 & 0 & \frac{\sqrt{30}}{5}
    \end{bmatrix}
   $$
   
   $$
   \mathbf{R_1 x = Q_1^T b} \Rightarrow
   \begin{bmatrix}
    \sqrt{30} & \frac{40}{\sqrt{30}} & \frac{78}{\sqrt{30}}\\
    0 & \frac{\sqrt{6}}{3} & 0\\
    0 & 0 & \frac{\sqrt{30}}{5}
    \end{bmatrix}
    \begin{bmatrix}
    x_1 \\ x_2 \\ x_3
    \end{bmatrix} = 
    \begin{bmatrix}
    \frac{1}{\sqrt{30}} & \frac{2}{\sqrt{30}}  &  \frac{3}{\sqrt{30}} & \frac{4}{\sqrt{30}}  \\
    \frac{2}{\sqrt{6}} & \frac{1}{\sqrt{6}}  & 0  & -\frac{1}{\sqrt{6}}  \\
    \frac{2}{\sqrt{30}} & -\frac{1}{\sqrt{30}}  & -\frac{4}{\sqrt{30}}  & \frac{3}{\sqrt{30}}  \\
    \end{bmatrix}
    \begin{bmatrix}
    1 \\ \frac{3}{2} \\ 3 \\ 6
    \end{bmatrix} = 
    \begin{bmatrix}
    \frac{37}{\sqrt{30}} \\ -\frac{5}{2\sqrt{6}} \\ \frac{13}{2\sqrt{30}}
    \end{bmatrix}
   $$
   backward substitution $\Rightarrow$ 
   $x_3 = \frac{13}{12} \Rightarrow x_2 = -\frac{5}{4} \Rightarrow x_1 = \frac{1}{12} $
   $\Rightarrow \mathbf{x} =
    \begin{bmatrix}
    \frac{1}{12} & -\frac{5}{4} & \frac{13}{12}
    \end{bmatrix}^T
   $
    
    \item
    
    {\bf Firstly:}\\
    $\because \bf A x = b, A(x + \delta x)  = b + \delta b \quad \Rightarrow \quad x = A^{\dagger} b, (x + \delta x) = A^{\dagger}(b + \delta b)$\\
    $\therefore \bf \|\delta x \| = \| (x + \delta x) - x \| = \| A^{\dagger} (b + \delta b - b) \| \le \|A^{\dagger}\| \| \delta b \|      $\\
    $\because \bf b = A x \quad \Rightarrow \quad \|b\| \le \|A\| \|x \| \quad \Rightarrow \quad \frac{1}{\|x\|} \le \frac{\|A\|}{\|b\|} $\\
    $\therefore \bf \frac{\|\delta x \|}{\|x\|} \le \|A^{\dagger}\| \| \delta b \| \frac{\|A\|}{\|b\|} = \|A\| \|A^{\dagger} \| \frac{\| \delta b \|}{\|b\|}$
    
    {\bf Secondly:}\\
    $\because \bf \|\delta b \| = \| (b + \delta b) - b \| = \| A (x + \delta x - x) \| \le \|A\| \| \delta x \|      $\\
    $\because \bf x = A^{\dagger} b \quad \Rightarrow \quad \|x\| \le \|A^{\dagger}\| \|b \| \quad \Rightarrow \quad \frac{1}{\|b\|} \le \frac{\|A^{\dagger}\|}{\|x\|} $\\
    $\therefore \bf \frac{\|\delta b \|}{\|b\|} \le \|A\| \| \delta x \| \frac{\|A^{\dagger}\|}{\|x\|} \quad \Rightarrow \quad \frac{1}{\|A\| \|A^{\dagger}\|} \frac{\| \delta b\|}{\|b\|} \le \frac{\| \delta x \|}{\|x \|}     $\\
    
    Then we can get:
    $\bf  \quad\quad \frac{1}{\|{\bf A}\| \|{\bf A}^{\dagger}\|} \frac{\|\delta {\bf b}\|}{\|{\bf b}\|}
    \leq \frac{\|\delta {\bf x}\|}{\|{\bf x}\|} \leq
    \|{\bf A}\| \|{\bf A}^{\dagger}\|\frac{\|\delta {\bf b}\|}{\|{\bf b}\|}$
    
    
    \item
    
    Using Matlab, the process is below:\\
    as for \textbf{problem(1)}, $b= (1,\frac{3}{2},3,6)^T$,\\
    $$
    \mathbf{C=  A^T A} = 
    \begin{bmatrix}
    15 & 16  &  15  \\
    16 & 18  &  17  \\
    15 & 17  &  17  
    \end{bmatrix} ,  \mathbf{d = A^T b} = 
    \begin{bmatrix}
    19 \\ 20  \\  14  
    \end{bmatrix} $$ 
    
    $$
    \text{cholesky decomposition:  } \quad
    \mathbf{C = G^T G}  \Rightarrow  \mathbf{G} = 
    \begin{bmatrix}
    \frac{1921}{496} & \frac{1921}{465}  & \frac{1921}{496}  \\
    0 & \frac{1624}{1681}  &  \frac{1681}{1624}  \\
    0 & 0  &  \frac{1404}{1457}  
    \end{bmatrix}
    $$
    
    $$
    \text{Forward and backward substitution:  } \quad
    \mathbf{x_{LS}}  = \begin{bmatrix}
    \frac{11}{13} & \frac{67}{13}  &  -\frac{66}{13}  
    \end{bmatrix} ^T
    $$\\
    as for \textbf{problem(2)}, $b=b + \delta b =  (\frac{11}{10},\frac{3}{2},3,6)^T$,\\
    $$\mathbf{C = A^T A } \text{is above}  , \mathbf{d = A^T b} = (\frac{191}{10}, \frac{101}{5}, \frac{71}{5} )^T , \text{cholesky decomposition is above.} 
    \Rightarrow \mathbf{x_{LS}} = \begin{bmatrix}
    \frac{97}{130} & \frac{683}{130}  &  -\frac{66}{13}  
    \end{bmatrix} ^T
    $$

    \item
    using matlab, computing:
    $\quad \mathbf{x_{LS}} = (A^T A)^{-1} A^T b $\\
    as for problem(1): $\quad \mathbf{x_{LS}} = \begin{bmatrix}
    \frac{15}{8} & -\frac{59}{40}  &  \frac{5}{8}  
    \end{bmatrix} ^T
    $\\
    as for problem(2): $\quad \mathbf{x_{LS}} = \begin{bmatrix}
    \frac{21}{10} & -\frac{163}{100}  &  \frac{13}{20}  
    \end{bmatrix} ^T
    $\\
    
    \item
    
    for $\mathbf{A}$ in (3):\\
    condition number: $\quad \|A\| \|A^{\dagger}\| = \frac{5610}{421} \approx 13.32 $ \\
    influence:$\quad \frac{\| \delta \mathbf{x} \|}{\| \mathbf{ x}\|} = \frac{125}{6438} \approx 0.0194 $
    
    for $\mathbf{A}$ in (4):\\
    condition number: $\quad \|A\| \|A^{\dagger}\| = \frac{2653}{36} \approx 73.69 $ \\
    influence:$\quad \frac{\| \delta \mathbf{x} \|}{\| \mathbf{ x}\|} = \frac{1231}{11065} \approx 0.1113 $\\
    
    We can get that: if the condition number is large, even a small error in {\bf b} may cause a large error in {\bf x}. The larger the condition number is , the larger error in {\bf x} will be caused by the same $\delta \mathbf{b}$.
    
\end{enumerate}


\newpage
\section{Underdetermined System}

\noindent\textbf{Problem 5 [Solving Underdetermined System by QR]}. \textcolor{blue}{(10 points + 5 points)}

Consider the following underdetermined system ${\bf Ax}={\bf b}$ with ${\bf A}\in \mathbb{R}^{m\times n}$ and $m<n$. Let 
    \[
    {\bf A}=
    \begin{bmatrix}
         1&2&2&0\\
         0&-2&2&1\\
         2&5&6&1
    \end{bmatrix}\,,
    {\bf b}=
    \begin{bmatrix}
        b_1\\b_2\\b_3
    \end{bmatrix}\,,
    \]
\begin{enumerate}
    \item Use Householder reflection to give the full QR decomposition of tall ${\bf A}^T$, i.e., $\A^T= \Q\R$ with $\Q$ being a square matrix with orthonormal columns.
    \item Give one possible solution via QR decomposition of $\A^T$, write down your solution using $\bf{b}$.
\end{enumerate}
\noindent
\textbf{Solution.}
Please insert your solution here ...
\begin{enumerate}
    \item 
    $$
    \mathbf{A}^T = 
    \begin{bmatrix}
         1 & 0 & 2 \\
         2 & -2 & 5\\
         2 & 2 & 6 \\
         0 & 1 & 1
    \end{bmatrix} = \mathbf{A_1} = (\mathbf{a_1,a_2,a_3})
    $$
    $$\mathbf{u_1  = a_1 + \| a_1 \|_2 e_1 } = (4,2,2,0)^T,\quad \mathbf{R_1 = I -2 \frac{u_1 u_1^T}{\| u_1 \|_2^2} ,\quad  (A_1)_{* j} = R_1 (A_1)_{* j} } $$
    $$
     \mathbf{(A_1)_{* 1} =  (A_1)_{* 1}}  - 2 \frac{\mathbf{u_1^T  (A_1)_{* j}}}{\| \mathbf{u}_1 \|_2^2 } \mathbf{u_1}= \begin{bmatrix}
         -3 \\ 0 \\ 0 \\ 0 
    \end{bmatrix}, \quad \mathbf{(A_1)_{* 2}} = 
    \begin{bmatrix}
         0 \\ -2 \\ 2 \\ 1 
    \end{bmatrix}, \quad \mathbf{(A_1)_{* 3}} = 
    \begin{bmatrix}
         -8 \\ 0 \\ 1 \\ 1 
    \end{bmatrix} \Rightarrow
    \mathbf{R_1 A_1} = 
    \begin{bmatrix}
         -3 & 0 & -8 \\
         0 & -2 & 0\\
         0 & 2 & 1 \\
         0 & 1 & 1
    \end{bmatrix}
    $$
    Set $$ \mathbf{A_2} = 
    \begin{bmatrix}
         -2 & 0  \\
         2 & 1 \\
         1 & 1 
    \end{bmatrix}
    $$
    $$\mathbf{u_2} = \begin{bmatrix}  0 \\ \mathbf{\hat{u}_2}  \end{bmatrix}, 
    \mathbf{\hat{u}_2} = \mathbf{(A_2)_{* 1} + \| (A_2)_{* 1}  \|_2 e_1 }  = (-5,2,1)^T,
    \mathbf{R_2} = \begin{bmatrix}
         \mathbf{I} & \mathbf{0}  \\
         \mathbf{0} & \mathbf{\hat{R}_2} 
    \end{bmatrix}, 
    \mathbf{\hat{R}_2 = I -2 \frac{\hat{u}_2 \hat{u}_2^T}{\| \hat{u}_2 \|_2^2}}
    $$
    The same as the first step, we can get $\mathbf{\hat{R}_2 A_2} = 
    \begin{bmatrix}
         3 & 1  \\
         0 & \frac{3}{5} \\
         0 & \frac{4}{5}
    \end{bmatrix}
    $\\
    Set $$ \mathbf{A_3} = \begin{bmatrix}  \frac{3}{5} \\ \frac{4}{5}  \end{bmatrix} $$
    $$\mathbf{u_3} = \begin{bmatrix}  0 \\ 0 \\ \mathbf{\hat{u}_3}  \end{bmatrix}, 
    \mathbf{\hat{u}_3} = \mathbf{(A_3)_{* 1} + \| (A_3)_{* 1}  \|_2 e_1 }  = (\frac{8}{5},\frac{4}{5})^T,
    \mathbf{R_3} = \begin{bmatrix}
         \mathbf{I} & \mathbf{0}  \\
         \mathbf{0} & \mathbf{\hat{R}_3} 
    \end{bmatrix}, 
    \mathbf{\hat{R}_3 = I -2 \frac{\hat{u}_3 \hat{u}_3^T}{\| \hat{u}_3 \|_2^2}}
    $$
    We can get $\mathbf{\hat{R}_3 A_3} = \begin{bmatrix}  -1 \\ 0  \end{bmatrix}$\\
    Next we will compute $\mathbf{Q}$ and $\mathbf{R}$:\\
    $$ 
    \mathbf{R} \text{ is the upper triangle matrix} \Rightarrow 
    \begin{bmatrix}
        -3 & 0 & -8  \\
        0 & 3 & 1 \\
        0 & 0 & -1 \\
        0 & 0 & 0
    \end{bmatrix}
    $$
    $$
    \mathbf{R_1} = \begin{bmatrix}
        -\frac{1}{3} & -\frac{2}{3} & -\frac{2}{3} & 0 \\
        -\frac{2}{3} & \frac{2}{3} & -\frac{1}{3} & 0 \\
        -\frac{2}{3} & -\frac{1}{3} & \frac{2}{3} & 0 \\
        0 & 0 & 0 & 1
    \end{bmatrix},\quad
    \mathbf{R_2} = \begin{bmatrix}
        1 & 0 & 0 & 0 \\
        0 & -\frac{2}{3} & \frac{2}{3} & \frac{1}{3} \\
        0 & \frac{2}{3} & \frac{11}{15} & -\frac{2}{15} \\
        0 & \frac{1}{3} & -\frac{2}{15} & \frac{14}{15}
    \end{bmatrix},\quad
    \mathbf{R_3} = \begin{bmatrix}
        1 & 0 & 0 & 0 \\
        0 & 1 & 0 & 0 \\
        0 & 0 & -\frac{3}{5} & -\frac{4}{5}\\
        0 & 0 & -\frac{4}{5} & \frac{3}{5}
    \end{bmatrix}$$
    $$
    \text{orthogonal matrix } \mathbf{Q^{-1} = R_3 R_2 R_1 } = 
    \begin{bmatrix}
        -\frac{1}{3} & -\frac{2}{3} & -\frac{2}{3} & 0 \\
        0 & -\frac{2}{3} & \frac{2}{3} & \frac{1}{3} \\
        \frac{2}{3} & -\frac{1}{3} & 0 & -\frac{2}{3} \\
        \frac{2}{3} & 0 & -\frac{1}{3} & \frac{2}{3}
    \end{bmatrix}
    $$
    So:
    $$
    \mathbf{A^T = Q R } = 
    \begin{bmatrix}
        -\frac{1}{3} & 0 & \frac{2}{3} & \frac{2}{3} \\
        -\frac{2}{3} & -\frac{2}{3} & -\frac{1}{3} & 0 \\
        -\frac{2}{3} & \frac{2}{3} & 0 & -\frac{1}{3} \\
        0 & \frac{1}{3} & -\frac{2}{3} & \frac{2}{3}
    \end{bmatrix}
    \begin{bmatrix}
        -3 & 0 & -8  \\
        0 & 3 & 1 \\
        0 & 0 & -1 \\
        0 & 0 & 0
    \end{bmatrix}
    $$
    
    \item
    
    $$\mathbf{A^T = Q R = }  
    \begin{bmatrix}  \mathbf{Q_1} & \mathbf{Q_2}  \end{bmatrix}  
    \begin{bmatrix}  \mathbf{R_1} \\ \mathbf{0}  \end{bmatrix} = \mathbf{Q_1 R_1 + Q_2 0} \Rightarrow
    \mathbf{A x = (Q_1 R_1 + Q_2 0)^T x = R_1^T Q_1^T x + 0^T Q_2^T x = b  }
    $$
    $$ \therefore \mathbf{Q_1^T x = R_1^{-T} b} \Rightarrow 
    \begin{bmatrix}  \mathbf{Q_1^T} \\ \mathbf{Q_2^T}  \end{bmatrix} 
    \mathbf{x = Q^T x} = \begin{bmatrix}  \mathbf{R_1^{-T} b} \\ \mathbf{d}  \end{bmatrix} \Rightarrow
    \therefore \mathbf{x} = \mathbf{Q}  \begin{bmatrix}  \mathbf{R_1^{-T} b} \\ \mathbf{d}  \end{bmatrix}
    \quad (\mathbf{d} \text{ could be set to } \mathbf{0})
    $$
    $$
    \mathbf{R_1^{-T}} = 
    \begin{bmatrix}
        -\frac{1}{3} & 0 & 0  \\
        0 & \frac{1}{3} & 0 \\
        \frac{8}{3} & \frac{1}{3} & -1 
    \end{bmatrix} \Rightarrow \text{so: }
    \mathbf{x} = 
    \begin{bmatrix}
        -\frac{1}{3} & 0 & \frac{2}{3} & \frac{2}{3} \\
        -\frac{2}{3} & -\frac{2}{3} & -\frac{1}{3} & 0 \\
        -\frac{2}{3} & \frac{2}{3} & 0 & -\frac{1}{3} \\
        0 & \frac{1}{3} & -\frac{2}{3} & \frac{2}{3}
    \end{bmatrix}
    \begin{bmatrix}  
    -\frac{1}{3} b_1 \\ 
    \frac{1}{3} b_2 \\
    \frac{8}{3} b_1 + \frac{1}{3} b_2 - b_3\\
    0
    \end{bmatrix} = 
    \begin{bmatrix}  
    \frac{17}{9} b_1 + \frac{2}{9} b_2 -\frac{2}{3} b_3 \\ 
    -\frac{2}{3} b_1 - \frac{1}{3} b_2 + \frac{1}{3} b_3 \\
    \frac{2}{9} b_1 + \frac{2}{9} b_2 \\
    -\frac{5}{3} b_1 - \frac{2}{9} b_2 + \frac{2}{3} b_3
    \end{bmatrix}
    $$
    
\end{enumerate}



\newpage
\section{Solving LS via Projection}
\noindent\textbf{Problem 6}. \textcolor{blue}{(Bonus question, 6 points + 4 points)}

Consider the Least Square (LS) problem:
\begin{equation}
    \label{eq:LS_problem}
    \min_{\mathbf{x}\in\mathbb{R}^n}\|\mathbf{A}\mathbf{x}-\mathbf{y}\|_2^2
\end{equation}
where $\mathbf{A}\in\mathbb{R}^{m\times n}$ ($m>n$) may not be full rank. Denote 
\begin{equation*}
    X_{\mathrm{LS}}=\left\{\mathbf{x}\in\mathbb{R}^n| \mathbf{A}^T\mathbf{A}\mathbf{x}=\mathbf{A}^T\mathbf{y}\right\}
\end{equation*}
as the set of all solutions to (\ref{eq:LS_problem}), and 
\begin{equation*}
    \mathbf{x}_{\mathrm{LS}}=\mathbf{A}^\dagger \mathbf{y}
\end{equation*}
where $\mathbf{A}^\dagger\in\mathbb{R}^{n\times m}$ is the \emph{pseudo inverse of $\mathbf{A}$} satisfies the following properties:
\begin{enumerate}
    \item $\mathbf{A}\mathbf{A}^\dagger\mathbf{A}=\mathbf{A}$.
    \item $\mathbf{A}^\dagger\mathbf{A}\mathbf{A}^\dagger=\mathbf{A}^\dagger$.
    \item $(\mathbf{A}\mathbf{A}^\dagger)^T=\mathbf{A}\mathbf{A}^\dagger$.
    \item $(\mathbf{A}^\dagger\mathbf{A})^T=\mathbf{A}^\dagger\mathbf{A}$.
\end{enumerate}

Answer the following questions:
\begin{enumerate}
    \item Prove that $\mathbf{x}_{\mathrm{LS}}$ is a solution to (\ref{eq:LS_problem}) and is of minimum $2$-norm in $X_{\mathrm{LS}}$, that is
    \begin{equation*}
        \mathbf{x}_{\mathrm{LS}}=\arg\min_{\mathbf{x}\in X_{\mathrm{LS}}}\|\mathbf{x}\|_2\  \,.
    \end{equation*}
    \textbf{Hint}. Notice that the orthogonal projection onto $\mathcal{N}(A)$ is given by
    \begin{equation*}
        \mathbf{\Pi}_{\mathcal{N}(A)}=\mathbf{I}-\mathbf{A}^\dagger\mathbf{A}
    \end{equation*}
    
    \item Prove that $X_{\mathrm{LS}}=\{\mathbf{x}_{\mathrm{LS}}\}$ if and only if $\mathrm{rank}({\bf A})=n$.
\end{enumerate}

\noindent
\textbf{Solution.}
Please insert your solution here ...

\end{document}



