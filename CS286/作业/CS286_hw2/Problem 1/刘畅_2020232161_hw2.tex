\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage[colorlinks,linkcolor=blue]{hyperref}
\usepackage{enumerate}
\usepackage{amssymb} 
\usepackage{bm}
\topmargin -.5in
\textheight 9in
\oddsidemargin -.25in
\evensidemargin -.25in
\textwidth 6.5in
\title{CS286 homework2}
\author{
Problem 1 (20 points)\\
Instructor: Jie Zheng (SIST)}
\date{Oct,2020}

\usepackage{natbib}
\usepackage{graphicx}

\begin{document}
\maketitle
\textbf{Requirements:}
\begin{itemize}
    \item The following are theory problems, so that you don't need coding. (Of course you can coding to verify your answers). 
    \item Please write down the key derivation and calculation steps, because only the answer will not be accepted.
    \item This part we only accept solutions in \textbf{pdf} format, but do not accept figure with handwriting. If you cannot handle \LaTeX well, you can write it with \textbf{Word} first and then convert it into a \textbf{pdf} file.
    \item Please name your file in the format of \textbf{name\_student\_id\_hw2}.
\end{itemize} 
\vspace{8ex}

\begin{enumerate}
    \item \textit{Convolutional Neural Network}(10 points).
    
    \begin{itemize}
        \item[(1)] Let input $\bm{x}$ be a matrix with shape of $64\times64\times3$,\\ layer $\bm{L_1}$ with 10 $4\times4$ filters with stride=2, no padding, dilation=1;\\ 
        layer $\bm{L_2}$ with 20 $3\times3$ filters with stride=4, padding=2, dilation=1;\\
        layer $\bm{L_3}$ with 10 $3\times3$ filters with stride=2, no padding, dilation=2.\\
        Denote $\bm{x_1}=\bm{L_1(x)}$, $\bm{x_2}=\bm{L_2(x_1)}$ and  $\bm{x_3}=\bm{L_3(x_2)}$.\\
        Compute the shapes of $\bm{x_1,x_2,x_3}$ and the numbers of parameters of layer $\bm{L_1,L_2}$ and $\bm{L_3}$. You must consider about bias(5 points).
        \\\\
        \textbf{Solution}:
        According to the definition, we can get: $w_{\text {out}}=\frac{w_{\text {in}}+2 * \text { padding }-K}{\text { stride }}+1$\\
        where $K= kernel +( kernel -1) \cdot (dilation -1)$\\
        Shape of $x_1$: \\
        $\therefore w_1=31 and n_1=10$ \\
        $\therefore L_1=(4\times4\times3+1) \times 10=490$\\
        Shape of $x_2$: \\
        $\therefore w_2=9 and n_2=20$ \\
        $\therefore L_2=(3\times3\times10+1) \times 20=1820$\\
        Shape of $x_3$: \\
        $\therefore w_3=3 and n_3=10$ \\
        $\therefore L_3=(3\times3\times20+1) \times 10=1810$\\
                        
        
        
        
        \item[(2)] Given a tensor $\bm{x=}$
\begin{tabular}{|c|c|c|c|c|}
\hline
1&2&-1&0&3\\
\hline 
2&-3&6&4&1\\
\hline
1&5&-2&0&3\\
\hline
1&3&4&5&7\\
\hline
-2&-1&0&3&4\\
\hline
\end{tabular} as input,\\
layer $\bm{L_1}$ with kernel 
\begin{tabular}{|c|c|}
\hline
-1&2\\
\hline 
3&1\\
\hline
\end{tabular}, dilation=2,no padding, stride=1; \\
layer $\bm{L_2}$ with kernel 
\begin{tabular}{|c|c|}
\hline
1&2\\
\hline 
-1&3\\
\hline
\end{tabular}, dilation=1,no padding, stride=1;\\
layer $\bm{L_3}$ is a max pooling layer with kernel size $2\times2$.\\
Denote $\bm{x_1}=\bm{L_1(x)}$, $\bm{x_2}=\bm{L_2(x_1)}$ and $\bm{x_3}=\bm{L_3(x_2)}$.\\ Compute $\bm{x_1,x_2}$ and $\bm{x_3}$(5 points).
\\\\    
\textbf{Solution}:
Considering the influence of dilation, $L_1$'s kernel should be 
\begin{tabular}{|c|c|c|}
	\hline
	-1&0&2\\
	\hline 
	0&0&0\\
	\hline
	3&0&1\\
	\hline
\end{tabular}\\
$\therefore \bm {x_1}=$
\begin{tabular}{|c|c|c|}
	\hline
	-2&13&4\\
	\hline 
	17&25&15\\
	\hline
	-11&-5&12\\
	\hline
\end{tabular}\\\\
$\therefore \bm{x_2}=$
\begin{tabular}{|c|c|}
	\hline
	82&41\\
	\hline
	63&96\\
	\hline
\end{tabular}\\
$\therefore \bm {x_3}=96$ 
\end{itemize}

    
    
    
    
    \newpage
    \item \textit{Long Short Term Memory networks}(5 points). Suppose \\
    $\bm{W_f}=\left[\begin{array}{cccc}
    1&2&4&5\\
    -1&2&0&1
    \end{array}\right]$, 
    $\bm{W_i}=\left[\begin{array}{cccc}
    -1&2&0&1\\ 
    1&-2&3&-1 
    \end{array}\right]$, 
    $\bm{W_c}=\left[\begin{array}{cccc}
    1&-2&3&-1\\
    2&4&1&-3
    \end{array}\right]$, 
    $\bm{W_o}=\left[\begin{array}{cccc}
    2&4&1&-3\\
    1&2&4&5
    \end{array}\right]$;\\ 
    $\bm{b_f}=\left[\begin{array}{c}
    1\\
    -2 
    \end{array}\right]$, 
    $\bm{b_i}=\left[\begin{array}{c}
    -1\\
    1 
    \end{array}\right]$, 
    $\bm{b_c}=\left[\begin{array}{c}
    0\\
    0 
    \end{array}\right]$, 
    $\bm{b_o}=\left[\begin{array}{c}
    0.5\\
    -0.1
    \end{array}\right]$;\\
    $\bm{x_t}=\left[\begin{array}{c}
    1\\
    0 
    \end{array}\right]$, 
    $\bm{h_{t-1}}=\left[\begin{array}{c}
    0.4\\
    -0.2 
    \end{array}\right]$, 
    $\bm{c_{t-1}}=\left[\begin{array}{c}
    -0.5\\
    0.3 
    \end{array}\right]$.\\ 
    Compute $\bm{c_{t}}$ and $\bm{h_{t}}$ based on standard LSTM structure, preserving 4 decimal places in your result.\\ (\textbf{Hint:} You can refer to this \href{http://colah.github.io/posts/2015-08-Understanding-LSTMs/}{link}.)\\\\
    \textbf{Solution}:\\
    $f_t=\sigma(W_f \times [h_{t-1}^T,x_t^T]^T+b_f )=
    \sigma(
    \begin{bmatrix}
    1&2&4&5\\
    -1&2&0&1\\
    \end{bmatrix}
    \begin{bmatrix}
    0.4\\-0.2\\1\\0
    \end{bmatrix}
    +
    \begin{bmatrix}
    1\\-2
    \end{bmatrix})=\sigma(
    \begin{bmatrix}
    5\\-2.8
    \end{bmatrix}
    =\begin{bmatrix}
    0.9933\\0.0573
    \end{bmatrix}
    )$\\
    $
    i_t=\sigma(W_i\times [h_{t-1}^T,x_t^T]^T+b_i)=
    \sigma(
    \begin{bmatrix}
    -1&2&0&1\\
    1&-2&3&-1\\
    \end{bmatrix}
    \begin{bmatrix}
    0.4\\-0.2\\1\\0
    \end{bmatrix}+
    \begin{bmatrix}
    -1\\1
    \end{bmatrix}
    )=
    \sigma(
    \begin{bmatrix}
    -1.8\\4.8
    \end{bmatrix}
    )
    =
    \begin{bmatrix}
    0.1419\\0.9918
    \end{bmatrix}
    $\\
    $\tilde{C_t}=\text{tanh}(
    \begin{bmatrix}
    1&-2&3&-1\\2&4&1&-3
    \end{bmatrix}
    \begin{bmatrix}
    0.4\\-0.2\\1\\0
    \end{bmatrix}
    +
    \begin{bmatrix}
    0\\0
    \end{bmatrix}   
    )=
    \text{tanh}
    \begin{bmatrix}
    3.8\\1
    \end{bmatrix}=
    \begin{bmatrix}
	0.9990\\0.7616
	\end{bmatrix}    
    $\\
    $C_t=
    \begin{bmatrix}
    0.9933\\0.0573
    \end{bmatrix}
    \times
    \begin{bmatrix}
	-0.5\\0.3
	\end{bmatrix}
	+
	\begin{bmatrix}
	0.1419\\0.9918
	\end{bmatrix} 
	\times
	\begin{bmatrix}
	0.9990\\0.7616
	\end{bmatrix}=   
    \begin{bmatrix}
    -0.3549\\0.7725
    \end{bmatrix}$\\
    
    $
    O_t=\sigma(
    \begin{bmatrix}
    2&4&1&-3\\
    1&2&4&5\\
    \end{bmatrix}
    \begin{bmatrix}
    0.4\\-0.2\\1\\0
    \end{bmatrix}
    +
    \begin{bmatrix}
    0.5\\-0.1
    \end{bmatrix})
    =\sigma(
    \begin{bmatrix}
    1.5\\3.9
    \end{bmatrix}
    )=
	\begin{bmatrix}
	0.8176\\0.9802
	\end{bmatrix}    
    $\\
    $\therefore 
    h_t=O_t \times \text{tanh}(C_t)=
    \begin{bmatrix}
    -0.2786\\0.6356
    \end{bmatrix}
    $\\\\
    
    \item 
    \textit{Variational Autoencoder}(5 points). 
    Denote $\bm{P_1}\sim \mathcal{N}(\bm{\mu},\bm{\sigma}^2)$, $\bm{P_2}\sim \mathcal{N}(0,1)$, prove that \[\bm{KL}(\bm{P_1}||\bm{P_2})=\frac{1}{2}\left(\bm{\mu}^2+\bm{\sigma}^2-\text{log}\bm{\sigma}^2-1\right),\]
    where $\mathcal{N}()$ is the Gaussian distribution, and $\bm{KL}$ is  the function of Kullback-Leibler divergence.\\\\
	\textbf{Solution}:\\
	According to the definition:\\
	\begin{align*}
	\bm{KL}(\bm{P_1}||\bm{P_2})&=
	\int_x\dfrac{1}{\sqrt{2\pi}\sigma}e^{-\dfrac{(x-\mu)^2}{2\sigma^2}}\text{log}\dfrac{\dfrac{1}{\sqrt{2\pi}\sigma}e^{-\dfrac{(x-\mu)^2}{2\sigma^2}}}{\dfrac{1}{\sqrt{2\pi}}e^{-\dfrac{x^2}{2}}}dx\\
							   &=
	\int_x\dfrac{1}{\sqrt{2\pi}\sigma}e^{-\dfrac{(x-\mu)^2}{2\sigma^2}}\left[\text{log}\dfrac{1}{\sigma}-\dfrac{(x-\mu)^2}{2\sigma^2}+\dfrac{x^2}{2}\right]dx\\
							   &=
	\text{log}\dfrac{1}{\sigma}\int_x\dfrac{1}{\sqrt{2\pi}\sigma}e^{-\dfrac{(x-\mu)^2}{2\sigma^2}}dx-\dfrac{1}{2\sigma^2}\int_x(x-\mu)^2\dfrac{1}{\sqrt{2\pi}\sigma}e^{-\dfrac{(x-\mu)^2}{2\sigma^2}}dx+\dfrac{1}{2}\int_xx^2\dfrac{1}{\sqrt{2\pi}\sigma}e^{-\dfrac{(x-\mu)^2}{2\sigma^2}}dx\\
							   &=
	\text{log}\dfrac{1}{\sigma} -\dfrac{1}{2\sigma^2} \times D(X-\mu)+\dfrac{1}{2}EX^2\\
							   &=
	\text{log}\dfrac{1}{\sigma}-\dfrac{1}{2\sigma^2}\times\sigma^2+\dfrac{1}{2}(DX+E^2X)						   \\					   
							   &=
	\text{log}\dfrac{1}{\sigma}-\dfrac{1}{2}+\dfrac{\sigma^2+\mu^2}{2}	
	\end{align*}
	Proved.
	


\end{enumerate}

\end{document}
