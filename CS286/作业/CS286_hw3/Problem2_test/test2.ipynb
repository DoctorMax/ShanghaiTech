{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2. Reinforcement Learning for BipedalWalker-v3 (35 points)\n",
    "This problem is to design a reinforcement learning algorithm that is applied to a robot, with the objective to maximize its reward, in the following game. \n",
    "### BipedalWalker-v3\n",
    "The reward is given for moving forward, accumulating over 300 points at the end. If the robot falls, it will be penalized by deducting 100 points. Applying a motor torque costs a small amount of points. The state of the robot consists of hull angle speed, angular velocity, horizontal speed, vertical speed, positions of joints, angular speeds of joints, contact positions of legs with the ground, and 10 lidar rangefinder measurements. There is no coordinate in the state vector.  \n",
    "![Alt Text](https://media.giphy.com/media/R89toZzap04ZDKHPkd/giphy.gif)  \n",
    "This game has continuous action space. You are required to apply the Twin Delayed DDPG (TD3) method in this game.  \n",
    "### References：\n",
    "You can read [this link](https://spinningup.openai.com/en/latest/algorithms/td3.html) to understand the **TD3** algorithm better.  \n",
    "You can visit [this link](https://dllglobal.com/challenges/reinforcement-learning) to understand the **BipedalWalker-v3** environment better.  \n",
    "### Requirements：\n",
    "* All of your code should be shown in this file.\n",
    "* Your network must be based on GRU; otherwise, you will get 0 point.\n",
    "* You must save your trained model named as **best_model.pt**.\n",
    "* The RL method you need to implement is TD3; otherwise you will get 0 point.\n",
    "* Please give some comments to your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 2.1 Import the packages and define helper funcitons and variables (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     16
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "import os, sys, random\n",
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "from tensorboardX import SummaryWriter\n",
    "import tqdm.notebook as tqdm\n",
    "# 请用GPU，不然宛若智障\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "class Replay_buffer():\n",
    "    def __init__(self, max_size):\n",
    "        self.storage = []\n",
    "        self.max_size = max_size\n",
    "        self.ptr = 0\n",
    "\n",
    "    def push(self, data):\n",
    "        if len(self.storage) == self.max_size:\n",
    "            self.storage[int(self.ptr)] = data\n",
    "            self.ptr = (self.ptr + 1) % self.max_size\n",
    "        else:\n",
    "            self.storage.append(data)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
    "        x, y, u, r, d = [], [], [], [], []\n",
    "\n",
    "        for i in ind:\n",
    "            X, Y, U, R, D = self.storage[i]\n",
    "            x.append(np.array(X, copy=False))\n",
    "            y.append(np.array(Y, copy=False))\n",
    "            u.append(np.array(U, copy=False))\n",
    "            r.append(np.array(R, copy=False))\n",
    "            d.append(np.array(D, copy=False))\n",
    "\n",
    "        return np.array(x), np.array(y), np.array(u), np.array(r).reshape(-1, 1), np.array(d).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 2.2 Build your network (<font color=red>which should include GRU cells</font>) (7 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0,
     22
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        self.GRU_layer = nn.GRU(input_size=state_dim, hidden_size=state_dim ,num_layers=1)\n",
    "        self.fc1 = nn.Linear(state_dim, 400)\n",
    "        self.fc2 = nn.Linear(400, 300)\n",
    "        self.fc3 = nn.Linear(300, action_dim)\n",
    "\n",
    "        self.max_action = max_action\n",
    "\n",
    "    def forward(self, state):\n",
    "        state=state.unsqueeze(0)\n",
    "        a , self.hidden = self.GRU_layer(state)\n",
    "        a = a.squeeze(0)\n",
    "        a = F.relu(self.fc1(a))\n",
    "        a = F.relu(self.fc2(a))\n",
    "        a = torch.tanh(self.fc3(a)) * self.max_action\n",
    "        return a\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, 400)\n",
    "        self.fc2 = nn.Linear(400, 300)\n",
    "        self.fc3 = nn.Linear(300, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        state_action = torch.cat([state, action], 1)\n",
    "\n",
    "        q = F.relu(self.fc1(state_action))\n",
    "        q = F.relu(self.fc2(q))\n",
    "        q = self.fc3(q)\n",
    "        return q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Define your TD3 algorithm (8 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     1,
     25,
     93,
     104
    ]
   },
   "outputs": [],
   "source": [
    "class TD3():\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "\n",
    "        self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
    "        self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
    "        self.critic_1 = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_1_target = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_2 = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_2_target = Critic(state_dim, action_dim).to(device)\n",
    "\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters())\n",
    "        self.critic_1_optimizer = optim.Adam(self.critic_1.parameters())\n",
    "        self.critic_2_optimizer = optim.Adam(self.critic_2.parameters())\n",
    "\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        self.critic_1_target.load_state_dict(self.critic_1.state_dict())\n",
    "        self.critic_2_target.load_state_dict(self.critic_2.state_dict())\n",
    "\n",
    "        self.max_action = max_action\n",
    "        self.memory = Replay_buffer(capacity)\n",
    "        self.writer = SummaryWriter(directory)\n",
    "        self.num_critic_update_iteration = 0\n",
    "        self.num_actor_update_iteration = 0\n",
    "        self.num_training = 0\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.tensor(state.reshape(1, -1)).float().to(device)\n",
    "        return self.actor(state).cpu().data.numpy().flatten()\n",
    "\n",
    "    def update(self, num_iteration):\n",
    "\n",
    "        if self.num_training % 500 == 0:\n",
    "            print(\"====================================\")\n",
    "            print(\"模型已经训练了{}次了\".format(self.num_training))\n",
    "            print(\"====================================\")\n",
    "        for i in range(num_iteration):\n",
    "            x, y, u, r, d = self.memory.sample(batch_size)\n",
    "            state = torch.FloatTensor(x).to(device)\n",
    "            action = torch.FloatTensor(u).to(device)\n",
    "            next_state = torch.FloatTensor(y).to(device)\n",
    "            done = torch.FloatTensor(d).to(device)\n",
    "            reward = torch.FloatTensor(r).to(device)\n",
    "\n",
    "            # Select next action according to target policy:\n",
    "            noise = torch.ones_like(action).data.normal_(0, policy_noise).to(device)\n",
    "            noise = noise.clamp(-noise_clip, noise_clip)\n",
    "            next_action = (self.actor_target(next_state) + noise)\n",
    "            next_action = next_action.clamp(-self.max_action, self.max_action)\n",
    "\n",
    "            # Compute target Q-value:\n",
    "            target_Q1 = self.critic_1_target(next_state, next_action)\n",
    "            target_Q2 = self.critic_2_target(next_state, next_action)\n",
    "            target_Q = torch.min(target_Q1, target_Q2)\n",
    "            target_Q = reward + ((1 - done) * gamma * target_Q).detach()\n",
    "\n",
    "            # Optimize Critic 1:\n",
    "            current_Q1 = self.critic_1(state, action)\n",
    "            loss_Q1 = F.mse_loss(current_Q1, target_Q)\n",
    "            self.critic_1_optimizer.zero_grad()\n",
    "            loss_Q1.backward()\n",
    "            self.critic_1_optimizer.step()\n",
    "            self.writer.add_scalar('Loss/Q1_loss', loss_Q1, global_step=self.num_critic_update_iteration)\n",
    "\n",
    "            # Optimize Critic 2:\n",
    "            current_Q2 = self.critic_2(state, action)\n",
    "            loss_Q2 = F.mse_loss(current_Q2, target_Q)\n",
    "            self.critic_2_optimizer.zero_grad()\n",
    "            loss_Q2.backward()\n",
    "            self.critic_2_optimizer.step()\n",
    "            self.writer.add_scalar('Loss/Q2_loss', loss_Q2, global_step=self.num_critic_update_iteration)\n",
    "            # Delayed policy updates:\n",
    "            if i % policy_delay == 0:\n",
    "                # Compute actor loss:\n",
    "                actor_loss = - self.critic_1(state, self.actor(state)).mean()\n",
    "\n",
    "                # Optimize the actor\n",
    "                self.actor_optimizer.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                self.actor_optimizer.step()\n",
    "                self.writer.add_scalar('Loss/actor_loss', actor_loss, global_step=self.num_actor_update_iteration)\n",
    "                for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "                    target_param.data.copy_(((1- tau) * target_param.data) + tau * param.data)\n",
    "\n",
    "                for param, target_param in zip(self.critic_1.parameters(), self.critic_1_target.parameters()):\n",
    "                    target_param.data.copy_(((1 - tau) * target_param.data) + tau * param.data)\n",
    "\n",
    "                for param, target_param in zip(self.critic_2.parameters(), self.critic_2_target.parameters()):\n",
    "                    target_param.data.copy_(((1 - tau) * target_param.data) + tau * param.data)\n",
    "\n",
    "                self.num_actor_update_iteration += 1\n",
    "        self.num_critic_update_iteration += 1\n",
    "#         print(3)\n",
    "        self.num_training += 1\n",
    "\n",
    "    def save(self):\n",
    "        torch.save(self.actor.state_dict(), directory+'actor.pth')\n",
    "        torch.save(self.actor_target.state_dict(), directory+'actor_target.pth')\n",
    "        torch.save(self.critic_1.state_dict(), directory+'critic_1.pth')\n",
    "        torch.save(self.critic_1_target.state_dict(), directory+'critic_1_target.pth')\n",
    "        torch.save(self.critic_2.state_dict(), directory+'critic_2.pth')\n",
    "        torch.save(self.critic_2_target.state_dict(), directory+'critic_2_target.pth')\n",
    "        print(\"====================================\")\n",
    "        print(\"模型已经保存了\")\n",
    "        print(\"====================================\")\n",
    "\n",
    "    def load(self):\n",
    "        self.actor.load_state_dict(torch.load(directory + 'actor.pth'))\n",
    "        self.actor_target.load_state_dict(torch.load(directory + 'actor_target.pth'))\n",
    "        self.critic_1.load_state_dict(torch.load(directory + 'critic_1.pth'))\n",
    "        self.critic_1_target.load_state_dict(torch.load(directory + 'critic_1_target.pth'))\n",
    "        self.critic_2.load_state_dict(torch.load(directory + 'critic_2.pth'))\n",
    "        self.critic_2_target.load_state_dict(torch.load(directory + 'critic_2_target.pth'))\n",
    "        print(\"====================================\")\n",
    "        print(\"模型已经加载了\")\n",
    "        print(\"====================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Define your training process and train your model (5 points)  \n",
    "You must use some data structures to collect the mean reward and mean loss in each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     32
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l:\\anaconda3\\envs\\cs286\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================\n",
      "Collection Experience...\n",
      "====================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba4f6d7d477c43c4a9d46d0958431476",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================\n",
      "模型已经训练了0次了\n",
      "====================================\n",
      "Epoch 0, the total_reward is \t-93.66, the step is \t91\n",
      "====================================\n",
      "模型已经保存了\n",
      "====================================\n",
      "Epoch 100, the total_reward is \t-113.28, the step is \t92\n",
      "====================================\n",
      "模型已经保存了\n",
      "====================================\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-3dc7608ac3c9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[0mtotal_reward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrender\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mrender_interval\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m             \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;31m#         if i+1 % 10 == 0:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32ml:\\anaconda3\\envs\\cs286\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode, **kwargs)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'human'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 240\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32ml:\\anaconda3\\envs\\cs286\\lib\\site-packages\\gym\\envs\\box2d\\bipedal_walker.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    494\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw_polyline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinewidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 496\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'rgb_array'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    497\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32ml:\\anaconda3\\envs\\cs286\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, return_rgb_array)\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 127\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0monetime_geoms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mreturn_rgb_array\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misopen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32ml:\\anaconda3\\envs\\cs286\\lib\\site-packages\\pyglet\\window\\win32\\__init__.py\u001b[0m in \u001b[0;36mflip\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    334\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_always_dwm\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dwm_composition_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_interval\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 336\u001b[1;33m                     \u001b[0m_dwmapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDwmFlush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 设置各种参数\n",
    "mode='train'\n",
    "env_name='BipedalWalker-v3'\n",
    "tau=0.005 # target smoothing coefficient\n",
    "target_update_interval=1\n",
    "test_iteration=10 #测试次数\n",
    "learning_rate=3e-4 #学习率\n",
    "gamma=0.99 # discounted factor\n",
    "capacity=5000 # replay buffer size\n",
    "num_iteration=10000 #  num of  games\n",
    "batch_size=100 # mini batch size\n",
    "seed=False\n",
    "random_seed=9527 #随机种子\n",
    "# optional parameters\n",
    "num_hidden_layers=2\n",
    "sample_frequency=256\n",
    "# render=False # show UI or not\n",
    "render=True # show UI or not\n",
    "log_interval=100 #\n",
    "load=False # load model\n",
    "render_interval=100 # after render_interval, the env.render() will work\n",
    "policy_noise=0.2\n",
    "noise_clip=0.5\n",
    "policy_delay=2\n",
    "exploration_noise=0.1\n",
    "max_episode=2000\n",
    "print_log=100\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "env = gym.make(env_name)\n",
    "if seed:\n",
    "    env.seed(random_seed)\n",
    "    torch.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "max_action = float(env.action_space.high[0])\n",
    "min_Val = torch.tensor(1e-7).float().to(device) # min value\n",
    "\n",
    "directory = './exp' + env_name +'./'\n",
    "\n",
    "\n",
    "agent = TD3(state_dim, action_dim, max_action)\n",
    "total_reward = 0 # 奖励/评分\n",
    "\n",
    "\n",
    "\n",
    "print(\"====================================\")\n",
    "print(\"Collection Experience...\")\n",
    "print(\"====================================\")\n",
    "# if load: agent.load()\n",
    "for i in tqdm.tqdm(range(num_iteration)):\n",
    "    state = env.reset() #初始化环境\n",
    "#     print(1)#############\n",
    "    for t in range(2000): #最多尝试2000次\n",
    "#         print(2)###############\n",
    "\n",
    "        action = agent.select_action(state)\n",
    "        action = action + np.random.normal(0, exploration_noise, size=env.action_space.shape[0])\n",
    "        action = action.clip(env.action_space.low, env.action_space.high)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        if render and i >= render_interval:\n",
    "            env.render()\n",
    "        agent.memory.push((state, next_state, action, reward, np.float(done)))\n",
    "#         if i+1 % 10 == 0:\n",
    "#             print('Episode {},  The memory size is {} '.format(i, len(agent.memory.storage)))\n",
    "#         if len(agent.memory.storage) >= capacity-1:\n",
    "#             agent.update(10)\n",
    "\n",
    "        state = next_state\n",
    "        if done or t == max_episode -1:\n",
    "            agent.update(10)\n",
    "            agent.writer.add_scalar('total_reward', total_reward, global_step=i)\n",
    "            if i % print_log == 0:\n",
    "                print(\"Epoch {}, the total_reward is \\t{:0.2f}, the step is \\t{}\".format(i, total_reward, t))\n",
    "            total_reward = 0\n",
    "            break\n",
    "\n",
    "    if i % log_interval == 0:\n",
    "        agent.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Show your change curves of reward and loss in two sub-pictures (5 points)\n",
    "Your reward and loss must be able to converge; otherwise this part will get 0 point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Test your model 10 times and print the mean reward (5 points) \n",
    "You should load your trained model **best_model.pt**. If your mean reward is >= 200 but < 250, you will get 3 points. If your mean reward is >= 250, you will get 5 points. If your mean reward is < 200, you will get 0 point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.load()\n",
    "for i in tqdm.tqdm(range(test_iteration)):\n",
    "    state = env.reset()\n",
    "    for t in count():\n",
    "        action = agent.select_action(state)\n",
    "        next_state, reward, done, info = env.step(np.float32(action))\n",
    "        total_reward += reward\n",
    "        env.render()\n",
    "        if done or t ==2000 :\n",
    "            print(\"Ep_i \\t{}, the total_reward is \\t{:0.2f}, the step is \\t{}\".format(i, total_reward, t))\n",
    "            total_reward = 0\n",
    "            break\n",
    "        state = next_state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
