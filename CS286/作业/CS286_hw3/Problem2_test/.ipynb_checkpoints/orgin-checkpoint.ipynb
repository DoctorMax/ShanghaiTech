{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2. Reinforcement Learning for BipedalWalker-v3 (35 points)\n",
    "This problem is to design a reinforcement learning algorithm that is applied to a robot, with the objective to maximize its reward, in the following game. \n",
    "### BipedalWalker-v3\n",
    "The reward is given for moving forward, accumulating over 300 points at the end. If the robot falls, it will be penalized by deducting 100 points. Applying a motor torque costs a small amount of points. The state of the robot consists of hull angle speed, angular velocity, horizontal speed, vertical speed, positions of joints, angular speeds of joints, contact positions of legs with the ground, and 10 lidar rangefinder measurements. There is no coordinate in the state vector.  \n",
    "![Alt Text](https://media.giphy.com/media/R89toZzap04ZDKHPkd/giphy.gif)  \n",
    "This game has continuous action space. You are required to apply the Twin Delayed DDPG (TD3) method in this game.  \n",
    "### References：\n",
    "You can read [this link](https://spinningup.openai.com/en/latest/algorithms/td3.html) to understand the **TD3** algorithm better.  \n",
    "You can visit [this link](https://dllglobal.com/challenges/reinforcement-learning) to understand the **BipedalWalker-v3** environment better.  \n",
    "### Requirements：\n",
    "* All of your code should be shown in this file.\n",
    "* Your network must be based on GRU; otherwise, you will get 0 point.\n",
    "* You must save your trained model named as **best_model.pt**.\n",
    "* The RL method you need to implement is TD3; otherwise you will get 0 point.\n",
    "* Please give some comments to your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Import the packages and define helper funcitons and variables (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import tqdm.notebook as tn\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size=5e5):\n",
    "        self.buffer = []\n",
    "        self.max_size = int(max_size)\n",
    "        self.size = 0\n",
    "    \n",
    "    def add(self, transition):\n",
    "        self.size +=1\n",
    "        # transiton is tuple of (state, action, reward, next_state, done)\n",
    "        self.buffer.append(transition)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        # delete 1/5th of the buffer when full\n",
    "        if self.size > self.max_size:\n",
    "            del self.buffer[0:int(self.size/5)]\n",
    "            self.size = len(self.buffer)\n",
    "        \n",
    "        indexes = np.random.randint(0, len(self.buffer), size=batch_size)\n",
    "        state, action, reward, next_state, done = [], [], [], [], []\n",
    "        \n",
    "        for i in indexes:\n",
    "            s, a, r, s_, d = self.buffer[i]\n",
    "            state.append(np.array(s, copy=False))\n",
    "            action.append(np.array(a, copy=False))\n",
    "            reward.append(np.array(r, copy=False))\n",
    "            next_state.append(np.array(s_, copy=False))\n",
    "            done.append(np.array(d, copy=False))\n",
    "        \n",
    "        return np.array(state), np.array(action), np.array(reward), np.array(next_state), np.array(done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Build your network (<font color=red>which should include GRU cells</font>) (7 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        self.l1 = nn.Linear(state_dim, 480)\n",
    "        self.l2 = nn.Linear(480, 240)\n",
    "        self.l3 = nn.Linear(240, action_dim)\n",
    "\n",
    "        self.max_action = max_action\n",
    "\n",
    "    def forward(self, state):\n",
    "        a = F.relu(self.l1(state))\n",
    "        a = F.relu(self.l2(a))\n",
    "        a = torch.tanh(self.l3(a)) * self.max_action\n",
    "        return a\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        self.l1 = nn.Linear(state_dim + action_dim, 560)\n",
    "        self.l2 = nn.Linear(560, 280)\n",
    "        self.l3 = nn.Linear(280, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        state_action = torch.cat([state, action], 1)\n",
    "\n",
    "        q = F.relu(self.l1(state_action))\n",
    "        q = F.relu(self.l2(q))\n",
    "        q = self.l3(q)\n",
    "        return q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Define your TD3 algorithm (8 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     91,
     101,
     112
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class TD3:\n",
    "    def __init__(self, lr, state_dim, action_dim, max_action):\n",
    "        \n",
    "        self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
    "        self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr)\n",
    "        \n",
    "        self.critic_1 = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_1_target = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_1_target.load_state_dict(self.critic_1.state_dict())\n",
    "        self.critic_1_optimizer = optim.Adam(self.critic_1.parameters(), lr=lr)\n",
    "        \n",
    "        self.critic_2 = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_2_target = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_2_target.load_state_dict(self.critic_2.state_dict())\n",
    "        self.critic_2_optimizer = optim.Adam(self.critic_2.parameters(), lr=lr)\n",
    "        \n",
    "        self.max_action = max_action\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
    "        return self.actor(state).cpu().data.numpy().flatten()\n",
    "    \n",
    "    def update(self, replay_buffer, n_iter, batch_size, gamma, polyak, policy_noise, noise_clip, policy_delay):\n",
    "        \n",
    "        for i in range(n_iter):\n",
    "            # Sample a batch of transitions from replay buffer:\n",
    "            state, action_, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            action = torch.FloatTensor(action_).to(device)\n",
    "            reward = torch.FloatTensor(reward).reshape((batch_size,1)).to(device)\n",
    "            next_state = torch.FloatTensor(next_state).to(device)\n",
    "            done = torch.FloatTensor(done).reshape((batch_size,1)).to(device)\n",
    "            \n",
    "            # Select next action according to target policy:\n",
    "            noise = torch.FloatTensor(action_).data.normal_(0, policy_noise).to(device)\n",
    "            noise = noise.clamp(-noise_clip, noise_clip)\n",
    "            next_action = (self.actor_target(next_state) + noise)\n",
    "            next_action = next_action.clamp(-self.max_action, self.max_action)\n",
    "            \n",
    "            # Compute target Q-value:\n",
    "            target_Q1 = self.critic_1_target(next_state, next_action)\n",
    "            target_Q2 = self.critic_2_target(next_state, next_action)\n",
    "            target_Q = torch.min(target_Q1, target_Q2)\n",
    "            target_Q = reward + ((1-done) * gamma * target_Q).detach()\n",
    "            \n",
    "            # Optimize Critic 1:\n",
    "            current_Q1 = self.critic_1(state, action)\n",
    "            loss_Q1 = F.mse_loss(current_Q1, target_Q)\n",
    "            self.critic_1_optimizer.zero_grad()\n",
    "            loss_Q1.backward()\n",
    "            self.critic_1_optimizer.step()\n",
    "            \n",
    "            # Optimize Critic 2:\n",
    "            current_Q2 = self.critic_2(state, action)\n",
    "            loss_Q2 = F.mse_loss(current_Q2, target_Q)\n",
    "            self.critic_2_optimizer.zero_grad()\n",
    "            loss_Q2.backward()\n",
    "            self.critic_2_optimizer.step()\n",
    "            \n",
    "            # Delayed policy updates:\n",
    "            if i % policy_delay == 0:\n",
    "                # Compute actor loss:\n",
    "                actor_loss = -self.critic_1(state, self.actor(state)).mean()\n",
    "                \n",
    "                # Optimize the actor\n",
    "                self.actor_optimizer.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                self.actor_optimizer.step()\n",
    "                \n",
    "                # Polyak averaging update:\n",
    "                for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "                    target_param.data.copy_( (polyak * target_param.data) + ((1-polyak) * param.data))\n",
    "                \n",
    "                for param, target_param in zip(self.critic_1.parameters(), self.critic_1_target.parameters()):\n",
    "                    target_param.data.copy_( (polyak * target_param.data) + ((1-polyak) * param.data))\n",
    "                \n",
    "                for param, target_param in zip(self.critic_2.parameters(), self.critic_2_target.parameters()):\n",
    "                    target_param.data.copy_( (polyak * target_param.data) + ((1-polyak) * param.data))\n",
    "        return actor_loss.cpu().data.numpy(), loss_Q1.cpu().data.numpy(), loss_Q2.cpu().data.numpy()\n",
    "                \n",
    "    def save(self, directory, name, ep):\n",
    "        torch.save(self.actor.state_dict(), '%s/%s_actor_ep%s.pth' % (directory, name, ep))\n",
    "        torch.save(self.actor_target.state_dict(), '%s/%s_actor_target_ep%s.pth' % (directory, name, ep))\n",
    "        \n",
    "        torch.save(self.critic_1.state_dict(), '%s/%s_crtic_1_ep%s.pth' % (directory, name, ep))\n",
    "        torch.save(self.critic_1_target.state_dict(), '%s/%s_critic_1_target_ep%s.pth' % (directory, name, ep))\n",
    "        \n",
    "        torch.save(self.critic_2.state_dict(), '%s/%s_crtic_2_ep%s.pth' % (directory, name, ep))\n",
    "        torch.save(self.critic_2_target.state_dict(), '%s/%s_critic_2_target_ep%s.pth' % (directory, name, ep))\n",
    "        \n",
    "    def load(self, directory, name, ep):\n",
    "        self.actor.load_state_dict(torch.load('%s/%s_actor_ep%s.pth' % (directory, name, ep), map_location=lambda storage, loc: storage))\n",
    "        self.actor_target.load_state_dict(torch.load('%s/%s_actor_target_ep%s.pth' % (directory, name, ep), map_location=lambda storage, loc: storage))\n",
    "        \n",
    "        self.critic_1.load_state_dict(torch.load('%s/%s_crtic_1_ep%s.pth' % (directory, name, ep), map_location=lambda storage, loc: storage))\n",
    "        self.critic_1_target.load_state_dict(torch.load('%s/%s_critic_1_target_ep%s.pth' % (directory, name, ep), map_location=lambda storage, loc: storage))\n",
    "        \n",
    "        self.critic_2.load_state_dict(torch.load('%s/%s_crtic_2_ep%s.pth' % (directory, name, ep), map_location=lambda storage, loc: storage))\n",
    "        self.critic_2_target.load_state_dict(torch.load('%s/%s_critic_2_target_ep%s.pth' % (directory, name, ep), map_location=lambda storage, loc: storage))\n",
    "        \n",
    "        \n",
    "    def load_actor(self, directory, name, ep):\n",
    "        self.actor.load_state_dict(torch.load('%s/%s_actor_ep%s.pth' % (directory, name, ep), map_location=lambda storage, loc: storage))\n",
    "        self.actor_target.load_state_dict(torch.load('%s/%s_actor_target_ep%s.pth' % (directory, name, ep), map_location=lambda storage, loc: storage))\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Define your training process and train your model (5 points)  \n",
    "You must use some data structures to collect the mean reward and mean loss in each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     37,
     44
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b9612ab0374456bb51515a46f38293c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep_begin\n",
      "reward_begin 0.003001689910888672\n",
      "reward_it 121\n",
      "reward_end 0.9245147705078125\n",
      "Episode: 1/10000,\tScore: -112.88,\tDistance: -12.88,\tactor_loss: -2.340635299682617,\tc1_loss:0.6948822140693665,\tc2_loss:4.519143104553223ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.0029916763305664062\n",
      "reward_it 1599\n",
      "reward_end 9.786808490753174\n",
      "Episode: 2/10000,\tScore: -180.67,\tDistance: -180.67,\tactor_loss: 0.07501813769340515,\tc1_loss:0.004702025558799505,\tc2_loss:0.00921682734042406ep_end 0.0010237693786621094\n",
      "ep_begin\n",
      "reward_begin 0.003990888595581055\n",
      "reward_it 1599\n",
      "reward_end 9.837672472000122\n",
      "Episode: 3/10000,\tScore: -176.51,\tDistance: -176.51,\tactor_loss: 0.8388140797615051,\tc1_loss:0.001285223988816142,\tc2_loss:0.0005672585684806108ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.003988504409790039\n",
      "reward_it 1599\n",
      "reward_end 9.922504901885986\n",
      "Episode: 4/10000,\tScore: -173.49,\tDistance: -173.49,\tactor_loss: 1.1490217447280884,\tc1_loss:0.0015933641698211432,\tc2_loss:0.0012668806593865156ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.0029757022857666016\n",
      "reward_it 1599\n",
      "reward_end 9.822739601135254\n",
      "Episode: 5/10000,\tScore: -173.39,\tDistance: -173.39,\tactor_loss: 1.6123967170715332,\tc1_loss:0.0029079641681164503,\tc2_loss:0.0009313015616498888ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.002992868423461914\n",
      "reward_it 1599\n",
      "reward_end 9.876596212387085\n",
      "Episode: 6/10000,\tScore: -173.95,\tDistance: -173.95,\tactor_loss: 1.8949979543685913,\tc1_loss:0.0007034649606794119,\tc2_loss:0.0012160310288891196ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.003989458084106445\n",
      "reward_it 1599\n",
      "reward_end 9.841662406921387\n",
      "Episode: 7/10000,\tScore: -159.01,\tDistance: -159.01,\tactor_loss: 2.4383904933929443,\tc1_loss:0.00307060475461185,\tc2_loss:0.006370824296027422ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.003960132598876953\n",
      "reward_it 1599\n",
      "reward_end 9.830747604370117\n",
      "Episode: 8/10000,\tScore: -148.62,\tDistance: -148.62,\tactor_loss: 2.5728917121887207,\tc1_loss:0.004653569310903549,\tc2_loss:0.005284181330353022ep_end 0.0009970664978027344\n",
      "ep_begin\n",
      "reward_begin 0.003994464874267578\n",
      "reward_it 113\n",
      "reward_end 0.7100687026977539\n",
      "Episode: 9/10000,\tScore: -128.68,\tDistance: -28.68,\tactor_loss: 2.7651662826538086,\tc1_loss:0.0015339922392740846,\tc2_loss:0.0024102882016450167ep_end 0.0010242462158203125\n",
      "ep_begin\n",
      "reward_begin 0.00399327278137207\n",
      "reward_it 1599\n",
      "reward_end 9.843682527542114\n",
      "Episode: 10/10000,\tScore: -148.64,\tDistance: -148.64,\tactor_loss: 2.55440354347229,\tc1_loss:0.01066740695387125,\tc2_loss:0.014029056765139103ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.0029926300048828125\n",
      "reward_it 1599\n",
      "reward_end 9.8735990524292\n",
      "Episode: 11/10000,\tScore: -164.14,\tDistance: -164.14,\tactor_loss: 2.8199236392974854,\tc1_loss:0.011544885113835335,\tc2_loss:0.011857136152684689ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.0029947757720947266\n",
      "reward_it 1599\n",
      "reward_end 10.045114755630493\n",
      "Episode: 12/10000,\tScore: -171.57,\tDistance: -171.57,\tactor_loss: 3.1023473739624023,\tc1_loss:0.018248774111270905,\tc2_loss:0.042496416717767715ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.003020048141479492\n",
      "reward_it 1599\n",
      "reward_end 9.905531406402588\n",
      "Episode: 13/10000,\tScore: -141.14,\tDistance: -141.14,\tactor_loss: 3.496433734893799,\tc1_loss:0.029415806755423546,\tc2_loss:0.036231379956007004ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.0029621124267578125\n",
      "reward_it 1599\n",
      "reward_end 9.941457986831665\n",
      "Episode: 14/10000,\tScore: -147.32,\tDistance: -147.32,\tactor_loss: 3.8726274967193604,\tc1_loss:29.040143966674805,\tc2_loss:3.5812325477600098ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.0029921531677246094\n",
      "reward_it 41\n",
      "reward_end 0.2573108673095703\n",
      "Episode: 15/10000,\tScore: -114.10,\tDistance: -14.10,\tactor_loss: 4.032626152038574,\tc1_loss:0.64202481508255,\tc2_loss:0.5813552737236023ep_end 0.000993490219116211\n",
      "ep_begin\n",
      "reward_begin 0.0029964447021484375\n",
      "reward_it 1599\n",
      "reward_end 9.993256330490112\n",
      "Episode: 16/10000,\tScore: -178.91,\tDistance: -178.91,\tactor_loss: 3.7369189262390137,\tc1_loss:0.07088613510131836,\tc2_loss:0.055735472589731216ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.003026723861694336\n",
      "reward_it 1599\n",
      "reward_end 9.917480945587158\n",
      "Episode: 17/10000,\tScore: -113.01,\tDistance: -113.01,\tactor_loss: 4.5447564125061035,\tc1_loss:0.9767699241638184,\tc2_loss:1.2096155881881714ep_end 0.001001596450805664\n",
      "ep_begin\n",
      "reward_begin 0.003992319107055664\n",
      "reward_it 130\n",
      "reward_end 0.8028457164764404\n",
      "Episode: 18/10000,\tScore: -131.93,\tDistance: -31.93,\tactor_loss: 4.388971328735352,\tc1_loss:0.0830630362033844,\tc2_loss:0.08617429435253143ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.0029878616333007812\n",
      "reward_it 1599\n",
      "reward_end 9.904529571533203\n",
      "Episode: 19/10000,\tScore: -155.31,\tDistance: -155.31,\tactor_loss: 4.5676398277282715,\tc1_loss:0.7379544377326965,\tc2_loss:0.7922917604446411ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.0029892921447753906\n",
      "reward_it 67\n",
      "reward_end 0.4188556671142578\n",
      "Episode: 20/10000,\tScore: -120.13,\tDistance: -20.13,\tactor_loss: 4.464268207550049,\tc1_loss:0.12956871092319489,\tc2_loss:0.12264129519462585ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.0030002593994140625\n",
      "reward_it 58\n",
      "reward_end 0.36100268363952637\n",
      "Episode: 21/10000,\tScore: -116.27,\tDistance: -16.27,\tactor_loss: 4.882669925689697,\tc1_loss:0.9135276675224304,\tc2_loss:0.2561170160770416ep_end 0.0010259151458740234\n",
      "ep_begin\n",
      "reward_begin 0.0039882659912109375\n",
      "reward_it 738\n",
      "reward_end 4.573745250701904\n",
      "Episode: 22/10000,\tScore: -190.50,\tDistance: -90.50,\tactor_loss: 5.069392204284668,\tc1_loss:0.0486082062125206,\tc2_loss:0.05148055776953697ep_end 0.0010280609130859375\n",
      "ep_begin\n",
      "reward_begin 0.003991127014160156\n",
      "reward_it 444\n",
      "reward_end 2.7217214107513428\n",
      "Episode: 23/10000,\tScore: -164.18,\tDistance: -64.18,\tactor_loss: 5.504586696624756,\tc1_loss:0.38578540086746216,\tc2_loss:0.3653741776943207ep_end 0.0009946823120117188\n",
      "ep_begin\n",
      "reward_begin 0.003992557525634766\n",
      "reward_it 1599\n",
      "reward_end 9.869614362716675\n",
      "Episode: 24/10000,\tScore: -143.46,\tDistance: -143.46,\tactor_loss: 5.400016784667969,\tc1_loss:0.0785783976316452,\tc2_loss:0.06871607899665833ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.003987550735473633\n",
      "reward_it 39\n",
      "reward_end 0.24434447288513184\n",
      "Episode: 25/10000,\tScore: -116.11,\tDistance: -16.11,\tactor_loss: 5.953237056732178,\tc1_loss:0.5302148461341858,\tc2_loss:0.496383398771286ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.0029942989349365234\n",
      "reward_it 165\n",
      "reward_end 1.0222678184509277\n",
      "Episode: 26/10000,\tScore: -113.68,\tDistance: -13.68,\tactor_loss: 6.163129806518555,\tc1_loss:0.032337818294763565,\tc2_loss:0.029067425057291985ep_end 0.0009968280792236328\n",
      "ep_begin\n",
      "reward_begin 0.003987312316894531\n",
      "reward_it 1599\n",
      "reward_end 9.861200332641602\n",
      "Episode: 27/10000,\tScore: -142.48,\tDistance: -142.48,\tactor_loss: 5.950898170471191,\tc1_loss:0.21417483687400818,\tc2_loss:0.12884248793125153ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.002962827682495117\n",
      "reward_it 53\n",
      "reward_end 0.3311142921447754\n",
      "Episode: 28/10000,\tScore: -114.43,\tDistance: -14.43,\tactor_loss: 6.5624680519104,\tc1_loss:0.3917109966278076,\tc2_loss:0.367180734872818ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.003983974456787109\n",
      "reward_it 159\n",
      "reward_end 0.9883627891540527\n",
      "Episode: 29/10000,\tScore: -134.30,\tDistance: -34.30,\tactor_loss: 5.856565952301025,\tc1_loss:0.14553336799144745,\tc2_loss:0.0556938573718071ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.0029659271240234375\n",
      "reward_it 129\n",
      "reward_end 0.8018884658813477\n",
      "Episode: 30/10000,\tScore: -130.40,\tDistance: -30.40,\tactor_loss: 5.801743030548096,\tc1_loss:0.106105737388134,\tc2_loss:0.14017221331596375ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.00299072265625\n",
      "reward_it 1599\n",
      "reward_end 9.890560150146484\n",
      "Episode: 31/10000,\tScore: -151.19,\tDistance: -151.19,\tactor_loss: 5.896514892578125,\tc1_loss:0.15645752847194672,\tc2_loss:0.09216639399528503ep_end 0.0010097026824951172\n",
      "ep_begin\n",
      "reward_begin 0.003976345062255859\n",
      "reward_it 142\n",
      "reward_end 0.8916158676147461\n",
      "Episode: 32/10000,\tScore: -137.39,\tDistance: -37.39,\tactor_loss: 6.468381404876709,\tc1_loss:0.12157128751277924,\tc2_loss:0.08478765189647675ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.0029926300048828125\n",
      "reward_it 266\n",
      "reward_end 1.662559986114502\n",
      "Episode: 33/10000,\tScore: -123.29,\tDistance: -23.29,\tactor_loss: 7.227894306182861,\tc1_loss:0.16628286242485046,\tc2_loss:0.32182958722114563ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.0029926300048828125\n",
      "reward_it 1599\n",
      "reward_end 9.913500785827637\n",
      "Episode: 34/10000,\tScore: -115.04,\tDistance: -115.04,\tactor_loss: 6.845540523529053,\tc1_loss:0.18553781509399414,\tc2_loss:0.12244441360235214ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.003958463668823242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward_it 133\n",
      "reward_end 0.8347682952880859\n",
      "Episode: 35/10000,\tScore: -129.75,\tDistance: -29.75,\tactor_loss: 7.739328384399414,\tc1_loss:0.2765083312988281,\tc2_loss:0.39342716336250305ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.003033876419067383\n",
      "reward_it 71\n",
      "reward_end 0.4517793655395508\n",
      "Episode: 36/10000,\tScore: -124.75,\tDistance: -24.75,\tactor_loss: 6.861245632171631,\tc1_loss:0.06528101861476898,\tc2_loss:0.07166662812232971ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.0029904842376708984\n",
      "reward_it 72\n",
      "reward_end 0.4557533264160156\n",
      "Episode: 37/10000,\tScore: -123.97,\tDistance: -23.97,\tactor_loss: 7.715202331542969,\tc1_loss:0.29580408334732056,\tc2_loss:0.07673881947994232ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.003990650177001953\n",
      "reward_it 1599\n",
      "reward_end 9.992337465286255\n",
      "Episode: 38/10000,\tScore: -157.75,\tDistance: -157.75,\tactor_loss: 6.939124584197998,\tc1_loss:0.43618956208229065,\tc2_loss:0.4712708294391632ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.0030264854431152344\n",
      "reward_it 86\n",
      "reward_end 0.5445375442504883\n",
      "Episode: 39/10000,\tScore: -127.25,\tDistance: -27.25,\tactor_loss: 9.38619613647461,\tc1_loss:0.13949474692344666,\tc2_loss:0.14601929485797882ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.0029921531677246094\n",
      "reward_it 140\n",
      "reward_end 0.9035892486572266\n",
      "Episode: 40/10000,\tScore: -131.63,\tDistance: -31.63,\tactor_loss: 8.055791854858398,\tc1_loss:0.15247537195682526,\tc2_loss:0.10583080351352692ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.0029802322387695312\n",
      "reward_it 136\n",
      "reward_end 0.873640775680542\n",
      "Episode: 41/10000,\tScore: -130.22,\tDistance: -30.22,\tactor_loss: 7.062786102294922,\tc1_loss:0.4115053415298462,\tc2_loss:0.49128586053848267ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.003957271575927734\n",
      "reward_it 42\n",
      "reward_end 0.26631593704223633\n",
      "Episode: 42/10000,\tScore: -119.34,\tDistance: -19.34,\tactor_loss: 7.345840930938721,\tc1_loss:0.37027212977409363,\tc2_loss:0.33870959281921387ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.004019260406494141\n",
      "reward_it 38\n",
      "reward_end 0.23836088180541992\n",
      "Episode: 43/10000,\tScore: -110.64,\tDistance: -10.64,\tactor_loss: 7.388984203338623,\tc1_loss:2.673961639404297,\tc2_loss:3.094782590866089ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.003988742828369141\n",
      "reward_it 87\n",
      "reward_end 0.5465424060821533\n",
      "Episode: 44/10000,\tScore: -130.43,\tDistance: -30.43,\tactor_loss: 8.55051040649414,\tc1_loss:0.5619269609451294,\tc2_loss:0.4318217635154724ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.0029976367950439453\n",
      "reward_it 105\n",
      "reward_end 0.650252103805542\n",
      "Episode: 45/10000,\tScore: -133.75,\tDistance: -33.75,\tactor_loss: 8.355170249938965,\tc1_loss:0.8040884137153625,\tc2_loss:0.5678247809410095ep_end 0.0010018348693847656\n",
      "ep_begin\n",
      "reward_begin 0.0029914379119873047\n",
      "reward_it 40\n",
      "reward_end 0.24933433532714844\n",
      "Episode: 46/10000,\tScore: -113.73,\tDistance: -13.73,\tactor_loss: 8.073722839355469,\tc1_loss:0.33823058009147644,\tc2_loss:0.30338215827941895ep_end 0.000997781753540039\n",
      "ep_begin\n",
      "reward_begin 0.003988742828369141\n",
      "reward_it 58\n",
      "reward_end 0.3590397834777832\n",
      "Episode: 47/10000,\tScore: -119.43,\tDistance: -19.43,\tactor_loss: 9.685317993164062,\tc1_loss:0.8139463663101196,\tc2_loss:1.0412909984588623ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.002987384796142578\n",
      "reward_it 57\n",
      "reward_end 0.35704636573791504\n",
      "Episode: 48/10000,\tScore: -123.10,\tDistance: -23.10,\tactor_loss: 7.135451316833496,\tc1_loss:1.499123215675354,\tc2_loss:1.3384652137756348ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.0029926300048828125\n",
      "reward_it 47\n",
      "reward_end 0.2982029914855957\n",
      "Episode: 49/10000,\tScore: -114.28,\tDistance: -14.28,\tactor_loss: 8.53203296661377,\tc1_loss:0.27458369731903076,\tc2_loss:0.2812991738319397ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.0029926300048828125\n",
      "reward_it 41\n",
      "reward_end 0.258312463760376\n",
      "Episode: 50/10000,\tScore: -111.63,\tDistance: -11.63,\tactor_loss: 9.496048927307129,\tc1_loss:0.3862491250038147,\tc2_loss:0.6076248288154602ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.0030219554901123047\n",
      "reward_it 166\n",
      "reward_end 1.0252289772033691\n",
      "Episode: 51/10000,\tScore: -115.54,\tDistance: -15.54,\tactor_loss: 9.054280281066895,\tc1_loss:0.29877588152885437,\tc2_loss:0.39408576488494873ep_end 0.0010254383087158203\n",
      "ep_begin\n",
      "reward_begin 0.0029909610748291016\n",
      "reward_it 58\n",
      "reward_end 0.3620319366455078\n",
      "Episode: 52/10000,\tScore: -110.72,\tDistance: -10.72,\tactor_loss: 10.181966781616211,\tc1_loss:0.3659549951553345,\tc2_loss:0.3234027028083801ep_end 0.0009980201721191406\n",
      "ep_begin\n",
      "reward_begin 0.003991127014160156\n",
      "reward_it 1599\n",
      "reward_end 9.9164879322052\n",
      "Episode: 53/10000,\tScore: -132.06,\tDistance: -132.06,\tactor_loss: 8.846631050109863,\tc1_loss:0.22879523038864136,\tc2_loss:0.2730734944343567ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.0029916763305664062\n",
      "reward_it 40\n",
      "reward_end 0.2593402862548828\n",
      "Episode: 54/10000,\tScore: -116.49,\tDistance: -16.49,\tactor_loss: 7.792063236236572,\tc1_loss:0.3329037129878998,\tc2_loss:0.1815117597579956ep_end 0.0009930133819580078\n",
      "ep_begin\n",
      "reward_begin 0.0039904117584228516\n",
      "reward_it 56\n",
      "reward_end 0.35504841804504395\n",
      "Episode: 55/10000,\tScore: -108.86,\tDistance: -8.86,\tactor_loss: 9.828550338745117,\tc1_loss:0.39674243330955505,\tc2_loss:0.2707728445529938ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.0029904842376708984\n",
      "reward_it 72\n",
      "reward_end 0.44979310035705566\n",
      "Episode: 56/10000,\tScore: -109.00,\tDistance: -9.00,\tactor_loss: 10.328023910522461,\tc1_loss:0.26964694261550903,\tc2_loss:0.3396855890750885ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.002994060516357422\n",
      "reward_it 42\n",
      "reward_end 0.2672572135925293\n",
      "Episode: 57/10000,\tScore: -111.15,\tDistance: -11.15,\tactor_loss: 9.271668434143066,\tc1_loss:0.3329206705093384,\tc2_loss:0.22156013548374176ep_end 0.0010268688201904297\n",
      "ep_begin\n",
      "reward_begin 0.00395965576171875\n",
      "reward_it 67\n",
      "reward_end 0.4248924255371094\n",
      "Episode: 58/10000,\tScore: -105.56,\tDistance: -5.56,\tactor_loss: 9.775793075561523,\tc1_loss:0.5488557815551758,\tc2_loss:0.5915160179138184ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.003962278366088867\n",
      "reward_it 38\n",
      "reward_end 0.23739266395568848\n",
      "Episode: 59/10000,\tScore: -117.04,\tDistance: -17.04,\tactor_loss: 9.470551490783691,\tc1_loss:0.2362118512392044,\tc2_loss:0.2036835253238678ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.0029916763305664062\n",
      "reward_it 50\n",
      "reward_end 0.3141322135925293\n",
      "Episode: 60/10000,\tScore: -108.75,\tDistance: -8.75,\tactor_loss: 9.638257026672363,\tc1_loss:0.3608785569667816,\tc2_loss:0.6078391671180725ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.0029959678649902344\n",
      "reward_it 67\n",
      "reward_end 0.4148869514465332\n",
      "Episode: 61/10000,\tScore: -107.35,\tDistance: -7.35,\tactor_loss: 10.04097843170166,\tc1_loss:0.6561530828475952,\tc2_loss:1.7132035493850708ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.003983736038208008\n",
      "reward_it 127\n",
      "reward_end 0.7968413829803467\n",
      "Episode: 62/10000,\tScore: -130.64,\tDistance: -30.64,\tactor_loss: 10.227343559265137,\tc1_loss:0.6332007646560669,\tc2_loss:0.7365349531173706ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.0029921531677246094\n",
      "reward_it 46\n",
      "reward_end 0.2892310619354248\n",
      "Episode: 63/10000,\tScore: -124.79,\tDistance: -24.79,\tactor_loss: 10.502172470092773,\tc1_loss:0.5585653781890869,\tc2_loss:0.740497350692749ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.0029859542846679688\n",
      "reward_it 40\n",
      "reward_end 0.25133204460144043\n",
      "Episode: 64/10000,\tScore: -112.93,\tDistance: -12.93,\tactor_loss: 10.084420204162598,\tc1_loss:0.8017070293426514,\tc2_loss:0.6462080478668213ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.0039615631103515625\n",
      "reward_it 41\n",
      "reward_end 0.25933003425598145\n",
      "Episode: 65/10000,\tScore: -113.37,\tDistance: -13.37,\tactor_loss: 11.390839576721191,\tc1_loss:1.550326943397522,\tc2_loss:1.4768577814102173ep_end 0.0010025501251220703\n",
      "ep_begin\n",
      "reward_begin 0.003984928131103516\n",
      "reward_it 56\n",
      "reward_end 0.3530614376068115\n",
      "Episode: 66/10000,\tScore: -108.04,\tDistance: -8.04,\tactor_loss: 11.578933715820312,\tc1_loss:3.7273881435394287,\tc2_loss:1.3025360107421875ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.0039937496185302734\n",
      "reward_it 66\n",
      "reward_end 0.41785311698913574\n",
      "Episode: 67/10000,\tScore: -108.89,\tDistance: -8.89,\tactor_loss: 11.301984786987305,\tc1_loss:0.29986491799354553,\tc2_loss:0.14564748108386993ep_end 0.000995635986328125\n",
      "ep_begin\n",
      "reward_begin 0.003020763397216797\n",
      "reward_it 61\n",
      "reward_end 0.3829476833343506\n",
      "Episode: 68/10000,\tScore: -106.55,\tDistance: -6.55,\tactor_loss: 10.775014877319336,\tc1_loss:0.2564190924167633,\tc2_loss:0.2963361144065857ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.0030202865600585938\n",
      "reward_it 42\n",
      "reward_end 0.26329565048217773\n",
      "Episode: 69/10000,\tScore: -113.52,\tDistance: -13.52,\tactor_loss: 12.684173583984375,\tc1_loss:0.5378439426422119,\tc2_loss:0.5605911612510681ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.0030221939086914062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward_it 46\n",
      "reward_end 0.2902226448059082\n",
      "Episode: 70/10000,\tScore: -122.85,\tDistance: -22.85,\tactor_loss: 11.351896286010742,\tc1_loss:1.0785061120986938,\tc2_loss:1.0933889150619507ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.002991199493408203\n",
      "reward_it 41\n",
      "reward_end 0.26526331901550293\n",
      "Episode: 71/10000,\tScore: -112.97,\tDistance: -12.97,\tactor_loss: 8.961634635925293,\tc1_loss:0.2704848647117615,\tc2_loss:0.40212759375572205ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.004017829895019531\n",
      "reward_it 43\n",
      "reward_end 0.2702772617340088\n",
      "Episode: 72/10000,\tScore: -118.36,\tDistance: -18.36,\tactor_loss: 12.81436538696289,\tc1_loss:1.4012659788131714,\tc2_loss:2.1321938037872314ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.0029916763305664062\n",
      "reward_it 60\n",
      "reward_end 0.3750007152557373\n",
      "Episode: 73/10000,\tScore: -108.38,\tDistance: -8.38,\tactor_loss: 10.899025917053223,\tc1_loss:19.560413360595703,\tc2_loss:19.7983341217041ep_end 0.0009665489196777344\n",
      "ep_begin\n",
      "reward_begin 0.0029921531677246094\n",
      "reward_it 42\n",
      "reward_end 0.26326823234558105\n",
      "Episode: 74/10000,\tScore: -111.76,\tDistance: -11.76,\tactor_loss: 12.125907897949219,\tc1_loss:1.720306158065796,\tc2_loss:1.4704185724258423ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.0029969215393066406\n",
      "reward_it 44\n",
      "reward_end 0.27824902534484863\n",
      "Episode: 75/10000,\tScore: -111.40,\tDistance: -11.40,\tactor_loss: 10.077686309814453,\tc1_loss:0.37697696685791016,\tc2_loss:0.3110300898551941ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.0029938220977783203\n",
      "reward_it 43\n",
      "reward_end 0.2712733745574951\n",
      "Episode: 76/10000,\tScore: -111.67,\tDistance: -11.67,\tactor_loss: 10.798341751098633,\tc1_loss:2.058093309402466,\tc2_loss:1.4007400274276733ep_end 0.0009682178497314453\n",
      "ep_begin\n",
      "reward_begin 0.004017829895019531\n",
      "reward_it 43\n",
      "reward_end 0.26927924156188965\n",
      "Episode: 77/10000,\tScore: -114.47,\tDistance: -14.47,\tactor_loss: 10.414111137390137,\tc1_loss:0.1951870620250702,\tc2_loss:0.25810369849205017ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.0029947757720947266\n",
      "reward_it 44\n",
      "reward_end 0.2752571105957031\n",
      "Episode: 78/10000,\tScore: -112.21,\tDistance: -12.21,\tactor_loss: 10.166946411132812,\tc1_loss:0.40877941250801086,\tc2_loss:0.266177773475647ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.0030226707458496094\n",
      "reward_it 55\n",
      "reward_end 0.3400866985321045\n",
      "Episode: 79/10000,\tScore: -111.56,\tDistance: -11.56,\tactor_loss: 11.477174758911133,\tc1_loss:1.9293590784072876,\tc2_loss:2.3123226165771484ep_end 0.0010066032409667969\n",
      "ep_begin\n",
      "reward_begin 0.0039823055267333984\n",
      "reward_it 144\n",
      "reward_end 0.8946104049682617\n",
      "Episode: 80/10000,\tScore: -137.66,\tDistance: -37.66,\tactor_loss: 11.051261901855469,\tc1_loss:0.6721916794776917,\tc2_loss:0.7357459664344788ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.0039577484130859375\n",
      "reward_it 63\n",
      "reward_end 0.39397478103637695\n",
      "Episode: 81/10000,\tScore: -110.75,\tDistance: -10.75,\tactor_loss: 12.16127872467041,\tc1_loss:0.35432735085487366,\tc2_loss:0.22911617159843445ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.002986431121826172\n",
      "reward_it 119\n",
      "reward_end 0.7410142421722412\n",
      "Episode: 82/10000,\tScore: -107.46,\tDistance: -7.46,\tactor_loss: 10.717960357666016,\tc1_loss:0.5810470581054688,\tc2_loss:0.7724613547325134ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.002968311309814453\n",
      "reward_it 204\n",
      "reward_end 1.26664137840271\n",
      "Episode: 83/10000,\tScore: -139.87,\tDistance: -39.87,\tactor_loss: 10.30230712890625,\tc1_loss:1.0599470138549805,\tc2_loss:0.6664454340934753ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.002987384796142578\n",
      "reward_it 61\n",
      "reward_end 0.3839759826660156\n",
      "Episode: 84/10000,\tScore: -110.81,\tDistance: -10.81,\tactor_loss: 12.605021476745605,\tc1_loss:0.6448141932487488,\tc2_loss:0.40994277596473694ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.003974437713623047\n",
      "reward_it 72\n",
      "reward_end 0.4437854290008545\n",
      "Episode: 85/10000,\tScore: -111.83,\tDistance: -11.83,\tactor_loss: 14.187806129455566,\tc1_loss:3.658731698989868,\tc2_loss:1.2177730798721313ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.0030226707458496094\n",
      "reward_it 86\n",
      "reward_end 0.5435452461242676\n",
      "Episode: 86/10000,\tScore: -111.90,\tDistance: -11.90,\tactor_loss: 13.678982734680176,\tc1_loss:0.6350562572479248,\tc2_loss:0.8519229888916016ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.003022432327270508\n",
      "reward_it 128\n",
      "reward_end 0.8068423271179199\n",
      "Episode: 87/10000,\tScore: -106.08,\tDistance: -6.08,\tactor_loss: 12.821622848510742,\tc1_loss:0.42142027616500854,\tc2_loss:0.5430700778961182ep_end 0.0010025501251220703\n",
      "ep_begin\n",
      "reward_begin 0.003954172134399414\n",
      "reward_it 57\n",
      "reward_end 0.362060546875\n",
      "Episode: 88/10000,\tScore: -104.17,\tDistance: -4.17,\tactor_loss: 11.941947937011719,\tc1_loss:1.1493797302246094,\tc2_loss:1.522641897201538ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.0029926300048828125\n",
      "reward_it 61\n",
      "reward_end 0.3869655132293701\n",
      "Episode: 89/10000,\tScore: -111.99,\tDistance: -11.99,\tactor_loss: 12.89295482635498,\tc1_loss:1.4001216888427734,\tc2_loss:3.1492669582366943ep_end 0.0009965896606445312\n",
      "ep_begin\n",
      "reward_begin 0.0039904117584228516\n",
      "reward_it 95\n",
      "reward_end 0.6053817272186279\n",
      "Episode: 90/10000,\tScore: -118.61,\tDistance: -18.61,\tactor_loss: 10.233695983886719,\tc1_loss:0.8575721383094788,\tc2_loss:0.6314280033111572ep_end 0.0010006427764892578\n",
      "ep_begin\n",
      "reward_begin 0.003985166549682617\n",
      "reward_it 1599\n",
      "reward_end 9.956383228302002\n",
      "Episode: 91/10000,\tScore: -83.66,\tDistance: -83.66,\tactor_loss: 14.061148643493652,\tc1_loss:1.161629319190979,\tc2_loss:1.1908631324768066ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.0029938220977783203\n",
      "reward_it 1599\n",
      "reward_end 9.916486740112305\n",
      "Episode: 92/10000,\tScore: -130.08,\tDistance: -130.08,\tactor_loss: 14.523796081542969,\tc1_loss:1.21193265914917,\tc2_loss:1.5579471588134766ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.00401759147644043\n",
      "reward_it 200\n",
      "reward_end 1.2466728687286377\n",
      "Episode: 93/10000,\tScore: -138.70,\tDistance: -38.70,\tactor_loss: 16.480661392211914,\tc1_loss:0.43512263894081116,\tc2_loss:0.2877592444419861ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.002982616424560547\n",
      "reward_it 272\n",
      "reward_end 1.6894850730895996\n",
      "Episode: 94/10000,\tScore: -145.88,\tDistance: -45.88,\tactor_loss: 13.847521781921387,\tc1_loss:0.5600786805152893,\tc2_loss:0.4455004930496216ep_end 0.0009682178497314453\n",
      "ep_begin\n",
      "reward_begin 0.004986286163330078\n",
      "reward_it 168\n",
      "reward_end 1.0302739143371582\n",
      "Episode: 95/10000,\tScore: -133.90,\tDistance: -33.90,\tactor_loss: 14.840590476989746,\tc1_loss:0.5653972029685974,\tc2_loss:0.35729292035102844ep_end 0.0009965896606445312\n",
      "ep_begin\n",
      "reward_begin 0.0039768218994140625\n",
      "reward_it 104\n",
      "reward_end 0.6542632579803467\n",
      "Episode: 96/10000,\tScore: -114.45,\tDistance: -14.45,\tactor_loss: 13.154128074645996,\tc1_loss:0.5289990305900574,\tc2_loss:0.615208625793457ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.003961324691772461\n",
      "reward_it 165\n",
      "reward_end 1.0452070236206055\n",
      "Episode: 97/10000,\tScore: -137.39,\tDistance: -37.39,\tactor_loss: 16.690998077392578,\tc1_loss:1.2237485647201538,\tc2_loss:0.6688528060913086ep_end 0.001024484634399414\n",
      "ep_begin\n",
      "reward_begin 0.004017829895019531\n",
      "reward_it 212\n",
      "reward_end 1.3314404487609863\n",
      "Episode: 98/10000,\tScore: -123.40,\tDistance: -23.40,\tactor_loss: 15.360729217529297,\tc1_loss:0.39611899852752686,\tc2_loss:0.18103380501270294ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.0029914379119873047\n",
      "reward_it 1599\n",
      "reward_end 9.856649398803711\n",
      "Episode: 99/10000,\tScore: -155.31,\tDistance: -155.31,\tactor_loss: 16.353591918945312,\tc1_loss:2.3137030601501465,\tc2_loss:2.4880690574645996ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.0030198097229003906\n",
      "reward_it 81\n",
      "reward_end 0.5135989189147949\n",
      "Episode: 100/10000,\tMean Score: -127.60,\tMean Distance: -51.60,\tactor_loss: 17.011096954345703,\tc1_loss:1.041976809501648,\tc2_loss:1.2184323072433472\n",
      "ep_end 0.011996030807495117\n",
      "ep_begin\n",
      "reward_begin 0.003961801528930664\n",
      "reward_it 41\n",
      "reward_end 0.2593345642089844\n",
      "Episode: 101/10000,\tScore: -109.96,\tDistance: -9.96,\tactor_loss: 16.169326782226562,\tc1_loss:0.6226427555084229,\tc2_loss:0.6953905820846558ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.0029790401458740234\n",
      "reward_it 81\n",
      "reward_end 0.5146288871765137\n",
      "Episode: 102/10000,\tScore: -123.93,\tDistance: -23.93,\tactor_loss: 15.331923484802246,\tc1_loss:0.86122065782547,\tc2_loss:1.2427128553390503ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.002990245819091797\n",
      "reward_it 112\n",
      "reward_end 0.7539858818054199\n",
      "Episode: 103/10000,\tScore: -129.50,\tDistance: -29.50,\tactor_loss: 16.434797286987305,\tc1_loss:0.3706822097301483,\tc2_loss:0.463805615901947ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.003991127014160156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward_it 83\n",
      "reward_end 0.551525354385376\n",
      "Episode: 104/10000,\tScore: -125.22,\tDistance: -25.22,\tactor_loss: 18.130956649780273,\tc1_loss:1.0483025312423706,\tc2_loss:0.846362292766571ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.003988027572631836\n",
      "reward_it 127\n",
      "reward_end 0.8377630710601807\n",
      "Episode: 105/10000,\tScore: -130.40,\tDistance: -30.40,\tactor_loss: 15.04436206817627,\tc1_loss:0.5913920998573303,\tc2_loss:0.838761568069458ep_end 0.0009944438934326172\n",
      "ep_begin\n",
      "reward_begin 0.003961801528930664\n",
      "reward_it 119\n",
      "reward_end 0.7570040225982666\n",
      "Episode: 106/10000,\tScore: -130.71,\tDistance: -30.71,\tactor_loss: 17.691064834594727,\tc1_loss:0.5633301138877869,\tc2_loss:0.6735989451408386ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.002994537353515625\n",
      "reward_it 138\n",
      "reward_end 0.8646583557128906\n",
      "Episode: 107/10000,\tScore: -131.04,\tDistance: -31.04,\tactor_loss: 14.2443208694458,\tc1_loss:0.25649046897888184,\tc2_loss:0.3263804614543915ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.002991914749145508\n",
      "reward_it 44\n",
      "reward_end 0.2782909870147705\n",
      "Episode: 108/10000,\tScore: -109.70,\tDistance: -9.70,\tactor_loss: 18.63195037841797,\tc1_loss:0.43675193190574646,\tc2_loss:0.41151076555252075ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.0029914379119873047\n",
      "reward_it 106\n",
      "reward_end 0.66522216796875\n",
      "Episode: 109/10000,\tScore: -122.69,\tDistance: -22.69,\tactor_loss: 15.205533981323242,\tc1_loss:1.5087288618087769,\tc2_loss:1.230046033859253ep_end 0.0009970664978027344\n",
      "ep_begin\n",
      "reward_begin 0.003989696502685547\n",
      "reward_it 59\n",
      "reward_end 0.37100791931152344\n",
      "Episode: 110/10000,\tScore: -116.91,\tDistance: -16.91,\tactor_loss: 17.92352867126465,\tc1_loss:2.6550192832946777,\tc2_loss:2.4622254371643066ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.002986907958984375\n",
      "reward_it 106\n",
      "reward_end 0.6672160625457764\n",
      "Episode: 111/10000,\tScore: -123.19,\tDistance: -23.19,\tactor_loss: 15.362663269042969,\tc1_loss:1.0816015005111694,\tc2_loss:1.0682209730148315ep_end 0.0009984970092773438\n",
      "ep_begin\n",
      "reward_begin 0.003988027572631836\n",
      "reward_it 109\n",
      "reward_end 0.6761939525604248\n",
      "Episode: 112/10000,\tScore: -124.34,\tDistance: -24.34,\tactor_loss: 16.958683013916016,\tc1_loss:0.4820190966129303,\tc2_loss:0.2680092453956604ep_end 0.0009958744049072266\n",
      "ep_begin\n",
      "reward_begin 0.0039615631103515625\n",
      "reward_it 259\n",
      "reward_end 1.5977587699890137\n",
      "Episode: 113/10000,\tScore: -112.16,\tDistance: -12.16,\tactor_loss: 17.486793518066406,\tc1_loss:2.4301791191101074,\tc2_loss:2.3171610832214355ep_end 0.0009949207305908203\n",
      "ep_begin\n",
      "reward_begin 0.0039899349212646484\n",
      "reward_it 1599\n",
      "reward_end 9.95339035987854\n",
      "Episode: 114/10000,\tScore: -97.51,\tDistance: -97.51,\tactor_loss: 15.893882751464844,\tc1_loss:0.4980705976486206,\tc2_loss:0.4954937994480133ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.003961324691772461\n",
      "reward_it 335\n",
      "reward_end 2.0655057430267334\n",
      "Episode: 115/10000,\tScore: -107.69,\tDistance: -7.69,\tactor_loss: 14.968558311462402,\tc1_loss:0.8080413341522217,\tc2_loss:1.0452353954315186ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.002991914749145508\n",
      "reward_it 139\n",
      "reward_end 0.8756604194641113\n",
      "Episode: 116/10000,\tScore: -126.37,\tDistance: -26.37,\tactor_loss: 17.25419807434082,\tc1_loss:0.5416027307510376,\tc2_loss:0.41796717047691345ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.0029931068420410156\n",
      "reward_it 155\n",
      "reward_end 0.9803502559661865\n",
      "Episode: 117/10000,\tScore: -133.66,\tDistance: -33.66,\tactor_loss: 14.98774528503418,\tc1_loss:0.3691054582595825,\tc2_loss:0.524804413318634ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.002992391586303711\n",
      "reward_it 154\n",
      "reward_end 0.9833700656890869\n",
      "Episode: 118/10000,\tScore: -97.07,\tDistance: 2.93,\tactor_loss: 17.886653900146484,\tc1_loss:0.4807707369327545,\tc2_loss:0.41235312819480896ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.002993345260620117\n",
      "reward_it 201\n",
      "reward_end 1.2646207809448242\n",
      "Episode: 119/10000,\tScore: -135.78,\tDistance: -35.78,\tactor_loss: 15.142194747924805,\tc1_loss:0.7537024021148682,\tc2_loss:0.7767667770385742ep_end 0.0009930133819580078\n",
      "ep_begin\n",
      "reward_begin 0.0049588680267333984\n",
      "reward_it 145\n",
      "reward_end 0.9085996150970459\n",
      "Episode: 120/10000,\tScore: -129.07,\tDistance: -29.07,\tactor_loss: 20.35520362854004,\tc1_loss:9.614862442016602,\tc2_loss:7.929792404174805ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.0029921531677246094\n",
      "reward_it 149\n",
      "reward_end 0.964421272277832\n",
      "Episode: 121/10000,\tScore: -126.61,\tDistance: -26.61,\tactor_loss: 17.753185272216797,\tc1_loss:0.43655645847320557,\tc2_loss:0.2699986696243286ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.002964019775390625\n",
      "reward_it 1599\n",
      "reward_end 10.034209489822388\n",
      "Episode: 122/10000,\tScore: -133.99,\tDistance: -133.99,\tactor_loss: 18.517667770385742,\tc1_loss:2.364999771118164,\tc2_loss:2.2927019596099854ep_end 0.0009949207305908203\n",
      "ep_begin\n",
      "reward_begin 0.003985881805419922\n",
      "reward_it 1599\n",
      "reward_end 10.029165029525757\n",
      "Episode: 123/10000,\tScore: -131.02,\tDistance: -131.02,\tactor_loss: 17.10167121887207,\tc1_loss:2.179351568222046,\tc2_loss:2.1629509925842285ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.0029926300048828125\n",
      "reward_it 1599\n",
      "reward_end 10.240652322769165\n",
      "Episode: 124/10000,\tScore: -132.25,\tDistance: -132.25,\tactor_loss: 17.431264877319336,\tc1_loss:0.4948349595069885,\tc2_loss:0.4398204982280731ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.004017829895019531\n",
      "reward_it 1599\n",
      "reward_end 10.19674015045166\n",
      "Episode: 125/10000,\tScore: -122.87,\tDistance: -122.87,\tactor_loss: 15.84644889831543,\tc1_loss:2.2843964099884033,\tc2_loss:1.8958712816238403ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.0029926300048828125\n",
      "reward_it 1599\n",
      "reward_end 10.1099693775177\n",
      "Episode: 126/10000,\tScore: -129.99,\tDistance: -129.99,\tactor_loss: 17.972490310668945,\tc1_loss:0.7095745205879211,\tc2_loss:0.6011626124382019ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.0029611587524414062\n",
      "reward_it 1599\n",
      "reward_end 10.011265277862549\n",
      "Episode: 127/10000,\tScore: -108.26,\tDistance: -108.26,\tactor_loss: 17.564128875732422,\tc1_loss:0.5558772683143616,\tc2_loss:0.43285617232322693ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.0029921531677246094\n",
      "reward_it 1599\n",
      "reward_end 9.97533130645752\n",
      "Episode: 128/10000,\tScore: -131.93,\tDistance: -131.93,\tactor_loss: 17.222537994384766,\tc1_loss:1.1912271976470947,\tc2_loss:1.470513105392456ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.0029897689819335938\n",
      "reward_it 166\n",
      "reward_end 1.035229206085205\n",
      "Episode: 129/10000,\tScore: -114.20,\tDistance: -14.20,\tactor_loss: 18.866914749145508,\tc1_loss:0.417397677898407,\tc2_loss:0.573574423789978ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.0029587745666503906\n",
      "reward_it 143\n",
      "reward_end 0.9375278949737549\n",
      "Episode: 130/10000,\tScore: -127.66,\tDistance: -27.66,\tactor_loss: 17.87574577331543,\tc1_loss:0.473589688539505,\tc2_loss:0.45351970195770264ep_end 0.0010232925415039062\n",
      "ep_begin\n",
      "reward_begin 0.004000186920166016\n",
      "reward_it 120\n",
      "reward_end 0.7549736499786377\n",
      "Episode: 131/10000,\tScore: -127.20,\tDistance: -27.20,\tactor_loss: 18.643468856811523,\tc1_loss:0.5339416265487671,\tc2_loss:0.5313647985458374ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.0030205249786376953\n",
      "reward_it 121\n",
      "reward_end 0.7918882369995117\n",
      "Episode: 132/10000,\tScore: -103.61,\tDistance: -3.61,\tactor_loss: 18.001249313354492,\tc1_loss:0.33724647760391235,\tc2_loss:0.5789599418640137ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.00299072265625\n",
      "reward_it 119\n",
      "reward_end 0.7659227848052979\n",
      "Episode: 133/10000,\tScore: -101.34,\tDistance: -1.34,\tactor_loss: 19.883161544799805,\tc1_loss:0.6357123255729675,\tc2_loss:1.404189109802246ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.002990245819091797\n",
      "reward_it 259\n",
      "reward_end 1.6535749435424805\n",
      "Episode: 134/10000,\tScore: -129.99,\tDistance: -29.99,\tactor_loss: 18.025163650512695,\tc1_loss:0.2642683684825897,\tc2_loss:0.24227194488048553ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.003965616226196289\n",
      "reward_it 628\n",
      "reward_end 3.955425977706909\n",
      "Episode: 135/10000,\tScore: -136.61,\tDistance: -36.61,\tactor_loss: 17.742666244506836,\tc1_loss:0.46643075346946716,\tc2_loss:0.37130260467529297ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.003950834274291992\n",
      "reward_it 578\n",
      "reward_end 3.6293251514434814\n",
      "Episode: 136/10000,\tScore: -163.65,\tDistance: -63.65,\tactor_loss: 16.13694953918457,\tc1_loss:0.8362678289413452,\tc2_loss:0.41095176339149475ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.0029914379119873047\n",
      "reward_it 1599\n",
      "reward_end 10.040159225463867\n",
      "Episode: 137/10000,\tScore: -69.04,\tDistance: -69.04,\tactor_loss: 13.61380386352539,\tc1_loss:1.5504029989242554,\tc2_loss:1.5047537088394165ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.002991914749145508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward_it 840\n",
      "reward_end 5.2908618450164795\n",
      "Episode: 138/10000,\tScore: -178.12,\tDistance: -78.12,\tactor_loss: 17.941125869750977,\tc1_loss:1.085044264793396,\tc2_loss:0.6035294532775879ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.002988100051879883\n",
      "reward_it 1599\n",
      "reward_end 10.15485405921936\n",
      "Episode: 139/10000,\tScore: -99.18,\tDistance: -99.18,\tactor_loss: 22.44602394104004,\tc1_loss:0.4803832769393921,\tc2_loss:0.5261297821998596ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.004019498825073242\n",
      "reward_it 107\n",
      "reward_end 0.6722006797790527\n",
      "Episode: 140/10000,\tScore: -111.67,\tDistance: -11.67,\tactor_loss: 19.553693771362305,\tc1_loss:0.906484067440033,\tc2_loss:0.6340849995613098ep_end 0.000997304916381836\n",
      "ep_begin\n",
      "reward_begin 0.00399017333984375\n",
      "reward_it 249\n",
      "reward_end 1.5727975368499756\n",
      "Episode: 141/10000,\tScore: -126.07,\tDistance: -26.07,\tactor_loss: 19.577463150024414,\tc1_loss:0.7265650033950806,\tc2_loss:0.5462464094161987ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.002990245819091797\n",
      "reward_it 90\n",
      "reward_end 0.5664565563201904\n",
      "Episode: 142/10000,\tScore: -104.76,\tDistance: -4.76,\tactor_loss: 20.09377670288086,\tc1_loss:0.7548794150352478,\tc2_loss:0.7784242033958435ep_end 0.0010254383087158203\n",
      "ep_begin\n",
      "reward_begin 0.003985404968261719\n",
      "reward_it 1599\n",
      "reward_end 10.019219636917114\n",
      "Episode: 143/10000,\tScore: -84.96,\tDistance: -84.96,\tactor_loss: 17.17597007751465,\tc1_loss:0.39733898639678955,\tc2_loss:0.4864327907562256ep_end 0.000997781753540039\n",
      "ep_begin\n",
      "reward_begin 0.003989458084106445\n",
      "reward_it 91\n",
      "reward_end 0.5664846897125244\n",
      "Episode: 144/10000,\tScore: -98.05,\tDistance: 1.95,\tactor_loss: 17.468353271484375,\tc1_loss:0.43861818313598633,\tc2_loss:0.35879915952682495ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.0029921531677246094\n",
      "reward_it 207\n",
      "reward_end 1.282571792602539\n",
      "Episode: 145/10000,\tScore: -99.32,\tDistance: 0.68,\tactor_loss: 17.920499801635742,\tc1_loss:1.0867953300476074,\tc2_loss:1.1108180284500122ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.003958940505981445\n",
      "reward_it 125\n",
      "reward_end 0.7709672451019287\n",
      "Episode: 146/10000,\tScore: -100.35,\tDistance: -0.35,\tactor_loss: 15.709148406982422,\tc1_loss:2.5820412635803223,\tc2_loss:2.0460286140441895ep_end 0.00099945068359375\n",
      "ep_begin\n",
      "reward_begin 0.003986358642578125\n",
      "reward_it 93\n",
      "reward_end 0.5824165344238281\n",
      "Episode: 147/10000,\tScore: -96.51,\tDistance: 3.49,\tactor_loss: 17.242103576660156,\tc1_loss:0.5438758134841919,\tc2_loss:0.47612449526786804ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.003988981246948242\n",
      "reward_it 308\n",
      "reward_end 1.9009454250335693\n",
      "Episode: 148/10000,\tScore: -118.03,\tDistance: -18.03,\tactor_loss: 20.856464385986328,\tc1_loss:16.870824813842773,\tc2_loss:16.22884178161621ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.003988742828369141\n",
      "reward_it 1599\n",
      "reward_end 9.895546913146973\n",
      "Episode: 149/10000,\tScore: -100.86,\tDistance: -100.86,\tactor_loss: 19.468564987182617,\tc1_loss:1.9843881130218506,\tc2_loss:1.5425161123275757ep_end 0.000997781753540039\n",
      "ep_begin\n",
      "reward_begin 0.002991199493408203\n",
      "reward_it 98\n",
      "reward_end 0.6293172836303711\n",
      "Episode: 150/10000,\tScore: -124.32,\tDistance: -24.32,\tactor_loss: 18.791345596313477,\tc1_loss:1.7668906450271606,\tc2_loss:2.585413694381714ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.003988027572631836\n",
      "reward_it 496\n",
      "reward_end 3.081761360168457\n",
      "Episode: 151/10000,\tScore: -113.19,\tDistance: -13.19,\tactor_loss: 19.707977294921875,\tc1_loss:1.403746485710144,\tc2_loss:0.955124020576477ep_end 0.000997304916381836\n",
      "ep_begin\n",
      "reward_begin 0.00396275520324707\n",
      "reward_it 198\n",
      "reward_end 1.2706310749053955\n",
      "Episode: 152/10000,\tScore: -110.28,\tDistance: -10.28,\tactor_loss: 17.585973739624023,\tc1_loss:14.944283485412598,\tc2_loss:14.915410041809082ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.002958059310913086\n",
      "reward_it 268\n",
      "reward_end 1.6895122528076172\n",
      "Episode: 153/10000,\tScore: -144.52,\tDistance: -44.52,\tactor_loss: 20.886493682861328,\tc1_loss:1.0996955633163452,\tc2_loss:1.1900370121002197ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.002991199493408203\n",
      "reward_it 296\n",
      "reward_end 1.8350951671600342\n",
      "Episode: 154/10000,\tScore: -121.60,\tDistance: -21.60,\tactor_loss: 20.604814529418945,\tc1_loss:0.6184508204460144,\tc2_loss:0.4664258062839508ep_end 0.0\n",
      "ep_begin\n",
      "reward_begin 0.005984067916870117\n",
      "reward_it 1599\n",
      "reward_end 9.954360008239746\n",
      "Episode: 155/10000,\tScore: -134.52,\tDistance: -134.52,\tactor_loss: 18.402311325073242,\tc1_loss:0.3208734691143036,\tc2_loss:0.4400462210178375ep_end 0.001027822494506836\n",
      "ep_begin\n",
      "reward_begin 0.003968715667724609\n",
      "reward_it 120\n",
      "reward_end 0.7629530429840088\n",
      "Episode: 156/10000,\tScore: -95.27,\tDistance: 4.73,\tactor_loss: 17.65114402770996,\tc1_loss:2.9053664207458496,\tc2_loss:2.291672945022583ep_end 0.0010361671447753906\n",
      "ep_begin\n",
      "reward_begin 0.0039746761322021484\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-2ea7b80f7af5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[1;31m# if episode is done then update policy:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmax_timesteps\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m             \u001b[0mactor_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc1_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc2_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolyak\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy_noise\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnoise_clip\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy_delay\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m             \u001b[0mactor_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactor_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m             \u001b[0mc1_losses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc1_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-0734e7ff4a59>\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, replay_buffer, n_iter, batch_size, gamma, polyak, policy_noise, noise_clip, policy_delay)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[1;31m# Compute target Q-value:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m             \u001b[0mtarget_Q1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic_1_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_action\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m             \u001b[0mtarget_Q2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic_2_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_action\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m             \u001b[0mtarget_Q\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_Q1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_Q2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32ml:\\anaconda3\\envs\\cs286\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-a53f113e84d0>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, state, action)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ml1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_action\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0mq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ml2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m         \u001b[0mq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ml3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mq\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32ml:\\anaconda3\\envs\\cs286\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32ml:\\anaconda3\\envs\\cs286\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32ml:\\anaconda3\\envs\\cs286\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1688\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1689\u001b[0m         \u001b[1;31m# fused op is marginally faster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1690\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1691\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1692\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "######### Hyperparameters #########\n",
    "gym.logger.set_level(40)\n",
    "env_name = \"BipedalWalker-v3\"\n",
    "env = gym.make(env_name)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "max_action = float(env.action_space.high[0])\n",
    "\n",
    "log_interval = 100  # print avg reward after interval\n",
    "random_seed = 0\n",
    "gamma = 0.99  # discount for future rewards\n",
    "batch_size = 100  # num of transitions sampled from replay buffer\n",
    "lr = 0.001\n",
    "exploration_noise = 0.1\n",
    "polyak = 0.995  # target policy update parameter (1-tau)\n",
    "policy_noise = 0.2  # target policy smoothing noise\n",
    "noise_clip = 0.5\n",
    "policy_delay = 2  # delayed policy updates parameter\n",
    "max_episodes = 10000  # max num of episodes\n",
    "max_timesteps = 2000  # max timesteps in one episode\n",
    "directory = \"./preTrained/\"  # save trained models\n",
    "filename = \"TD3_{}_{}\".format(env_name, random_seed)\n",
    "\n",
    "start_episode = 0\n",
    "\n",
    "\n",
    "policy = TD3(lr, state_dim, action_dim, max_action)\n",
    "replay_buffer = ReplayBuffer()\n",
    "\n",
    "if random_seed:\n",
    "    print(\"Random Seed: {}\".format(random_seed))\n",
    "    env.seed(random_seed)\n",
    "    torch.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "LOAD = False\n",
    "if LOAD:\n",
    "    start_episode = 1000\n",
    "    policy.load(directory, filename, str(start_episode))\n",
    "\n",
    "# logging variables:\n",
    "scores = []\n",
    "mean_scores = []\n",
    "last_scores = deque(maxlen=log_interval)\n",
    "distances = []\n",
    "mean_distances = []\n",
    "last_distance = deque(maxlen=log_interval)\n",
    "losses_mean_episode = []\n",
    "\n",
    "# training procedure:\n",
    "for ep in tn.tqdm(range(start_episode + 1, max_episodes + 1)):\n",
    "    t1=time.time()\n",
    "    print('ep_begin')\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    total_distance = 0\n",
    "    actor_losses = []\n",
    "    c1_losses = []\n",
    "    c2_losses = []\n",
    "    \n",
    "    t2=time.time()\n",
    "    print('reward_begin',t2-t1)\n",
    "    for t in range(max_timesteps):\n",
    "\n",
    "        # select action and add exploration noise:\n",
    "        action = policy.select_action(state)\n",
    "        action = action + np.random.normal(0, exploration_noise, size=env.action_space.shape[0])\n",
    "        action = action.clip(env.action_space.low, env.action_space.high)\n",
    "\n",
    "        # take action in env:\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        replay_buffer.add((state, action, reward, next_state, float(done)))\n",
    "        state = next_state\n",
    "\n",
    "        total_reward += reward\n",
    "        if reward != -100:\n",
    "            total_distance += reward\n",
    "\n",
    "        # if episode is done then update policy:\n",
    "        if done or t == (max_timesteps - 1):\n",
    "            actor_loss, c1_loss, c2_loss = policy.update(replay_buffer, t, batch_size, gamma, polyak, policy_noise, noise_clip, policy_delay)\n",
    "            actor_losses.append(actor_loss)\n",
    "            c1_losses.append((c1_loss))\n",
    "            c2_losses.append(c2_loss)\n",
    "            print('reward_it',t)\n",
    "            break\n",
    "    t3=time.time()\n",
    "    print('reward_end',t3-t2)\n",
    "    mean_loss_actor = np.mean(actor_losses)\n",
    "    mean_loss_c1 = np.mean(c1_losses)\n",
    "    mean_loss_c2 = np.mean(c2_losses)\n",
    "    losses_mean_episode.append((ep, mean_loss_actor, mean_loss_c1, mean_loss_c2))\n",
    "    print('\\rEpisode: {}/{},\\tScore: {:.2f},\\tDistance: {:.2f},\\tactor_loss: {},\\tc1_loss:{},\\tc2_loss:{}'\n",
    "        .format(ep, max_episodes,total_reward,total_distance,mean_loss_actor,mean_loss_c1, mean_loss_c2),end=\"\")\n",
    "\n",
    "    # logging updates:\n",
    "    scores.append(total_reward)\n",
    "    distances.append(total_distance)\n",
    "    last_scores.append(total_reward)\n",
    "    last_distance.append(total_distance)\n",
    "    mean_score = np.mean(last_scores)\n",
    "    mean_distance = np.mean(last_distance)\n",
    "#     FILE = 'record.dat'\n",
    "#     data = [ep, total_reward, total_distance, mean_loss_actor, mean_loss_c1, mean_loss_c2]\n",
    "#     with open(FILE, \"ab\") as f:\n",
    "#         pickle.dump(data, f)\n",
    "\n",
    "    # if avg reward > 300 then save and stop traning:\n",
    "    # end = True\n",
    "    # for s in last_scores:\n",
    "    #     if s < 300:\n",
    "    #         end = False\n",
    "    #         break\n",
    "    # if end and len(last_scores) == 100:\n",
    "    #     print(\"########## Solved! ###########\")\n",
    "    #     name = filename + '_solved'\n",
    "    #     policy.save(directory, name, str(ep))\n",
    "    #     break\n",
    "\n",
    "    # print avg reward every log interval:\n",
    "    if ep % log_interval == 0:\n",
    "        policy.save(directory, filename, str(ep))\n",
    "        mean_scores.append(mean_score)\n",
    "        mean_distances.append(mean_distance)\n",
    "        print('\\rEpisode: {}/{},\\tMean Score: {:.2f},\\tMean Distance: {:.2f},\\tactor_loss: {},\\tc1_loss:{},\\tc2_loss:{}'\n",
    "            .format(ep, max_episodes, mean_score, mean_distance, mean_loss_actor, mean_loss_c1, mean_loss_c2))\n",
    "        FILE = 'record_mean.dat'\n",
    "        data = [ep, mean_score, mean_distance, mean_loss_actor, mean_loss_c1, mean_loss_c2]\n",
    "        with open(FILE, \"ab\") as f:\n",
    "            pickle.dump(data, f)\n",
    "        if mean_score >= 300 and len(last_scores) == 100:\n",
    "            print(\"########## Solved! ###########\")\n",
    "            name = filename + '_solved'\n",
    "            policy.save(directory, name, str(ep))\n",
    "            break\n",
    "    t4=time.time()\n",
    "    print('ep_end',t4-t3)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 2.5 Show your change curves of reward and loss in two sub-pictures (5 points)\n",
    "Your reward and loss must be able to converge; otherwise this part will get 0 point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = []\n",
    "with open('./record.dat', 'rb') as fr:\n",
    "    try:\n",
    "        while True:\n",
    "            data.append(pickle.load(fr))\n",
    "    except EOFError:\n",
    "        pass\n",
    "data = pd.DataFrame(np.array(data))\n",
    "\n",
    "scores = []\n",
    "distances = []\n",
    "actor_losses = []\n",
    "critic1_losses = []\n",
    "critic2_losses = []\n",
    "\n",
    "\n",
    "for r in data[1]:\n",
    "    scores.append(r)\n",
    "\n",
    "for r in data[2]:\n",
    "    distances.append(r)\n",
    "\n",
    "for r in data[3]:\n",
    "    actor_losses.append(r)\n",
    "\n",
    "for r in data[4]:\n",
    "    critic1_losses.append(r)\n",
    "\n",
    "for r in data[4]:\n",
    "    critic2_losses.append(r)\n",
    "\n",
    "data_mean = []\n",
    "with open('./record_mean.dat', 'rb') as record_mean:\n",
    "    try:\n",
    "        while True:\n",
    "            data_mean.append(pickle.load(record_mean))\n",
    "    except EOFError:\n",
    "        pass\n",
    "data_mean = pd.DataFrame(np.array(data_mean))\n",
    "\n",
    "episode = []\n",
    "scores_mean = []\n",
    "distances_mean = []\n",
    "\n",
    "for r in data_mean[0]:\n",
    "    episode.append(r)\n",
    "\n",
    "for r in data_mean[1]:\n",
    "    scores_mean.append(r)\n",
    "\n",
    "for r in data_mean[2]:\n",
    "    distances_mean.append(r)\n",
    "\n",
    "\n",
    "# plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "x = np.arange(1, len(scores) + 1)\n",
    "y = scores\n",
    "plt.plot(x, y)\n",
    "plt.grid()\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.title('Scores on BipedalWalker-v3')\n",
    "plt.show()\n",
    "namefig = \"plot/scores_TD3_episodes\" + str(data.shape[0]) + \".jpg\"\n",
    "fig.savefig(namefig, dpi=300)\n",
    "\n",
    "# plot the distance and mean\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "x = np.arange(1, len(distances) + 1)\n",
    "y = distances\n",
    "plt.plot(x, y)\n",
    "plt.grid()\n",
    "plt.ylabel('Distance')\n",
    "plt.xlabel('Episode #')\n",
    "plt.title('Distances on BipedalWalker-v3')\n",
    "plt.show()\n",
    "namefig = \"plot/distances_TD3_episodes\" + str(data.shape[0]) + \".jpg\"\n",
    "fig.savefig(namefig, dpi=300)\n",
    "\n",
    "\n",
    "\n",
    "# plot the loss\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "x = np.arange(1, len(actor_losses) + 1)\n",
    "y = actor_losses\n",
    "plt.plot(x, y)\n",
    "plt.grid()\n",
    "plt.ylabel('Actor Loss')\n",
    "plt.xlabel('Episode #')\n",
    "plt.title('Actor Loss on BipedalWalker-v3')\n",
    "plt.show()\n",
    "namefig = \"plot/actor_loss_TD3_episodes\" + str(data.shape[0]) + \".jpg\"\n",
    "fig.savefig(namefig, dpi=300)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "x = np.arange(1, len(critic1_losses) + 1)\n",
    "y = critic1_losses\n",
    "plt.plot(x, y)\n",
    "plt.grid()\n",
    "plt.ylabel('Critic1 Loss')\n",
    "plt.xlabel('Episode #')\n",
    "plt.title('Critic Loss on BipedalWalker-v3')\n",
    "plt.show()\n",
    "namefig = \"plot/critic1_loss_TD3_episodes\" + str(data.shape[0]) + \".jpg\"\n",
    "fig.savefig(namefig, dpi=300)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "x = np.arange(1, len(critic2_losses) + 1)\n",
    "y = critic2_losses\n",
    "plt.plot(x, y)\n",
    "plt.grid()\n",
    "plt.ylabel('Critic2 Loss')\n",
    "plt.xlabel('Episode #')\n",
    "plt.title('Critic Loss on BipedalWalker-v3')\n",
    "plt.show()\n",
    "namefig = \"plot/critic2_loss_TD3_episodes\" + str(data.shape[0]) + \".jpg\"\n",
    "fig.savefig(namefig, dpi=300)\n",
    "\n",
    "\n",
    "\n",
    "# plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "x = episode\n",
    "y = scores_mean\n",
    "plt.plot(x, y)\n",
    "plt.grid()\n",
    "plt.ylabel('Mean Scores every 100 episodes')\n",
    "plt.xlabel('Episode #')\n",
    "plt.title('Mean Scores on BipedalWalker-v3')\n",
    "plt.show()\n",
    "namefig = \"plot/mean_scores_TD3_episodes\" + str(data.shape[0]) + \".jpg\"\n",
    "fig.savefig(namefig, dpi=300)\n",
    "\n",
    "# plot the distance and mean\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "x = episode\n",
    "y = distances_mean\n",
    "plt.plot(x, y)\n",
    "plt.grid()\n",
    "plt.ylabel('Mean Distances every 100 episodes')\n",
    "plt.xlabel('Episode #')\n",
    "plt.title('Mean Distances on BipedalWalker-v3')\n",
    "plt.show()\n",
    "namefig = \"plot/mean_distances_TD3_episodes\" + str(data.shape[0]) + \".jpg\"\n",
    "fig.savefig(namefig, dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Test your model 10 times and print the mean reward (5 points) \n",
    "You should load your trained model **best_model.pt**. If your mean reward is >= 200 but < 250, you will get 3 points. If your mean reward is >= 250, you will get 5 points. If your mean reward is < 200, you will get 0 point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "import gym\n",
    "from TD3 import TD3\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "gym.logger.set_level(40)\n",
    "env_name = \"BipedalWalker-v3\"\n",
    "random_seed = 0\n",
    "n_episodes = 100\n",
    "lr = 0.002\n",
    "max_timesteps = 2000\n",
    "render = False\n",
    "save_gif = False\n",
    "\n",
    "filename = \"TD3_{}_{}\".format(env_name, random_seed)\n",
    "filename += '_solved'\n",
    "directory = \"./preTrained/\".format(env_name)\n",
    "episode = 1073\n",
    "\n",
    "env = gym.make(env_name)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "max_action = float(env.action_space.high[0])\n",
    "\n",
    "policy = TD3(lr, state_dim, action_dim, max_action)\n",
    "\n",
    "policy.load_actor(directory, filename, episode)\n",
    "\n",
    "scores = []\n",
    "\n",
    "for ep in range(1, n_episodes+1):\n",
    "    ep_reward = 0\n",
    "    state = env.reset()\n",
    "    for t in range(max_timesteps):\n",
    "        action = policy.select_action(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        ep_reward += reward\n",
    "        if render:\n",
    "            env.render()\n",
    "            if save_gif:\n",
    "                 img = env.render(mode = 'rgb_array')\n",
    "                 img = Image.fromarray(img)\n",
    "                 img.save('./gif/{}.jpg'.format(t))\n",
    "        if done:\n",
    "            break\n",
    "    scores.append(ep_reward)\n",
    "    #print('Episode: {}\\tReward: {}'.format(ep, int(ep_reward)))\n",
    "    env.close()\n",
    "\n",
    "\n",
    "print(\"Score media\", np.mean(scores))\n",
    "    \n",
    "# plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "x = np.arange(1, len(scores) + 1)\n",
    "y = scores\n",
    "plt.plot(x, y)\n",
    "plt.grid()\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.title('Test Scores on BipedalWalker-v3')\n",
    "plt.show()\n",
    "namefig = \"plot/test_scores_TD3_episodes\" + str(episode) + \".jpg\"\n",
    "fig.savefig(namefig, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()[np.newaxis,:].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
