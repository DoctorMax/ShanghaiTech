{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T11:33:21.039469Z",
     "start_time": "2020-12-24T11:33:21.020519Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "刘某历经千辛万苦，终于整利索的代码，附有详细中文注释\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T11:33:23.196701Z",
     "start_time": "2020-12-24T11:33:21.040466Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# 调库\n",
    "import pickle # 用来序列化的包\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.preprocessing.text import Tokenizer # 该类允许使用两种方法向量化一个文本语料库： 将每个文本转化为一个整数序列（每个整数都是词典中标记的索引）； 或者将其转化为一个向量，其中每个标记的系数可以是二进制值、词频、TF-IDF权重等。\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model, Input\n",
    "from keras.layers import Embedding, Dense, TimeDistributed, Concatenate, BatchNormalization, Bidirectional\n",
    "from keras.layers import Activation, Dropout, Conv1D\n",
    "from keras.layers import LSTM\n",
    "# from tensorflow.python.keras.layers import CuDNNLSTM as LSTM\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from keras.metrics import categorical_accuracy\n",
    "from keras import backend as K\n",
    "from keras.regularizers import l1, l2\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T11:33:23.802082Z",
     "start_time": "2020-12-24T11:33:23.197699Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5534, 39900)\n",
      "(514, 39900)\n"
     ]
    }
   ],
   "source": [
    "# 读数据,这两个数据都是做过onehot编码的\n",
    "# cb6133         = np.load(\"../data/cb6133.npy\")\n",
    "cb6133filtered = np.load(\"../data/cullpdb+profile_6133_filtered.npy.gz\")\n",
    "cb513          = np.load(\"../data/cb513+profile_split1.npy.gz\")\n",
    "\n",
    "# 看一眼数据大小\n",
    "# print(cb6133.shape)\n",
    "print(cb6133filtered.shape) #(5534, 39900),39900=57*700\n",
    "print(cb513.shape)\n",
    "# cb6133filtered[0,-57:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T11:33:23.817042Z",
     "start_time": "2020-12-24T11:33:23.805075Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# 部分超参数设置\n",
    "# gpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.5)  ##不建议0.8，因为可能有别的东西也用显存\n",
    "# config = tf.compat.v1.ConfigProto(gpu_options=gpu_options)\n",
    "# config.gpu_options.allow_growth = True\n",
    "# session = tf.compat.v1.InteractiveSession(config=config)\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "\n",
    "maxlen_seq = r = 700 # protein residues padded to 700，统一设置成700的序列长度\n",
    "f = 57  # number of features for each residue，onehot后的总特征数量=22+8+5+22\n",
    "residue_list = list('ACEDGFIHKMLNQPSRTWVYX') + ['NoSeq'] # 长度是21+1\n",
    "q8_list      = list('LBEGIHST') + ['NoSeq']\n",
    "columns = [\"id\", \"len\", \"input\", \"profiles\", \"expected\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T11:33:23.847960Z",
     "start_time": "2020-12-24T11:33:23.818040Z"
    },
    "code_folding": [
     0,
     1,
     37,
     45,
     56,
     64,
     67,
     71,
     99,
     122
    ]
   },
   "outputs": [],
   "source": [
    "# 函数定义\n",
    "def get_data(arr, bounds=None): # 把预测得到的分段的 onehot编码 解码成我们想要的氨基酸序列\n",
    "    \n",
    "    if bounds is None: bounds = range(len(arr)) # 应该是蛋白质序列的条数\n",
    "    \n",
    "    data = [None for i in bounds] # 新建了一个长度和arr一样的空列表\n",
    "    for i in bounds:\n",
    "        seq, q8, profiles = '', '', []\n",
    "        for j in range(r):\n",
    "            jf = j*f # 1*57---->700*57\n",
    "            \n",
    "            # Residue convert from one-hot to decoded，应该是从onehot编码解码为氨基酸序列\n",
    "            residue_onehot = arr[i,jf+0:jf+22] # 取了某一条氨基酸序列中的某个氨基酸的57各特征中的前22个特征的概率？\n",
    "            residue = residue_list[np.argmax(residue_onehot)] # 这22个特征中概率最大的成为这个氨基酸的特征，此处为氨基酸的种类\n",
    "\n",
    "            # Q8 one-hot encoded to decoded structure symbol，应该是从onehot解码为氨基酸二级结构\n",
    "            residue_q8_onehot = arr[i,jf+22:jf+31] # 同上\n",
    "            residue_q8 = q8_list[np.argmax(residue_q8_onehot)] # 同上\n",
    "\n",
    "            if residue == 'NoSeq': break      # terminating sequence symbol，遇到编码为NoSeq，应该是意味着这串氨基酸序列结束了\n",
    "\n",
    "            # 下面这仨应该就是蛋白质的一些其他特征，包括profile（亲戚关系？）啥的，可能还有什么像是溶剂可溶性？\n",
    "            nc_terminals = arr[i,jf+31:jf+33] # nc_terminals = [0. 0.]\n",
    "            sa = arr[i,jf+33:jf+35]           # sa = [0. 0.]\n",
    "            profile = arr[i,jf+35:jf+57]      # profile features，这个就是PSSM亲戚特征\n",
    "            \n",
    "            \n",
    "            \n",
    "            seq += residue # concat residues into amino acid sequence，把氨基酸序列预测结果连接起来\n",
    "            q8  += residue_q8 # concat secondary structure into secondary structure sequence， 把氨基酸二级结构预测结果连接起来\n",
    "            profiles.append(profile) # 将亲戚信息保存起来，后续用来作为网络的输入的一部分\n",
    "        \n",
    "        data[i] = [str(i+1), len(seq), seq, np.array(profiles), q8] # 【蛋白质编号，蛋白质长度，蛋白质序列，蛋白质亲戚信息，蛋白质二级结构】\n",
    "    \n",
    "    return pd.DataFrame(data, columns=columns) # 输出成pandas的数据结构，二维的貌似？\n",
    "\n",
    "# The custom accuracy metric used for this task，要自己定义一个正确率，不然keras默认的那个accuracy是包含那些700的长度后面补的NoSeq\n",
    "def myaccuracy(y_true, y_pred):\n",
    "    y = tf.argmax(y_true, axis =- 1) # 用最后一个维度进行封包，然后不同包之间进行比较，比较对应位置的大小\n",
    "    y_ = tf.argmax(y_pred, axis =- 1)\n",
    "    mask = tf.greater(y, 0) # y中比0大的赋值为true，比0小和等于0的赋值为false，这样子就把后面那些空的去掉了，只比对有值的内容的正确率\n",
    "    return K.cast(K.equal(tf.boolean_mask(y, mask), tf.boolean_mask(y_, mask)), K.floatx()) # floatx是默认的浮点数类型，tf.boolean_mask返回mask为true的y的对应位置的内容，equal返回布尔型的list，cast类型转换此处为1.和0\n",
    "\n",
    "# Maps the sequence to a one-hot encoding\n",
    "# 简单onehot解码\n",
    "def onehot_to_seq(oh_seq, index):\n",
    "    s = ''\n",
    "    for o in oh_seq:\n",
    "        i = np.argmax(o)\n",
    "        if i != 0:\n",
    "            s += index[i] # 字符串加法\n",
    "        else:\n",
    "            break\n",
    "    return s\n",
    "\n",
    "# 简单onehot编码\n",
    "def seq2onehot(seq, n):\n",
    "    out = np.zeros((len(seq), maxlen_seq, n)) # （氨基酸序列条数，700，22（onehot的维度））\n",
    "    for i in range(len(seq)): # i是第几条氨基酸序列的序号\n",
    "        for j in range(maxlen_seq): # j是第i条氨基酸序列的第几个氨基酸的序号，1---700\n",
    "            out[i, j, seq[i, j]] = 1 # 懂了，seq[i,j]这个值是多少（x），我们就在第几(x)个位置填1，这个值只能有22种可能性\n",
    "    return out\n",
    "\n",
    "# ngrams第N个词的出现只与前面N-1个词相关，而与其它任何词都不相关，整句的概率就是各个词出现概率的乘积。这些概率可以通过直接从语料中统计N个词同时出现的次数得到。常用的是二元的Bi-Gram和三元的Tri-Gram\n",
    "def seq2ngrams(seqs, n = 1): # 此处n=1，所以某一条氨基酸序列中每个氨基酸前后不相关，所以此时实际效果就像把序列每个氨基酸分开了\n",
    "    return np.array([[seq[i : i + n] for i in range(len(seq))] for seq in seqs])\n",
    "\n",
    "def decode_results(y_, reverse_decoder_index): # 根据字典进行onehot解码\n",
    "    print(\"prediction: \" + str(onehot_to_seq(y_, reverse_decoder_index).upper())) # .upper()小写字母转为大写字母\n",
    "    return str(onehot_to_seq(y_, reverse_decoder_index).upper())\n",
    "\n",
    "def run_test(_model, data1, data2, data3, csv_name, npy_name):\n",
    "    reverse_decoder_index = {value:key for key,value in tokenizer_decoder.word_index.items()} # 把字典反向，用来解码，现在是1:'h'这样子\n",
    "    reverse_encoder_index = {value:key for key,value in tokenizer_encoder.word_index.items()} # 把字典反向用来编码\n",
    "    \n",
    "    # Get predictions using our model\n",
    "    y_test_pred = _model.predict([data1, data2, data3]) # 【我们自己onehot的氨基酸种类序列，embedding张成的64维的氨基酸种类序列，PSSM信息】\n",
    "\n",
    "    decoded_y_pred = []\n",
    "    for i in range(len(test_input_data)):\n",
    "        res = decode_results(y_test_pred[i], reverse_decoder_index) # 应该是从onehot解码成二级结构序列\n",
    "        decoded_y_pred.append(res)\n",
    "\n",
    "    # Set Columns，设置输出的dataframe有哪些内容\n",
    "    out_df = pd.DataFrame()\n",
    "    out_df[\"id\"] = test_df.id.values\n",
    "    out_df[\"real\"] = test_df.expected.values\n",
    "    out_df[\"predict\"] = decoded_y_pred\n",
    "\n",
    "    # 输出一下测试集的预测正确率\n",
    "    testaccseq=myaccuracy(test_target_data,y_test_pred)\n",
    "    testacc=np.sum(testaccseq)/testaccseq.shape\n",
    "    print('测试集的正确率：',testacc)\n",
    "    # Save results，这里保存的是解码后的\n",
    "    with open(csv_name, \"w\") as f:\n",
    "        out_df.to_csv(f, index=False)\n",
    "\n",
    "    np.save(npy_name, y_test_pred) # 这里保存的是没解码的\n",
    "\n",
    "def CNN_BILSTM():\n",
    "    # Inp is one-hot encoded version of inp_alt，就是这句英文的意思，需要在调用的时候被输入\n",
    "    input = Input(shape = (maxlen_seq,))\n",
    "    input2 = Input(shape=(maxlen_seq,22))\n",
    "\n",
    "    x = Embedding(input_dim = n_words, output_dim = 128, input_length = maxlen_seq)(input)\n",
    "    x = Concatenate(axis=2)([x,input2])\n",
    "    z = Conv1D(64, 11, strides=1, padding='same')(x)\n",
    "    w = Conv1D(64, 7, strides=1, padding='same')(x)\n",
    "    x = Concatenate(axis=2)([x,z])\n",
    "    x = Concatenate(axis=2)([x,w])\n",
    "    z = Conv1D(64, 5, strides=1, padding='same')(x)\n",
    "    w = Conv1D(64, 3, strides=1, padding='same')(x)\n",
    "    x = Concatenate(axis=2)([x,z])\n",
    "    x = Concatenate(axis=2)([x,w])\n",
    "    x = Bidirectional(LSTM(units = 128, return_sequences = True))(x)\n",
    "\n",
    "    y = TimeDistributed(Dense(n_tags, activation = \"softmax\"))(x)\n",
    "\n",
    "    model = Model([input,input2], y)# 继承keras的Model类，设置输入为两个元素的list，输出为y\n",
    "    \n",
    "    return model # 定义网络\n",
    "\n",
    "def train(X_train, y_train, X_val=None, y_val=None):\n",
    "    \"\"\"\n",
    "    Main Training function with the following properties:\n",
    "        Optimizer - Nadam\n",
    "        Loss function - Categorical Crossentropy\n",
    "        Batch Size - 128 (any more will exceed Collab GPU RAM)\n",
    "        Epochs - 50\n",
    "    \"\"\"\n",
    "    model = CNN_BILSTM() # 实例化自己定义的网络\n",
    "    model.compile(\n",
    "        optimizer=\"Nadam\", # 某种优化算法\n",
    "        loss = \"categorical_crossentropy\", # 交叉熵损失函数\n",
    "        metrics = [\"accuracy\", myaccuracy]) # 衡量指标：700个的正确率（其中包括后面补的0），我自己写的正确率（序列多长算多长）\n",
    "    \n",
    "    \n",
    "    #########################\n",
    "    print(model.summary())\n",
    "    \n",
    "    \n",
    "    if X_val is not None and y_val is not None:\n",
    "        history = model.fit( X_train, y_train,\n",
    "            batch_size = 128, epochs = 100,\n",
    "            validation_data = (X_val, y_val)) # 用来评估损失，以及在每轮结束时的任何模型度量指标。模型将不会在这个数据上进行训练。\n",
    "    else:\n",
    "        history = model.fit( X_train, y_train,\n",
    "            batch_size = 64, epochs = 100, verbose=1) # 一个 History 对象。其 History.history 属性是连续 epoch 训练损失和评估值，以及验证集损失和评估值的记录（如果适用）。\n",
    "\n",
    "    return history, model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T11:33:41.581548Z",
     "start_time": "2020-12-24T11:33:23.848957Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# 数据处理\n",
    "'''\n",
    "此处稍微记录一下,免得再看到处翻\n",
    "\n",
    "通用：\n",
    "tokenizer_encoder：氨基酸种类词典\n",
    "tokenizer_decoder：氨基酸二级结构词典\n",
    "n_words：氨基酸种类词典词种类数，22\n",
    "n_tags：氨基酸二级结构词典词种类数，9\n",
    "\n",
    "训练：\n",
    "train_df：训练数据的dataframe，其中包括【序号，序列长度，氨基酸种类序列，PSSM信息，二级结构种类序列】\n",
    "train_input_seqs：氨基酸种类序列\n",
    "train_target_seqs：二级结构种类序列\n",
    "train_input_grams：氨基酸种类序列切分开\n",
    "tokenizer_encoder.texts_to_sequences(train_input_grams)：氨基酸种类序列切分开后转成数字序列，其中数字种类总共有22种\n",
    "train_input_data_alt：氨基酸种类序列，切分开，后转成数字序列，后第二个维度扩展到700，也就是都认为长度是700\n",
    "train_input_data：氨基酸种类序列，切分开，后转成数字序列，后第二个维度扩展到700，onehot\n",
    "tokenizer_decoder.texts_to_sequences(train_target_seqs)：二级结构种类序列转成数字序列，数字种类共9种\n",
    "sequence.pad_sequences(train_target_data,maxlen = maxlen_seq, padding='post')：二级结构种类序列转成数字序列后第二个维度扩展到700\n",
    "train_target_seqs：二级结构种类序列转成数字序列后第二个维度扩展到700后进行onehot并且再复制一份反的用来做交叉熵\n",
    "train_profiles：未处理的onehot的PSSM信息\n",
    "train_profiles_np：补长到700的onehot的PSSM信息\n",
    "\n",
    "测试：\n",
    "test_df：同\n",
    "test_input_seqs：\n",
    "test_input_grams：\n",
    "tokenizer_encoder.texts_to_sequences(test_input_grams)：种类\n",
    "test_input_data_alt：数字序列\n",
    "test_input_data：\n",
    "tokenizer_decoder.texts_to_sequences(test_df.expected.values)：二级结构\n",
    "sequence.pad_sequences(test_target_data, maxlen = maxlen_seq, padding='post')：数字序列\n",
    "test_target_data：将测试集的氨基酸二级结构序列进行onehot\n",
    "test_profiles\n",
    "test_profiles_np\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "# Train-test Specification，得到了漂亮的数据结构\n",
    "train_df = get_data(cb6133filtered) # 用来训练\n",
    "test_df  = get_data(cb513) # 用来测试\n",
    "\n",
    "# Computes and returns the n-grams of a particular sequence, defaults to trigrams\n",
    "# Loading and converting the inputs to trigrams，加载并且将其转置，此处转置没啥用，但是如果上面n大于1，就有用了\n",
    "train_input_seqs, train_target_seqs = \\\n",
    "    train_df[['input', 'expected']][(train_df.len.astype(int) <= maxlen_seq)].values.T # astpye(int)将int64转为int32，.values提取值（不包含序号），.T表示转置\n",
    "train_input_grams = seq2ngrams(train_input_seqs)\n",
    "\n",
    "# Same for test，同上\n",
    "test_input_seqs = test_df['input'].values.T\n",
    "test_input_grams = seq2ngrams(test_input_seqs)\n",
    "\n",
    "# Initializing and defining the tokenizer encoders and decoders based on the train set\n",
    "# Tokenizer是一个用于向量化文本，或将文本转换为序列（即单词在字典中的下标构成的列表，从1算起）的类。Tokenizer实际上只是生成了一个字典，并且统计了词频等信息，并没有把文本转成需要的向量表示。\n",
    "tokenizer_encoder = Tokenizer()\n",
    "tokenizer_encoder.fit_on_texts(train_input_grams) # 生成氨基酸种类词典\n",
    "tokenizer_decoder = Tokenizer(char_level = True) # char_level：如果为True，每个字符将被视为一个标记\n",
    "tokenizer_decoder.fit_on_texts(train_target_seqs) # 生成氨基酸二级结构词典\n",
    "\n",
    "# Computing the number of words and number of tags for the keras model，统计一下两个字典的数量，keras运行时需要这个参数\n",
    "n_words = len(tokenizer_encoder.word_index) + 1\n",
    "n_tags = len(tokenizer_decoder.word_index) + 1\n",
    "\n",
    "# Using the tokenizer to encode and decode the sequences for use in training，用字典来将字符转换为数字\n",
    "# Inputs\n",
    "train_input_data = tokenizer_encoder.texts_to_sequences(train_input_grams) # 将氨基酸序列通过字典,变成数字序列\n",
    "train_input_data = sequence.pad_sequences(train_input_data,\n",
    "                                          maxlen = maxlen_seq, padding='post') # 把不等长的list变成等长,默认填充是０,padding='post'是后填充即后面是0\n",
    "\n",
    "# Targets\n",
    "train_target_data = tokenizer_decoder.texts_to_sequences(train_target_seqs) # 将氨基酸二级结构序列通过字典，变成数字序列\n",
    "train_target_data = sequence.pad_sequences(train_target_data,\n",
    "                                           maxlen = maxlen_seq, padding='post') # 同上\n",
    "train_target_data = to_categorical(train_target_data) # to_categorical就是将类别向量转换为二进制（只有0和1）的矩阵类型表示，此处用途为简易onehot\n",
    "\n",
    "# Use the same tokenizer defined on train for tokenization of test，对测试集进行同上操作\n",
    "test_input_data = tokenizer_encoder.texts_to_sequences(test_input_grams)\n",
    "test_input_data = sequence.pad_sequences(test_input_data,\n",
    "                                         maxlen = maxlen_seq, padding='post')\n",
    "test_target_data = tokenizer_decoder.texts_to_sequences(test_df.expected.values) # 将测试集的氨基酸二级结构序列通过字典，变成数字序列\n",
    "test_target_data = sequence.pad_sequences(test_target_data,\n",
    "                                           maxlen = maxlen_seq, padding='post') # 同上\n",
    "test_target_data = seq2onehot(test_target_data,n_tags) # 将测试集的氨基酸二级结构序列进行onehot\n",
    "\n",
    "\n",
    "train_input_data_alt = train_input_data # 做了个备份\n",
    "train_input_data = seq2onehot(train_input_data, n_words) # 对训练集的data进行onehot\n",
    "train_profiles = train_df.profiles.values # 取出PSSM信息\n",
    "\n",
    "test_input_data_alt = test_input_data # 同上\n",
    "test_input_data = seq2onehot(test_input_data, n_words)\n",
    "test_profiles = test_df.profiles.values\n",
    "\n",
    "# 下面这一段的作用是把5534条不同长度的氨基酸序列中每个氨基酸的PSSM信息，存成一样700长的氨基酸序列中每个氨基酸的PSSM信息，存成三维的np数组，补充出来的那些氨基酸的PSSM信息都是0\n",
    "train_profiles_np = np.zeros((len(train_profiles), maxlen_seq, 22))\n",
    "for i, profile in enumerate(train_profiles):\n",
    "    for j in range(profile.shape[0]):\n",
    "        for k in range(profile.shape[1]):\n",
    "            train_profiles_np[i, j, k] = profile[j, k]\n",
    "\n",
    "# 同上\n",
    "test_profiles_np = np.zeros((len(test_profiles), maxlen_seq, 22))\n",
    "for i, profile in enumerate(test_profiles):\n",
    "    for j in range(profile.shape[0]):\n",
    "        for k in range(profile.shape[1]):\n",
    "            test_profiles_np[i, j, k] = profile[j, k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T13:43:28.908047Z",
     "start_time": "2020-12-24T11:33:41.582545Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 700)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 700, 128)     2816        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 700, 22)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 700, 150)     0           embedding[0][0]                  \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 700, 64)      105664      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 700, 214)     0           concatenate[0][0]                \n",
      "                                                                 conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 700, 64)      67264       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 700, 278)     0           concatenate_1[0][0]              \n",
      "                                                                 conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 700, 64)      89024       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 700, 342)     0           concatenate_2[0][0]              \n",
      "                                                                 conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 700, 64)      53440       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 700, 406)     0           concatenate_3[0][0]              \n",
      "                                                                 conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 700, 256)     548864      concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, 700, 9)       2313        bidirectional[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 869,385\n",
      "Trainable params: 869,385\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "87/87 [==============================] - 14s 164ms/step - loss: 0.4460 - accuracy: 0.8395 - myaccuracy: 0.5008\n",
      "Epoch 2/100\n",
      "87/87 [==============================] - 13s 146ms/step - loss: 0.3182 - accuracy: 0.8854 - myaccuracy: 0.6250\n",
      "Epoch 3/100\n",
      "25/87 [=======>......................] - ETA: 8s - loss: 0.2984 - accuracy: 0.8925 - myaccuracy: 0.6481"
     ]
    }
   ],
   "source": [
    "# 模型训练以及保存信息\n",
    "# 打包一下训练用的数据（data和label）\n",
    "X_train = [train_input_data_alt, train_profiles_np]\n",
    "y_train = train_target_data\n",
    "\n",
    "history, model = train(X_train, y_train) # 模型训练，并且输出训练历史，有可以设置的参数\n",
    "\n",
    "# 画图\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "# 画loss图\n",
    "plt.plot(history.history['loss'],label='train loss')\n",
    "plt.grid(True)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.savefig('train_loss.png', dpi=500, bbox_inches='tight')\n",
    "plt.show()\n",
    "# 画正确率图\n",
    "plt.plot(history.history['myaccuracy'],label='train accuracy')\n",
    "plt.grid(True)\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.savefig('train_accuracy.png', dpi=500, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Save the model as a JSON format，把模型结构和参数权重保存下来\n",
    "model.save_weights(\"cb513_weights_1.h5\")\n",
    "with open(\"model_1.json\", \"w\") as json_file:\n",
    "    json_file.write(model.to_json())\n",
    "\n",
    "# Save training history for parsing，把训练过程保存下来\n",
    "with open(\"history_1.pkl\", \"wb\") as hist_file:\n",
    "    pickle.dump(history.history, hist_file)\n",
    "    \n",
    "# model.load_weights(\"cb513_weights_1.h5\")\n",
    "# with open(\"history_1.pkl\",'rb') as f:\n",
    "#     data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-24T13:43:34.142152Z",
     "start_time": "2020-12-24T13:43:28.909041Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# 测试预测效果\n",
    "# Predict on test dataset and save the output\n",
    "run_test(model,\n",
    "    test_input_data[:],\n",
    "    test_input_data_alt[:],\n",
    "    test_profiles_np[:],\n",
    "    \"cb513_test_1.csv\", \"cb513_test_prob_1.npy\") # 保存的是解码后的和没解码的"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
