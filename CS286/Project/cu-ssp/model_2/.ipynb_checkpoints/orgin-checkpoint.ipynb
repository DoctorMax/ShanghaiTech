{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import backend as K\n",
    "from keras.preprocessing import sequence\n",
    "import Levenshtein\n",
    "import pickle\n",
    "\n",
    "# The custom accuracy metric used for this task\n",
    "def myaccuracy(y_true, y_pred):\n",
    "    y = tf.argmax(y_true, axis =- 1)\n",
    "    y_ = tf.argmax(y_pred, axis =- 1)\n",
    "    mask = tf.greater(y, 0)\n",
    "    return K.cast(K.equal(tf.boolean_mask(y, mask), tf.boolean_mask(y_, mask)), K.floatx())\n",
    "\n",
    "# Maps the sequence to a one-hot encoding\n",
    "def onehot_to_seq(oh_seq, index, length=None):\n",
    "    s = ''\n",
    "    if length is None:\n",
    "        for idx, o in enumerate(oh_seq):\n",
    "            i = np.argmax(o)\n",
    "            if i != 0:\n",
    "                s += index[i]\n",
    "            else:\n",
    "                break\n",
    "    else:\n",
    "        for idx, o in enumerate(oh_seq):\n",
    "            i = np.argmax(o[1:])\n",
    "            if idx < length:\n",
    "                s += index[i+1]\n",
    "            else:\n",
    "                break\n",
    "    return s\n",
    "\n",
    "# prints the results\n",
    "def print_results(x, y_, revsere_decoder_index):\n",
    "    # print(\"input     : \" + str(x))\n",
    "    # print(\"prediction: \" + str(onehot_to_seq(y_, revsere_decoder_index).upper()))\n",
    "    print(str(onehot_to_seq(y_, revsere_decoder_index).upper()))\n",
    "\n",
    "def decode_predictions(y_, revsere_decoder_index, length=None):\n",
    "    return str(onehot_to_seq(y_, revsere_decoder_index, length=length).upper())\n",
    "\n",
    "\n",
    "def predict_all(model, test_df, tokenizer_encoder, tokenizer_decoder, n_gram, augmented_input=None, max_len=None, filepath=\"submission.csv\"):\n",
    "    test_input_ids = test_df['id'].values\n",
    "    test_input_seqs = test_df['input'].values.T\n",
    "    test_input_grams = seq2ngrams(test_input_seqs, n=n_gram)\n",
    "    \n",
    "    revsere_decoder_index = {value:key for key,value in tokenizer_decoder.word_index.items()}\n",
    "\n",
    "    if max_len is None:\n",
    "        max_test_length = max([len(x) for x in test_input_grams])\n",
    "    else:\n",
    "        max_test_length = max_len \n",
    "    test_input_data_full = tokenizer_encoder.texts_to_sequences(test_input_grams)\n",
    "    test_input_data_full = sequence.pad_sequences(test_input_data_full, maxlen = max_test_length, padding = 'post')\n",
    "    if augmented_input is None:\n",
    "        y_test_pred = model.predict(test_input_data_full[:])\n",
    "    else:\n",
    "        y_test_pred = model.predict([test_input_data_full[:], augmented_input])\n",
    "    np.save(filepath.replace(\".csv\", \"_raw_pred.npy\"), y_test_pred)\n",
    "    y_test_pred_decoded = []\n",
    "    for i in range(len(y_test_pred)):\n",
    "        decoded = decode_predictions(y_test_pred[i], revsere_decoder_index, length=len(test_input_grams[i]))\n",
    "        y_test_pred_decoded.append(decoded)\n",
    "    test_pred_df = pd.DataFrame({'id':test_input_ids, \"expected\": y_test_pred_decoded},\n",
    "                                columns = ['id', 'expected'])\n",
    "    if np.all(np.array([len(x) for x in test_pred_df['expected']]) == np.array([len(x) for x in test_df['input']])):\n",
    "        print(\"All length match\")\n",
    "    else:\n",
    "        print(\"Some lengths do not match!\")\n",
    "    test_pred_df.to_csv(filepath, index=False)\n",
    "    return test_pred_df\n",
    "\n",
    "def ham_distance(x, y):\n",
    "    return np.sum([a != b for a, b in zip(x, y)])\n",
    "\n",
    "def edit_score(input_df, pred_df, filepath=\"edit_score.csv\", plot=True):\n",
    "    assert np.all(input_df['id'].values == pred_df['id'].values)\n",
    "    if not np.all(np.array([len(x) for x in pred_df['expected']]) == np.array([len(x) for x in input_df['input']])):\n",
    "        print(\"Some lengths do not match!\")\n",
    "        return None, None \n",
    "    output_df = input_df.copy().reset_index(drop=True)\n",
    "    lev_dist = [Levenshtein.distance(x, y) for x, y in zip(input_df['expected'], pred_df['expected'])]\n",
    "    ham_dist = [ham_distance(x, y) for x, y in zip(input_df['expected'], pred_df['expected'])]\n",
    "    lev_score = np.mean(lev_dist)\n",
    "    ham_score = np.mean(ham_dist)\n",
    "\n",
    "    total_ham = np.sum(ham_dist)\n",
    "    total_len = input_df['expected'].map(len).sum()\n",
    "    accuracy = 1 - total_ham / total_len\n",
    "\n",
    "    output_df['predicted'] = pred_df['expected'].values\n",
    "    output_df['levdist'] = np.array(lev_dist)\n",
    "    output_df['hamdist'] = np.array(ham_dist)\n",
    "    output_df['levpercent'] = output_df['levdist'] / output_df['len']\n",
    "    output_df['hampercent'] = output_df['hamdist'] / output_df['len']\n",
    "    output_df['accuracy'] = 1 - output_df['hampercent']\n",
    "    ham_percent = np.mean(output_df['hampercent'])\n",
    "    mean_acc = np.mean(output_df['accuracy'])\n",
    "\n",
    "    output_df.to_csv(filepath, index=False)\n",
    "    print_str = \"total acc: {:.4f}, mean acc: {:.4f}, lev: {:.1f}, ham: {:.1f}\".format(accuracy, mean_acc, lev_score, ham_score)\n",
    "    print(print_str)\n",
    "    output_df.plot(\"len\", \"accuracy\", kind=\"scatter\")\n",
    "    plt.hlines(y=accuracy, xmin=0, xmax=output_df['len'].max())\n",
    "    plt.title(print_str)\n",
    "    plt.savefig(filepath.replace(\".csv\", \"_plot.png\"))\n",
    "    if plot:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "    return accuracy, output_df\n",
    "\n",
    "# Computes and returns the n-grams of a particualr sequence, defaults to trigrams\n",
    "def seq2ngrams(seqs, n = 3):\n",
    "    return np.array([[seq[i : i + n] for i in range(len(seq))] for seq in seqs])\n",
    "\n",
    "def load_augmented_data(npy_path, max_len, centered=False):\n",
    "    data = np.load(npy_path)\n",
    "    residue_list = ['A', 'C', 'E', 'D', 'G', 'F', 'I', 'H', 'K', 'M', 'L', 'N', 'Q', 'P', 'S', 'R', 'T', 'W', 'V', 'Y', 'X','NoSeq']\n",
    "    q8_list = ['L', 'B', 'E', 'G', 'I', 'H', 'S', 'T','NoSeq']\n",
    "\n",
    "    data_reshape = data.reshape(data.shape[0], 700, -1)\n",
    "    residue_onehot = data_reshape[:,:,0:22]\n",
    "    residue_q8_onehot = data_reshape[:,:,22:31]\n",
    "    profile = data_reshape[:,:,35:57]\n",
    "    # if centered:\n",
    "    #     profile = profile - 0.5  # range [0,1]\n",
    "\n",
    "    if max_len > profile.shape[1]:\n",
    "        zero_arr = np.zeros((profile.shape[0], max_len - profile.shape[1], profile.shape[2]))\n",
    "        zero_arr[:,:,-1] = 1.0\n",
    "        profile_padded = np.concatenate([profile, zero_arr], axis=1)\n",
    "    else:\n",
    "        profile_padded = profile\n",
    "\n",
    "    residue_array = np.array(residue_list)[residue_onehot.argmax(2)]\n",
    "    q8_array = np.array(q8_list)[residue_q8_onehot.argmax(2)]\n",
    "    residue_str_list = []\n",
    "    q8_str_list = []\n",
    "    for vec in residue_array:\n",
    "        x = ''.join(vec[vec != 'NoSeq'])\n",
    "        residue_str_list.append(x)\n",
    "    for vec in q8_array:\n",
    "        x = ''.join(vec[vec != 'NoSeq'])\n",
    "        q8_str_list.append(x)\n",
    "\n",
    "    id_list = np.arange(1, len(residue_array) + 1)\n",
    "    len_list = np.array([len(x) for x in residue_str_list])\n",
    "\n",
    "    train_df = pd.DataFrame({'id': id_list, 'len': len_list, 'input': residue_str_list, 'expected': q8_str_list})\n",
    "    return train_df, profile_padded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 128)    2816        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None, 22)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, None, 150)    0           embedding[0][0]                  \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, None, 128)    57728       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, None, 128)    512         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu (ReLU)                    (None, None, 128)    0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, None, 128)    49280       re_lu[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, None, 128)    0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, None, 128)    512         dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_1 (ReLU)                  (None, None, 128)    0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, None, 128)    49280       re_lu_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, None, 128)    0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, None, 128)    512         max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_2 (ReLU)                  (None, None, 128)    0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, None, 192)    73920       re_lu_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, None, 192)    0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, None, 192)    768         dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_3 (ReLU)                  (None, None, 192)    0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, None, 192)    110784      re_lu_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, None, 192)    0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, None, 192)    768         max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_4 (ReLU)                  (None, None, 192)    0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, None, 384)    221568      re_lu_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, None, 384)    0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, None, 384)    1536        dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_5 (ReLU)                  (None, None, 384)    0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, None, 384)    442752      re_lu_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, None, 384)    0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, None, 384)    1536        max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_6 (ReLU)                  (None, None, 384)    0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, None, 768)    885504      re_lu_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, None, 768)    0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, None, 768)    3072        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_7 (ReLU)                  (None, None, 768)    0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, None, 768)    1770240     re_lu_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, None, 768)    0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, None, 768)    3072        max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_8 (ReLU)                  (None, None, 768)    0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, None, 1536)   3540480     re_lu_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, None, 1536)   0           conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, None, 1536)   6144        dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_9 (ReLU)                  (None, None, 1536)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, None, 1536)   7079424     re_lu_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, None, 1536)   6144        conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_10 (ReLU)                 (None, None, 1536)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling1d (UpSampling1D)    (None, None, 1536)   0           re_lu_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, None, 768)    2360064     up_sampling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, 1536)   0           conv1d_8[0][0]                   \n",
      "                                                                 conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, None, 1536)   6144        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_11 (ReLU)                 (None, None, 1536)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, None, 768)    3539712     re_lu_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, None, 768)    0           conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, None, 768)    3072        dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_12 (ReLU)                 (None, None, 768)    0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, None, 768)    1770240     re_lu_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, None, 768)    3072        conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_13 (ReLU)                 (None, None, 768)    0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling1d_1 (UpSampling1D)  (None, None, 768)    0           re_lu_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, None, 384)    590208      up_sampling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, None, 768)    0           conv1d_6[0][0]                   \n",
      "                                                                 conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, None, 768)    3072        concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_14 (ReLU)                 (None, None, 768)    0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, None, 384)    885120      re_lu_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, None, 384)    0           conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, None, 384)    1536        dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_15 (ReLU)                 (None, None, 384)    0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, None, 384)    442752      re_lu_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, None, 384)    1536        conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_16 (ReLU)                 (None, None, 384)    0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling1d_2 (UpSampling1D)  (None, None, 384)    0           re_lu_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, None, 192)    147648      up_sampling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, None, 384)    0           conv1d_4[0][0]                   \n",
      "                                                                 conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, None, 384)    1536        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_17 (ReLU)                 (None, None, 384)    0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, None, 192)    221376      re_lu_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, None, 192)    0           conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, None, 192)    768         dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_18 (ReLU)                 (None, None, 192)    0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, None, 192)    110784      re_lu_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, None, 192)    768         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_19 (ReLU)                 (None, None, 192)    0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling1d_3 (UpSampling1D)  (None, None, 192)    0           re_lu_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, None, 128)    49280       up_sampling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, None, 256)    0           conv1d_2[0][0]                   \n",
      "                                                                 conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, None, 256)    1024        concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_20 (ReLU)                 (None, None, 256)    0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, None, 128)    98432       re_lu_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, None, 128)    0           conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, None, 128)    512         dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_21 (ReLU)                 (None, None, 128)    0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, None, 128)    49280       re_lu_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, None, 128)    512         conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_22 (ReLU)                 (None, None, 128)    0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, None, 9)      1161        re_lu_22[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 24,597,961\n",
      "Trainable params: 24,573,897\n",
      "Non-trainable params: 24,064\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0020000000949949026.\n",
      "Epoch 1/90\n",
      " 1/41 [..............................] - ETA: 0s - loss: 2.5436 - accuracy: 0.0454 - myaccuracy: 0.0933WARNING:tensorflow:From l:\\anaconda3\\envs\\cs286\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.4444 - accuracy: 0.8561 - myaccuracy: 0.5454\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.79844, saving model to ../logs/20201224-191136-script_name\\best_val_acc.h5\n",
      "41/41 [==============================] - 69s 2s/step - loss: 0.4444 - accuracy: 0.8561 - myaccuracy: 0.5454 - val_loss: 1.0186 - val_accuracy: 0.7984 - val_myaccuracy: 0.2545\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0020000000949949026.\n",
      "Epoch 2/90\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.2847 - accuracy: 0.8984 - myaccuracy: 0.6362\n",
      "Epoch 00002: val_accuracy did not improve from 0.79844\n",
      "41/41 [==============================] - 73s 2s/step - loss: 0.2847 - accuracy: 0.8984 - myaccuracy: 0.6362 - val_loss: 2.2745 - val_accuracy: 0.7927 - val_myaccuracy: 0.2956\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0020000000949949026.\n",
      "Epoch 3/90\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.2639 - accuracy: 0.9049 - myaccuracy: 0.6591\n",
      "Epoch 00003: val_accuracy did not improve from 0.79844\n",
      "41/41 [==============================] - 70s 2s/step - loss: 0.2639 - accuracy: 0.9049 - myaccuracy: 0.6591 - val_loss: 3.4279 - val_accuracy: 0.7539 - val_myaccuracy: 0.2428\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0020000000949949026.\n",
      "Epoch 4/90\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.2526 - accuracy: 0.9084 - myaccuracy: 0.6716\n",
      "Epoch 00004: val_accuracy did not improve from 0.79844\n",
      "41/41 [==============================] - 68s 2s/step - loss: 0.2526 - accuracy: 0.9084 - myaccuracy: 0.6716 - val_loss: 1.6140 - val_accuracy: 0.7686 - val_myaccuracy: 0.2599\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0020000000949949026.\n",
      "Epoch 5/90\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.2452 - accuracy: 0.9110 - myaccuracy: 0.6809\n",
      "Epoch 00005: val_accuracy improved from 0.79844 to 0.80254, saving model to ../logs/20201224-191136-script_name\\best_val_acc.h5\n",
      "41/41 [==============================] - 74s 2s/step - loss: 0.2452 - accuracy: 0.9110 - myaccuracy: 0.6809 - val_loss: 1.1112 - val_accuracy: 0.8025 - val_myaccuracy: 0.3105\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.0020000000949949026.\n",
      "Epoch 6/90\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.2411 - accuracy: 0.9124 - myaccuracy: 0.6859\n",
      "Epoch 00006: val_accuracy improved from 0.80254 to 0.90074, saving model to ../logs/20201224-191136-script_name\\best_val_acc.h5\n",
      "41/41 [==============================] - 64s 2s/step - loss: 0.2411 - accuracy: 0.9124 - myaccuracy: 0.6859 - val_loss: 0.3060 - val_accuracy: 0.9007 - val_myaccuracy: 0.6336\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.0020000000949949026.\n",
      "Epoch 7/90\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.2354 - accuracy: 0.9143 - myaccuracy: 0.6928\n",
      "Epoch 00007: val_accuracy did not improve from 0.90074\n",
      "41/41 [==============================] - 60s 1s/step - loss: 0.2354 - accuracy: 0.9143 - myaccuracy: 0.6928 - val_loss: 0.3012 - val_accuracy: 0.8984 - val_myaccuracy: 0.6235\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.0020000000949949026.\n",
      "Epoch 8/90\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.2321 - accuracy: 0.9155 - myaccuracy: 0.6971\n",
      "Epoch 00008: val_accuracy did not improve from 0.90074\n",
      "41/41 [==============================] - 109s 3s/step - loss: 0.2321 - accuracy: 0.9155 - myaccuracy: 0.6971 - val_loss: 0.4014 - val_accuracy: 0.8834 - val_myaccuracy: 0.5680\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.0020000000949949026.\n",
      "Epoch 9/90\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.2297 - accuracy: 0.9165 - myaccuracy: 0.7007\n",
      "Epoch 00009: val_accuracy improved from 0.90074 to 0.91134, saving model to ../logs/20201224-191136-script_name\\best_val_acc.h5\n",
      "41/41 [==============================] - 54s 1s/step - loss: 0.2297 - accuracy: 0.9165 - myaccuracy: 0.7007 - val_loss: 0.2586 - val_accuracy: 0.9113 - val_myaccuracy: 0.6715\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0020000000949949026.\n",
      "Epoch 10/90\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.2260 - accuracy: 0.9177 - myaccuracy: 0.7048\n",
      "Epoch 00010: val_accuracy improved from 0.91134 to 0.91806, saving model to ../logs/20201224-191136-script_name\\best_val_acc.h5\n",
      "41/41 [==============================] - 60s 1s/step - loss: 0.2260 - accuracy: 0.9177 - myaccuracy: 0.7048 - val_loss: 0.2317 - val_accuracy: 0.9181 - val_myaccuracy: 0.6964\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.0020000000949949026.\n",
      "Epoch 11/90\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.2231 - accuracy: 0.9187 - myaccuracy: 0.7084\n",
      "Epoch 00011: val_accuracy improved from 0.91806 to 0.91937, saving model to ../logs/20201224-191136-script_name\\best_val_acc.h5\n",
      "41/41 [==============================] - 52s 1s/step - loss: 0.2231 - accuracy: 0.9187 - myaccuracy: 0.7084 - val_loss: 0.2244 - val_accuracy: 0.9194 - val_myaccuracy: 0.7013\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.0020000000949949026.\n",
      "Epoch 12/90\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.2219 - accuracy: 0.9191 - myaccuracy: 0.7098\n",
      "Epoch 00012: val_accuracy did not improve from 0.91937\n",
      "41/41 [==============================] - 77s 2s/step - loss: 0.2219 - accuracy: 0.9191 - myaccuracy: 0.7098 - val_loss: 0.3035 - val_accuracy: 0.9035 - val_myaccuracy: 0.6426\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.0020000000949949026.\n",
      "Epoch 13/90\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.2194 - accuracy: 0.9201 - myaccuracy: 0.7134\n",
      "Epoch 00013: val_accuracy did not improve from 0.91937\n",
      "41/41 [==============================] - 67s 2s/step - loss: 0.2194 - accuracy: 0.9201 - myaccuracy: 0.7134 - val_loss: 0.2527 - val_accuracy: 0.9144 - val_myaccuracy: 0.6827\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.0020000000949949026.\n",
      "Epoch 14/90\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.2181 - accuracy: 0.9204 - myaccuracy: 0.7147\n",
      "Epoch 00014: val_accuracy did not improve from 0.91937\n",
      "41/41 [==============================] - 117s 3s/step - loss: 0.2181 - accuracy: 0.9204 - myaccuracy: 0.7147 - val_loss: 0.3086 - val_accuracy: 0.9042 - val_myaccuracy: 0.6452\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.0020000000949949026.\n",
      "Epoch 15/90\n",
      " 6/41 [===>..........................] - ETA: 52s - loss: 0.2182 - accuracy: 0.9205 - myaccuracy: 0.7201"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.metrics import categorical_accuracy\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import TensorBoard, LearningRateScheduler, ModelCheckpoint, ReduceLROnPlateau\n",
    "from datetime import datetime\n",
    "import os, pickle\n",
    "\n",
    "\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "from tensorflow.compat.v1 import ConfigProto \n",
    "from tensorflow.compat.v1 import InteractiveSession \n",
    "config = ConfigProto() \n",
    "config.gpu_options.allow_growth = True \n",
    "session = InteractiveSession(config=config)\n",
    "\n",
    "\n",
    "maxlen_seq = 768\n",
    "\n",
    "cb513filename =\"../data/cb513+profile_split1.npy.gz\"\n",
    "cb6133filteredfilename = \"../data/cullpdb+profile_6133_filtered.npy.gz\"\n",
    "\n",
    "\n",
    "train_df, train_augment_data = load_augmented_data(cb6133filteredfilename, maxlen_seq)\n",
    "test_df, test_augment_data = load_augmented_data(cb513filename, maxlen_seq)\n",
    "\n",
    "n_samples = len(train_df)\n",
    "np.random.seed(0)\n",
    "validation_idx = np.random.choice(np.arange(n_samples), size=300, replace=False)\n",
    "training_idx = np.array(list(set(np.arange(n_samples))-set(validation_idx)))\n",
    "\n",
    "val_df = train_df.iloc[validation_idx]\n",
    "\n",
    "# Loading and converting the inputs to ngrams\n",
    "train_input_seqs, train_target_seqs = train_df[['input', 'expected']].values.T\n",
    "train_input_grams = seq2ngrams(train_input_seqs, n=1)\n",
    "\n",
    "# Initializing and defining the tokenizer encoders and decoders based on the train set\n",
    "tokenizer_encoder = Tokenizer()\n",
    "tokenizer_encoder.fit_on_texts(train_input_grams)\n",
    "tokenizer_decoder = Tokenizer(char_level = True)\n",
    "tokenizer_decoder.fit_on_texts(train_target_seqs)\n",
    "\n",
    "# Using the tokenizer to encode and decode the sequences for use in training\n",
    "# Inputs\n",
    "train_input_data = tokenizer_encoder.texts_to_sequences(train_input_grams)\n",
    "train_input_data = sequence.pad_sequences(train_input_data, maxlen = maxlen_seq, padding = 'post', truncating='post')\n",
    "\n",
    "# Targets\n",
    "train_target_data = tokenizer_decoder.texts_to_sequences(train_target_seqs)\n",
    "train_target_data = sequence.pad_sequences(train_target_data, maxlen = maxlen_seq, padding = 'post', truncating='post')\n",
    "train_target_data = to_categorical(train_target_data)\n",
    "\n",
    "# Computing the number of words and number of tags to be passed as parameters to the keras model\n",
    "n_words = len(tokenizer_encoder.word_index) + 1\n",
    "n_tags = len(tokenizer_decoder.word_index) + 1\n",
    "\n",
    "############################################\n",
    "# Splitting the data for train and validation sets\n",
    "\n",
    "X_val = train_input_data[validation_idx]\n",
    "X_train = train_input_data[training_idx]\n",
    "y_val = train_target_data[validation_idx]\n",
    "y_train = train_target_data[training_idx]\n",
    "\n",
    "X_train_augment = train_augment_data[training_idx]\n",
    "X_val_augment = train_augment_data[validation_idx]\n",
    "\n",
    "############################################\n",
    "# save preprocessed val and test data and tokenizer\n",
    "\n",
    "# script_name = os.path.basename(__file__).split(\".\")[0]\n",
    "script_name='script_name'\n",
    "model_name = datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"-\" + script_name\n",
    "log_dir = '../logs/{}'.format(model_name)\n",
    "os.mkdir(log_dir)\n",
    "\n",
    "val_df.to_csv(os.path.join(log_dir, 'val_data.csv'))\n",
    "np.save(os.path.join(log_dir, 'val_augment_data.npy'), X_val_augment)\n",
    "test_df.to_csv(os.path.join(log_dir, 'test_data.csv'))\n",
    "np.save(os.path.join(log_dir, 'test_augment_data.npy'), test_augment_data)\n",
    "\n",
    "with open(os.path.join(log_dir, 'tokenizer_encoder.pickle'), 'wb') as handle:\n",
    "    pickle.dump(tokenizer_encoder, handle)\n",
    "\n",
    "with open(os.path.join(log_dir, 'tokenizer_decoder.pickle'), 'wb') as handle:\n",
    "    pickle.dump(tokenizer_decoder, handle)\n",
    "\n",
    "############################################\n",
    "# Dropout to prevent overfitting. \n",
    "droprate = 0.3\n",
    "\n",
    "\n",
    "def conv_block(x, n_channels, droprate):\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv1D(n_channels, 3, padding = 'same', kernel_initializer = 'he_normal')(x) \n",
    "    x = Dropout(droprate)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv1D(n_channels, 3, padding = 'same', kernel_initializer = 'he_normal')(x)\n",
    "    return x \n",
    "\n",
    "def up_block(x, n_channels):\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    x = UpSampling1D(size = 2)(x)\n",
    "    x = Conv1D(n_channels, 2, padding = 'same', kernel_initializer = 'he_normal')(x)\n",
    "    return x\n",
    "\n",
    "input = Input(shape = (None, ))\n",
    "augment_input = Input(shape = (None, 22))\n",
    "\n",
    "# Defining an embedding layer mapping from the words (n_words) to a vector of len 128\n",
    "embed_input = Embedding(input_dim = n_words, output_dim = 128, input_length = None)(input)\n",
    "\n",
    "merged_input = concatenate([embed_input, augment_input], axis = 2)\n",
    "merged_input = Conv1D(128, 3, padding = 'same', kernel_initializer = 'he_normal')(merged_input) \n",
    "\n",
    "conv1 = conv_block(merged_input, 128, droprate)\n",
    "pool1 = MaxPooling1D(pool_size=2)(conv1)\n",
    "\n",
    "conv2 = conv_block(pool1, 192, droprate)\n",
    "pool2 = MaxPooling1D(pool_size=2)(conv2)\n",
    "\n",
    "conv3 = conv_block(pool2, 384, droprate)\n",
    "pool3 = MaxPooling1D(pool_size=2)(conv3)\n",
    "\n",
    "conv4 = conv_block(pool3, 768, droprate)\n",
    "pool4 = MaxPooling1D(pool_size=2)(conv4)\n",
    "\n",
    "conv5 = conv_block(pool4, 1536, droprate)\n",
    "\n",
    "up4 = up_block(conv5, 768)\n",
    "up4 = concatenate([conv4,up4], axis = 2)\n",
    "up4 = conv_block(up4, 768, droprate)\n",
    "\n",
    "up3 = up_block(up4, 384)\n",
    "up3 = concatenate([conv3,up3], axis = 2)\n",
    "up3 = conv_block(up3, 384, droprate)\n",
    "\n",
    "up2 = up_block(up3, 192)\n",
    "up2 = concatenate([conv2,up2], axis = 2)\n",
    "up2 = conv_block(up2, 192, droprate)\n",
    "\n",
    "up1 = up_block(up2, 128)\n",
    "up1 = concatenate([conv1,up1], axis = 2)\n",
    "up1 = conv_block(up1, 128, droprate)\n",
    "\n",
    "up1 = BatchNormalization()(up1)\n",
    "up1 = ReLU()(up1)\n",
    "\n",
    "# the following it equivalent to Conv1D with kernel size 1\n",
    "# A dense layer to output from the LSTM's64 units to the appropriate number of tags to be fed into the decoder\n",
    "y = TimeDistributed(Dense(n_tags, activation = \"softmax\"))(up1)\n",
    "\n",
    "\n",
    "# Defining the model as a whole and printing the summary\n",
    "model = Model([input, augment_input], y)\n",
    "model.summary()\n",
    "\n",
    "optim = RMSprop(lr=0.002)\n",
    "\n",
    "def scheduler(i, lr):\n",
    "    if i in [60]:\n",
    "        return lr * 0.5\n",
    "    return lr\n",
    "\n",
    "reduce_lr = LearningRateScheduler(schedule=scheduler, verbose=1)\n",
    "# reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.5,\n",
    "#                             patience=8, min_lr=0.0005, verbose=1)\n",
    "\n",
    "# Setting up the model with categorical x-entropy loss and the custom accuracy function as accuracy\n",
    "model.compile(optimizer = optim, loss = \"categorical_crossentropy\", metrics = [\"accuracy\", myaccuracy])\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=log_dir)\n",
    "\n",
    "checkpoint = ModelCheckpoint(os.path.join(log_dir, \"best_val_acc.h5\"),\n",
    "                            monitor='val_accuracy',\n",
    "                            verbose=1,\n",
    "                            save_best_only=True,\n",
    "                            mode='max')\n",
    "\n",
    "# Training the model on the training data and validating using the validation set\n",
    "model.fit([X_train, X_train_augment], y_train, batch_size = 128, \n",
    "            validation_data = ([X_val, X_val_augment], y_val), verbose = 1,\n",
    "            callbacks=[tensorboard, reduce_lr, checkpoint], \n",
    "            epochs = 90)\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "model = load_model(os.path.join(log_dir, \"best_val_acc.h5\"))\n",
    "\n",
    "val_pred_df = predict_all(model, val_df, tokenizer_encoder, tokenizer_decoder, n_gram=1,  max_len=maxlen_seq, \n",
    "                            augmented_input=X_val_augment,\n",
    "                            filepath = os.path.join(log_dir, \"val_pred_{}.csv\".format(model_name)))\n",
    "val_score, val_score_df = edit_score(val_df, val_pred_df,\n",
    "                                    filepath = os.path.join(log_dir, \"val_score_{}.csv\".format(model_name)), plot=False)\n",
    "plt.close()\n",
    "K.clear_session()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
