{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def read_image_and_name(path):\n",
    "    imgdir = os.listdir(path)\n",
    "    imglst = []\n",
    "    imgs = []\n",
    "    for v in imgdir:\n",
    "        imglst.append(path + v)\n",
    "        imgs.append(cv2.imread(path + v))\n",
    "    print(imglst)\n",
    "    print('original images shape: ' + str(np.array(imgs).shape))\n",
    "    return imglst,imgs\n",
    "\n",
    "def read_label_and_name(path):\n",
    "    labeldir = os.listdir(path)\n",
    "    labellst = []\n",
    "    labels = []\n",
    "    for v in labeldir:\n",
    "        labellst.append(path + v)\n",
    "        labels.append(np.asarray(Image.open(path + v)))\n",
    "    print(labellst)\n",
    "    print('original labels shape: ' + str(np.array(labels).shape))\n",
    "    return labellst,labels\n",
    "\n",
    "def resize(imgs,resize_height, resize_width):\n",
    "    img_resize = []\n",
    "    for file in imgs:\n",
    "        img_resize.append(cv2.resize(file,(resize_height,resize_width)))\n",
    "    return img_resize\n",
    "\n",
    "\n",
    "def SaltAndPepper(src,percetage):  \n",
    "    list=[]\n",
    "    for j in range(src.shape[0]):\n",
    "        SP_NoiseImg=src[j].copy()\n",
    "        SP_NoiseNum=int(percetage*src[j].shape[0]*src[j].shape[1]) \n",
    "        for i in range(SP_NoiseNum): \n",
    "            randR=np.random.randint(0,src[j].shape[0]-1) \n",
    "            randG=np.random.randint(0,src[j].shape[1]-1) \n",
    "            randB=np.random.randint(0,3)\n",
    "            if np.random.randint(0,1)==0: \n",
    "                SP_NoiseImg[randR,randG,randB]=0 \n",
    "            else: \n",
    "                SP_NoiseImg[randR,randG,randB]=255 \n",
    "        list.append(src[j])\n",
    "        list.append(SP_NoiseImg)\n",
    "    return np.array(list)\n",
    "\n",
    "def rotate(image, angle=15, scale=0.9):\n",
    "    list=[]\n",
    "    for j in range(image.shape[0]):\n",
    "        w = image[j].shape[1]\n",
    "        h = image[j].shape[0]\n",
    "        #rotate matrix\n",
    "        M = cv2.getRotationMatrix2D((w/2,h/2), angle, scale)\n",
    "        #rotate\n",
    "        image2 = cv2.warpAffine(image[j],M,(w,h))\n",
    "        list.append(image[j])\n",
    "        list.append(image2)\n",
    "    return np.array(list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#将N张576x576的图片裁剪成48x48\n",
    "def crop(image,dx):\n",
    "    list = []\n",
    "    for i in range(image.shape[0]):\n",
    "        for x in range(image.shape[1] // dx):\n",
    "            for y in range(image.shape[2] // dx):\n",
    "                list.append(image[ i,  y*dx : (y+1)*dx,  x*dx : (x+1)*dx]) #这里的list一共append了20x12x12=2880次所以返回的shape是(2880,48,48)\n",
    "    return np.array(list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def dice_coefficient(a,b):\n",
    "    return 2.*np.sum(a*b)/(np.sum(a)+np.sum(b))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 网络预测输出转换成图像子块\n",
    "# 网络预测输出 size=[Npatches, patch_height*patch_width, 2]\n",
    "def pred_to_imgs(pred, patch_height, patch_width, mode=\"original\"):\n",
    "    assert (len(pred.shape)==3)  #3D array: (Npatches,height*width,2)\n",
    "    assert (pred.shape[2]==2 )  #check the classes are 2  # 确认是否为二分类\n",
    "    pred_images = np.empty((pred.shape[0],pred.shape[1]))  #(Npatches,height*width)\n",
    "    if mode==\"original\": # 网络概率输出\n",
    "        for i in range(pred.shape[0]):\n",
    "            for pix in range(pred.shape[1]):\n",
    "                pred_images[i,pix]=pred[i,pix,1] #pred[:, :, 0] 是反分割图像输出 pred[:, :, 1]是分割输出\n",
    "    elif mode==\"threshold\": # 网络概率-阈值输出\n",
    "        for i in range(pred.shape[0]):\n",
    "            for pix in range(pred.shape[1]):\n",
    "                if pred[i,pix,1]>=0.5:\n",
    "                    pred_images[i,pix]=1\n",
    "                else:\n",
    "                    pred_images[i,pix]=0\n",
    "    else:\n",
    "        print(\"mode \" +str(mode) +\" not recognized, it can be 'original' or 'threshold'\")\n",
    "        exit()\n",
    "    # 输出形式改写成(Npatches,1, patch_height, patch_width)\n",
    "    pred_images = np.reshape(pred_images,(pred_images.shape[0],1, patch_height, patch_width))\n",
    "    return pred_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32ml:\\anaconda3\\envs\\cs286\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     63\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pywrap_tensorflow_internal\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m   \u001b[1;31m# This try catch logic is because there is no bazel equivalent for py_extension.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed: 找不到指定的模块。",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32ml:\\anaconda3\\envs\\cs286\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomRotation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32ml:\\anaconda3\\envs\\cs286\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlazy_loader\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLazyLoader\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_LazyLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32ml:\\anaconda3\\envs\\cs286\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32ml:\\anaconda3\\envs\\cs286\\lib\\site-packages\\tensorflow\\python\\eager\\context.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrewriter_config_pb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tfe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtf2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32ml:\\anaconda3\\envs\\cs286\\lib\\site-packages\\tensorflow\\python\\pywrap_tfe.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;31m# pylint: disable=invalid-import-order,g-bad-import-order, wildcard-import, unused-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pywrap_tfe\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32ml:\\anaconda3\\envs\\cs286\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     82\u001b[0m above this error message when asking for help.\"\"\" % traceback.format_exc()\n\u001b[1;32m---> 83\u001b[1;33m   \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"l:\\anaconda3\\envs\\cs286\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed: 找不到指定的模块。\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/errors\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-8b276d77e3f3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMaxPooling2D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mUpSampling2D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mReshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDropout\u001b[0m \u001b[1;31m#core内部定义了一系列常用的网络层，包括全连接、激活层等\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAdam\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLearningRateScheduler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32ml:\\anaconda3\\envs\\cs286\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     raise ImportError(\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[1;34m'Keras requires TensorFlow 2.2 or higher. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         'Install TensorFlow via `pip install tensorflow`')\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, concatenate, Conv2D, MaxPooling2D, UpSampling2D, Reshape, core, Dropout #core内部定义了一系列常用的网络层，包括全连接、激活层等\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "\n",
    "def get_unet(n_ch,patch_height,patch_width):\n",
    "    inputs = Input(shape=(n_ch,patch_height,patch_width))\n",
    "    #data_format：字符串，“channels_first”或“channels_last”之一，代表图像的通道维的位置。\n",
    "    #以128x128的RGB图像为例，“channels_first”应将数据组织为（3,128,128），而“channels_last”应将数据组织为（128,128,3）。该参数的默认值是~/.keras/keras.json中设置的值，若从未设置过，则为“channels_last”。\n",
    "    conv1 = Conv2D(64, (3, 3), activation='relu', padding='same',data_format='channels_first')(inputs)\n",
    "    conv1 = Dropout(0.2)(conv1)\n",
    "    conv1 = Conv2D(64, (3, 3), activation='relu', padding='same',data_format='channels_first')(conv1)\n",
    "    pool1 = MaxPooling2D((2, 2))(conv1)\n",
    "    #\n",
    "    conv2 = Conv2D(128, (3, 3), activation='relu', padding='same',data_format='channels_first')(pool1)\n",
    "    conv2 = Dropout(0.2)(conv2)\n",
    "    conv2 = Conv2D(128, (3, 3), activation='relu', padding='same',data_format='channels_first')(conv2)\n",
    "    pool2 = MaxPooling2D((2, 2))(conv2)\n",
    "    #\n",
    "    conv3 = Conv2D(256, (3, 3), activation='relu', padding='same',data_format='channels_first')(pool2)\n",
    "    conv3 = Dropout(0.2)(conv3)\n",
    "    conv3 = Conv2D(256, (3, 3), activation='relu', padding='same',data_format='channels_first')(conv3)\n",
    "    pool3 = MaxPooling2D((2, 2))(conv3)\n",
    "    #\n",
    "    conv4 = Conv2D(512, (3, 3), activation='relu', padding='same',data_format='channels_first')(pool3)\n",
    "    conv4 = Dropout(0.2)(conv4)\n",
    "    conv4 = Conv2D(512, (3, 3), activation='relu', padding='same',data_format='channels_first')(conv4)\n",
    "    pool4 = MaxPooling2D((2, 2))(conv4)\n",
    "    #\n",
    "    conv5 = Conv2D(1024, (3, 3), activation='relu', padding='same',data_format='channels_first')(pool4)\n",
    "    conv5 = Dropout(0.2)(conv5)\n",
    "    conv5 = Conv2D(1024, (3, 3), activation='relu', padding='same',data_format='channels_first')(conv5)\n",
    "    up1 = UpSampling2D(size=(2, 2))(conv5)\n",
    "    up1 = concatenate([conv4,up1],axis=1)\n",
    "    conv6 = Conv2D(512, (3, 3), activation='relu', padding='same',data_format='channels_first')(up1)\n",
    "    conv6 = Dropout(0.2)(conv6)\n",
    "    conv6 = Conv2D(512, (3, 3), activation='relu', padding='same',data_format='channels_first')(conv6)\n",
    "    #\n",
    "    up2 = UpSampling2D(size=(2, 2))(conv6)\n",
    "    up2 = concatenate([conv3,up2], axis=1)\n",
    "    conv7 = Conv2D(256, (3, 3), activation='relu', padding='same',data_format='channels_first')(up2)\n",
    "    conv7 = Dropout(0.2)(conv7)\n",
    "    conv7 = Conv2D(256, (3, 3), activation='relu', padding='same',data_format='channels_first')(conv7)\n",
    "    #\n",
    "    up3 = UpSampling2D(size=(2, 2))(conv7)\n",
    "    up3 = concatenate([conv2,up3], axis=1)\n",
    "    conv8 = Conv2D(128, (3, 3), activation='relu', padding='same',data_format='channels_first')(up3)\n",
    "    conv8 = Dropout(0.2)(conv8)\n",
    "    conv8 = Conv2D(128, (3, 3), activation='relu', padding='same',data_format='channels_first')(conv8)\n",
    "    #\n",
    "    up4 = UpSampling2D(size=(2, 2))(conv8)\n",
    "    up4 = concatenate([conv1,up4], axis=1)\n",
    "    conv9 = Conv2D(64, (3, 3), activation='relu', padding='same',data_format='channels_first')(up4)\n",
    "    conv9 = Dropout(0.2)(conv9)\n",
    "    conv9 = Conv2D(64, (3, 3), activation='relu', padding='same',data_format='channels_first')(conv9)\n",
    "    #\n",
    "    #1×1的卷积的作用\n",
    "\t#大概有两个方面的作用：1. 实现跨通道的交互和信息整合2. 进行卷积核通道数的降维和升维。\n",
    "    conv10 = Conv2D(2, (1, 1), activation='relu',padding='same',data_format='channels_first')(conv9)\n",
    "    conv10 = core.Reshape((2,patch_height*patch_width))(conv10) #此时output的shape是(batchsize,2,patch_height*patch_width)\n",
    "    conv10 = core.Permute((2,1))(conv10)    #此时output的shape是(Npatch,patch_height*patch_width,2)即输出维度是(Npatch,2304,2)\n",
    "    ############\n",
    "    conv11 = core.Activation('softmax')(conv10)\n",
    "    model = Model(inputs=inputs, outputs=conv11)\n",
    "    # sgd = SGD(lr=0.01, decay=1e-6, momentum=0.3, nesterov=False)\n",
    "    model.compile(optimizer=Adam(lr=0.001), loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "    '''\n",
    "    模型Model的compile方法:\n",
    "\tcompile(self, optimizer, loss, metrics=None, loss_weights=None, sample_weight_mode=None, weighted_metrics = None, target_tensors=None)\n",
    "\t本函数编译模型以供训练，参数有\n",
    "\toptimizer：         优化器，为预定义优化器名或优化器对.可以在调用model.compile()之前初始化一个优化器对象，然后传入该函数。\n",
    "\tloss：              损失函数，为预定义损失函数名或一个目标函数\n",
    "\tmetrics：           列表，包含评估模型在训练和测试时的性能的指标，典型用法是metrics=['accuracy']如果要在多输出模型中为不同的输出指定不同的指标，可像该参数传递一个字典，例如metrics={'ouput_a': 'accuracy'}\n",
    "\tsample_weight_mode：如果需要按时间步为样本赋权（2D权矩阵），将该值设为“temporal”。默认为“None”，代表按样本赋权（1D权）。如果模型有多个输出，可以向该参数传入指定sample_weight_mode的字典或列表。在下面fit函数的解释中有相关的参考内容。\n",
    "\tweighted_metrics:   metrics列表，在训练和测试过程中，这些metrics将由sample_weight或clss_weight计算并赋权\n",
    "\ttarget_tensors:     默认情况下，Keras将为模型的目标创建一个占位符，该占位符在训练过程中将被目标数据代替。如果你想使用自己的目标张量（相应的，Keras将不会在训练时期望为这些目标张量载入外部的numpy数据），你可以通过该参数手动指定。目标张量可以是一个单独的张量（对应于单输出模型），也可以是一个张量列表，或者一个name->tensor的张量字典。\n",
    "\tkwargs：            使用TensorFlow作为后端请忽略该参数，若使用Theano/CNTK作为后端，kwargs的值将会传递给 K.function。如果使用TensorFlow为后端，这里的值会被传给tf.Session.run\n",
    "\t在Keras中，compile主要完成损失函数和优化器的一些配置，是为训练服务的。\n",
    "    '''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "from tensorflow.compat.v1 import GPUOptions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "gpu_options = GPUOptions(per_process_gpu_memory_fraction=0.9)\n",
    "config = ConfigProto(gpu_options=gpu_options)\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#参数和路径\n",
    "resize_height, resize_width = (576, 576)\n",
    "dx = 48\n",
    "# img_path = 'DRIVE/training/images/'\n",
    "# label_path = 'DRIVE/training/1st_manual/'\n",
    "img_path = 'data/train/image/'\n",
    "label_path = 'data/train/mask/'\n",
    "\n",
    "\n",
    "#读取数据并resize\n",
    "imglst,images = read_image_and_name(img_path)\n",
    "labellst,labels = read_label_and_name(label_path)\n",
    "imgs_resize = resize(images,resize_height, resize_width)\n",
    "labels_resize = resize(labels,resize_height, resize_width)\n",
    "\n",
    "#将imgs列表和manuals列表转换成numpy数组\n",
    "X_train = np.array(imgs_resize)\n",
    "print('X_train.shape'+str(X_train.shape))\n",
    "Y_train = np.array(labels_resize)\n",
    "print('Y_train.shape'+str(Y_train.shape))\n",
    "\n",
    "#标准化\n",
    "X_train = X_train.astype('float32')/255\n",
    "Y_train = Y_train.astype('float32')/255\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #提取训练集的G通道\n",
    "# X_train = X_train[...,1]\n",
    "\n",
    "# #对训练数据进行裁剪\n",
    "# X_train = crop(X_train,dx)\n",
    "# Y_train = crop(Y_train,dx)\n",
    "# print('X_train shape: '+str(X_train.shape)) #X_train(2880,48,48)\n",
    "# print('Y_train shape: '+str(Y_train.shape)) #Y_train(2880,48,48)\n",
    "\n",
    "# #X_train增加一维变成(2880,1,48,48)\n",
    "# X_train = X_train[:,np.newaxis, ...]\n",
    "# print('X_train shape: '+str(X_train.shape))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "############图像增强\n",
    "X_train=rotate(X_train,angle=10,scale=1)\n",
    "print('X_train.shape:'+str(X_train.shape))\n",
    "Y_train=rotate(Y_train,angle=10,scale=1)\n",
    "print('Y_train.shape:'+str(Y_train.shape))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train=SaltAndPepper(X_train,0.1)\n",
    "print('X_train.shape:'+str(X_train.shape))\n",
    "Y_train=SaltAndPepper(Y_train,0)\n",
    "print('Y_train.shape:'+str(Y_train.shape))\n",
    "\n",
    "\n",
    "\n",
    "#将单通道转换为三通道\n",
    "X_train_0=X_train[...,0]\n",
    "print('X_train_0.shape:'+str(X_train_0.shape))\n",
    "X_train_1=X_train[...,1]\n",
    "print('X_train_1.shape:'+str(X_train_1.shape))\n",
    "X_train_2=X_train[...,2]\n",
    "print('X_train_2.shape:'+str(X_train_2.shape))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train_0 = crop(X_train_0,dx)\n",
    "print('X_train_0.shape:'+str(X_train_0.shape))\n",
    "X_train_1 = crop(X_train_1,dx)\n",
    "print('X_train_1.shape:'+str(X_train_1.shape))\n",
    "X_train_2 = crop(X_train_2,dx)\n",
    "print('X_train_2.shape:'+str(X_train_2.shape))\n",
    "Y_train = crop(Y_train,dx)\n",
    "print('Y_train.shape:'+str(Y_train.shape))#Y_train(2880,48,48)\n",
    "X_train_0 = X_train_0[:,np.newaxis, ...]\n",
    "print('X_train_0.shape:'+str(X_train_0.shape))\n",
    "X_train_1 = X_train_1[:,np.newaxis, ...]\n",
    "print('X_train_1.shape:'+str(X_train_1.shape))\n",
    "X_train_2 = X_train_2[:,np.newaxis, ...]\n",
    "print('X_train_2.shape:'+str(X_train_2.shape))\n",
    "\n",
    "X_train=np.concatenate((X_train_0,X_train_1,X_train_2),axis=1)\n",
    "print('X_train.shape:'+str(X_train.shape))\n",
    "\n",
    "\n",
    "\n",
    "#将Y_train改为单通道？？\n",
    "Y_train=Y_train[...,0]\n",
    "print('Y_train.shape:'+str(Y_train.shape))\n",
    "\n",
    "\n",
    "\n",
    "#Y_train改变shape变成(2880,2304),保持第一维不变，其他维合并\n",
    "Y_train = Y_train.reshape(Y_train.shape[0],-1)\n",
    "print('Y_train shape: '+str(Y_train.shape))\n",
    "Y_train =Y_train[..., np.newaxis]  #增加一维变成(2880,2304,1)\n",
    "print('Y_train shape: '+str(Y_train.shape))\n",
    "temp = 1 - Y_train\n",
    "Y_train = np.concatenate([Y_train, temp], axis=2) #变成(2880,2304,2)\n",
    "print('Y_train shape: '+str(Y_train.shape))\n",
    "\n",
    "#获得model\n",
    "model = get_unet(X_train.shape[1],X_train.shape[2],X_train.shape[3])\n",
    "model.summary() #输出参数Param计算过程\n",
    "checkpointer = ModelCheckpoint(filepath='best_weights.h5',verbose=1,monitor='val_accuracy',mode='auto',save_best_only=True)\n",
    "model.compile(optimizer=Adam(lr=0.001),loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#tensorboard\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "\n",
    "\n",
    "model.fit(X_train,Y_train,batch_size=14,epochs=20,verbose=1,shuffle=True,validation_split=0.2,callbacks=[checkpointer,tensorboard_callback])\n",
    "print('ok')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "resize_height, resize_width = (576, 576)\n",
    "dx = 48\n",
    "#读取预测图片\n",
    "# imgs = cv2.imread('data/test/images/01_test.tif')[...,1] #读取G通道\n",
    "imgs = cv2.imread('data/test/images/01_test.tif') #读取G通道\n",
    "imgs = np.array(cv2.resize(imgs,(resize_height,resize_width))) #imgs现在是576x576大小\n",
    "#读取预测图片的标签\n",
    "label = np.array(Image.open('data/test/mask/01_test.tif'))\n",
    "print(label.shape)\n",
    "\n",
    "\n",
    "label = np.array(cv2.resize(label,(resize_height, resize_width)))\n",
    "print(label.shape)\n",
    "\n",
    "\n",
    "\n",
    "#预测图片和标签标准化\n",
    "X_test = imgs.astype('float32')/255\n",
    "print('X_test original shape: '+str(X_test.shape))\n",
    "Y_test = label.astype('float32')/255\n",
    "\n",
    "\n",
    "#对预测图片进行裁剪按行优先，裁剪成(144,48,48)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# list = []\n",
    "# for i in range(resize_height//dx):\n",
    "#     for j in range(resize_width//dx):\n",
    "#         list.append(X_test[i*dx:(i+1)*dx, j*dx:(j+1)*dx])\n",
    "# X_test = np.array(list)[:,np.newaxis,...] #增加一维变成(144,1,48,48)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_test_0=X_test[...,0]\n",
    "X_test_1=X_test[...,1]\n",
    "X_test_2=X_test[...,2]\n",
    "list_0 = []\n",
    "for i in range(resize_height//dx):\n",
    "    for j in range(resize_width//dx):\n",
    "        list_0.append(X_test_0[i*dx:(i+1)*dx, j*dx:(j+1)*dx])\n",
    "X_test_0 = np.array(list_0)[:,np.newaxis,...] #增加一维变成(144,1,48,48)\n",
    "list_1 = []\n",
    "for i in range(resize_height//dx):\n",
    "    for j in range(resize_width//dx):\n",
    "        list_1.append(X_test_1[i*dx:(i+1)*dx, j*dx:(j+1)*dx])\n",
    "X_test_1 = np.array(list_1)[:,np.newaxis,...] #增加一维变成(144,1,48,48)\n",
    "list_2 = []\n",
    "for i in range(resize_height//dx):\n",
    "    for j in range(resize_width//dx):\n",
    "        list_2.append(X_test_2[i*dx:(i+1)*dx, j*dx:(j+1)*dx])\n",
    "X_test_2 = np.array(list_2)[:,np.newaxis,...] #增加一维变成(144,1,48,48)\n",
    "\n",
    "\n",
    "#X_test换channel位置\n",
    "X_test=np.concatenate((X_test_0,X_test_1,X_test_2),axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('input shape: '+str(X_test.shape))\n",
    "\n",
    "#加载模型和权重并预测\n",
    "model = get_unet(X_test.shape[1],X_test.shape[2],X_test.shape[3])\n",
    "\n",
    "\n",
    "model.load_weights('best_weights.h5')\n",
    "\n",
    "Y_pred = model.predict(X_test)\n",
    "print('predict shape: '+str(Y_pred.shape)) #预测结果的shape是(Npatches,patch_height*patch_width,2)\n",
    "\n",
    "#把预测输出的numpy数组拼接还原再显示\n",
    "Y_pred = Y_pred[..., 0]  #二分类提取出分割前景 现在Y_pred的shape是(144,2304) 且这个144是按照行优先来拼接的\n",
    "\n",
    "#对预测结果进行拼接，将(144,2304)拼接成(576,576)\n",
    "t=0\n",
    "image = np.zeros((resize_height,resize_width))\n",
    "for i in range(resize_height//dx):\n",
    "    for j in range(resize_width//dx):\n",
    "        temp = Y_pred[t].reshape(dx,dx)\n",
    "        image[i*dx:(i+1)*dx, j*dx:(j+1)*dx] = temp\n",
    "        t = t+1\n",
    "image = cv2.resize(image,((Y_test.shape[1], Y_test.shape[0]))) #将576x576大小的图像还原成原图像大小\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(image)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(Y_test)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice =dice_coefficient(image,Y_test)\n",
    "iou=dice/(2-dice)\n",
    "print(dice)\n",
    "print(iou)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.ndimage import morphology\n",
    "\n",
    "def surfd(input1, input2, sampling=1, connectivity=1):\n",
    "\n",
    "    input_1 = np.atleast_1d(input1.astype(np.bool))\n",
    "    input_2 = np.atleast_1d(input2.astype(np.bool))\n",
    "    conn = morphology.generate_binary_structure(input_1.ndim, connectivity)\n",
    "    S = (input_1.astype('uint8') - (morphology.binary_erosion(input_1, conn).astype('uint8'))).astype('bool')\n",
    "    Sprime = (input_2.astype('uint8') - (morphology.binary_erosion(input_2, conn))).astype('uint8')\n",
    "\n",
    "    # S = input_1 - morphology.binary_erosion(input_1, conn)\n",
    "    # Sprime = input_2 - morphology.binary_erosion(input_2, conn)\n",
    "\n",
    "    dta = morphology.distance_transform_edt(~S,sampling)\n",
    "    dtb = morphology.distance_transform_edt(~Sprime,sampling)\n",
    "\n",
    "    sds = np.concatenate([np.ravel(dta[Sprime!=0]), np.ravel(dtb[S!=0])])\n",
    "    return sds\n",
    "\n",
    "assd=np.mean(surfd(image,Y_test,sampling=1, connectivity=1))\n",
    "assd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a=np.array([[1,2],[3,4]])\n",
    "# b=np.array([[1,2],[3,4]])\n",
    "# np.dot(a,b)#矩阵乘法\n",
    "# a*b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
