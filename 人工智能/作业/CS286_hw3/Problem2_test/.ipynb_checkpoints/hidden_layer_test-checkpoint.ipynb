{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2. Reinforcement Learning for BipedalWalker-v3 (35 points)\n",
    "This problem is to design a reinforcement learning algorithm that is applied to a robot, with the objective to maximize its reward, in the following game. \n",
    "### BipedalWalker-v3\n",
    "The reward is given for moving forward, accumulating over 300 points at the end. If the robot falls, it will be penalized by deducting 100 points. Applying a motor torque costs a small amount of points. The state of the robot consists of hull angle speed, angular velocity, horizontal speed, vertical speed, positions of joints, angular speeds of joints, contact positions of legs with the ground, and 10 lidar rangefinder measurements. There is no coordinate in the state vector.  \n",
    "![Alt Text](https://media.giphy.com/media/R89toZzap04ZDKHPkd/giphy.gif)  \n",
    "This game has continuous action space. You are required to apply the Twin Delayed DDPG (TD3) method in this game.  \n",
    "### References：\n",
    "You can read [this link](https://spinningup.openai.com/en/latest/algorithms/td3.html) to understand the **TD3** algorithm better.  \n",
    "You can visit [this link](https://dllglobal.com/challenges/reinforcement-learning) to understand the **BipedalWalker-v3** environment better.  \n",
    "### Requirements：\n",
    "* All of your code should be shown in this file.\n",
    "* Your network must be based on GRU; otherwise, you will get 0 point.\n",
    "* You must save your trained model named as **best_model.pt**.\n",
    "* The RL method you need to implement is TD3; otherwise you will get 0 point.\n",
    "* Please give some comments to your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Import the packages and define helper funcitons and variables (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import tqdm.notebook as tn\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size=5e5):\n",
    "        self.buffer = []\n",
    "        self.max_size = int(max_size)\n",
    "        self.size = 0\n",
    "    \n",
    "    def add(self, transition):\n",
    "        self.size +=1\n",
    "        # transiton is tuple of (state, action, reward, next_state, done)\n",
    "        self.buffer.append(transition)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        # delete 1/5th of the buffer when full\n",
    "        if self.size > self.max_size:\n",
    "            del self.buffer[0:int(self.size/5)]\n",
    "            self.size = len(self.buffer)\n",
    "        \n",
    "        indexes = np.random.randint(0, len(self.buffer), size=batch_size)\n",
    "        state, action, reward, next_state, done, hidden_layer = [], [], [], [], [], []\n",
    "        \n",
    "        for i in indexes:\n",
    "            s, a, r, s_, d, h = self.buffer[i]\n",
    "            state.append(np.array(s, copy=False))\n",
    "            action.append(np.array(a, copy=False))\n",
    "            reward.append(np.array(r, copy=False))\n",
    "            next_state.append(np.array(s_, copy=False))\n",
    "            done.append(np.array(d, copy=False))\n",
    "            hidden_layer.append(np.array(h, copy=False))\n",
    "        \n",
    "        return np.array(state), np.array(action), np.array(reward), np.array(next_state), np.array(done), np.array(hidden_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Build your network (<font color=red>which should include GRU cells</font>) (7 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        # state_dim=24\n",
    "        self.GRU_layer = nn.GRU(input_size=state_dim, hidden_size=state_dim ,num_layers=1)\n",
    "        self.l4 = nn.Linear(state_dim, action_dim)\n",
    "        self.max_action = max_action\n",
    "\n",
    "    def forward(self, state, hidden_layer):\n",
    "        state=state.unsqueeze(0)\n",
    "        hidden_layer=hidden_layer.unsqueeze(0)\n",
    "        a , self.hidden = self.GRU_layer(state,hidden_layer)\n",
    "        a = a.squeeze(0)\n",
    "        a = self.l4(a)\n",
    "        a = torch.tanh(a) * self.max_action\n",
    "        return a , self.hidden\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        self.l1 = nn.Linear(state_dim + action_dim, 560)\n",
    "        self.l2 = nn.Linear(560, 280)\n",
    "        self.l3 = nn.Linear(280, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        state_action = torch.cat([state, action], 1)\n",
    "        q = F.relu(self.l1(state_action))\n",
    "        q = F.relu(self.l2(q))\n",
    "        q = self.l3(q)\n",
    "        return q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Define your TD3 algorithm (8 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     91,
     101,
     112
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "# from model import Actor, Critic\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class TD3:\n",
    "    def __init__(self, lr, state_dim, action_dim, max_action):\n",
    "        \n",
    "        self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
    "        self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr)\n",
    "        \n",
    "        self.critic_1 = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_1_target = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_1_target.load_state_dict(self.critic_1.state_dict())\n",
    "        self.critic_1_optimizer = optim.Adam(self.critic_1.parameters(), lr=lr)\n",
    "        \n",
    "        self.critic_2 = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_2_target = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_2_target.load_state_dict(self.critic_2.state_dict())\n",
    "        self.critic_2_optimizer = optim.Adam(self.critic_2.parameters(), lr=lr)\n",
    "        \n",
    "        self.max_action = max_action\n",
    "    \n",
    "    def select_action(self, state, hidden_layer):\n",
    "        state = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
    "        action = self.actor(state,hidden_layer)\n",
    "        return action[0].cpu().data.numpy().flatten() , action[1].cpu().data.numpy().flatten()\n",
    "    \n",
    "    def update(self, replay_buffer, n_iter, batch_size, gamma, polyak, policy_noise, noise_clip, policy_delay):\n",
    "        \n",
    "        for i in range(n_iter):\n",
    "            # Sample a batch of transitions from replay buffer:\n",
    "            state, action_, reward, next_state, done, hidden_layer = replay_buffer.sample(batch_size)\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            action = torch.FloatTensor(action_).to(device)\n",
    "            reward = torch.FloatTensor(reward).reshape((batch_size,1)).to(device)\n",
    "            next_state = torch.FloatTensor(next_state).to(device)\n",
    "            done = torch.FloatTensor(done).reshape((batch_size,1)).to(device)\n",
    "            hidden_layer = torch.FloatTensor(hidden_layer).to(device)\n",
    "            \n",
    "            # Select next action according to target policy:\n",
    "            noise = torch.FloatTensor(action_).data.normal_(0, policy_noise).to(device)\n",
    "            noise = noise.clamp(-noise_clip, noise_clip)\n",
    "            next_action = (self.actor_target(next_state,hidden_layer)[0] + noise)\n",
    "            next_action = next_action.clamp(-self.max_action, self.max_action)\n",
    "            \n",
    "            # Compute target Q-value:\n",
    "            target_Q1 = self.critic_1_target(next_state, next_action)\n",
    "            target_Q2 = self.critic_2_target(next_state, next_action)\n",
    "            target_Q = torch.min(target_Q1, target_Q2)\n",
    "            target_Q = reward + ((1-done) * gamma * target_Q).detach()\n",
    "            \n",
    "            # Optimize Critic 1:\n",
    "            current_Q1 = self.critic_1(state, action)\n",
    "            loss_Q1 = F.mse_loss(current_Q1, target_Q)\n",
    "            self.critic_1_optimizer.zero_grad()\n",
    "            loss_Q1.backward()\n",
    "            self.critic_1_optimizer.step()\n",
    "            \n",
    "            # Optimize Critic 2:\n",
    "            current_Q2 = self.critic_2(state, action)\n",
    "            loss_Q2 = F.mse_loss(current_Q2, target_Q)\n",
    "            self.critic_2_optimizer.zero_grad()\n",
    "            loss_Q2.backward()\n",
    "            self.critic_2_optimizer.step()\n",
    "            \n",
    "            # Delayed policy updates:\n",
    "            if i % policy_delay == 0:\n",
    "                # Compute actor loss:\n",
    "                actor_loss = -self.critic_1(state, self.actor(state,hidden_layer)[0]).mean()\n",
    "                \n",
    "                # Optimize the actor\n",
    "                self.actor_optimizer.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                self.actor_optimizer.step()\n",
    "                \n",
    "                # Polyak averaging update:\n",
    "                for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "                    target_param.data.copy_( (polyak * target_param.data) + ((1-polyak) * param.data))\n",
    "                \n",
    "                for param, target_param in zip(self.critic_1.parameters(), self.critic_1_target.parameters()):\n",
    "                    target_param.data.copy_( (polyak * target_param.data) + ((1-polyak) * param.data))\n",
    "                \n",
    "                for param, target_param in zip(self.critic_2.parameters(), self.critic_2_target.parameters()):\n",
    "                    target_param.data.copy_( (polyak * target_param.data) + ((1-polyak) * param.data))\n",
    "        return actor_loss.cpu().data.numpy(), loss_Q1.cpu().data.numpy(), loss_Q2.cpu().data.numpy()\n",
    "                \n",
    "    def save(self, directory, name, ep):\n",
    "        torch.save(self.actor.state_dict(), '%s/%s_actor_ep%s.pth' % (directory, name, ep))\n",
    "        torch.save(self.actor_target.state_dict(), '%s/%s_actor_target_ep%s.pth' % (directory, name, ep))\n",
    "        \n",
    "        torch.save(self.critic_1.state_dict(), '%s/%s_crtic_1_ep%s.pth' % (directory, name, ep))\n",
    "        torch.save(self.critic_1_target.state_dict(), '%s/%s_critic_1_target_ep%s.pth' % (directory, name, ep))\n",
    "        \n",
    "        torch.save(self.critic_2.state_dict(), '%s/%s_crtic_2_ep%s.pth' % (directory, name, ep))\n",
    "        torch.save(self.critic_2_target.state_dict(), '%s/%s_critic_2_target_ep%s.pth' % (directory, name, ep))\n",
    "        \n",
    "    def load(self, directory, name, ep):\n",
    "        self.actor.load_state_dict(torch.load('%s/%s_actor_ep%s.pth' % (directory, name, ep), map_location=lambda storage, loc: storage))\n",
    "        self.actor_target.load_state_dict(torch.load('%s/%s_actor_target_ep%s.pth' % (directory, name, ep), map_location=lambda storage, loc: storage))\n",
    "        \n",
    "        self.critic_1.load_state_dict(torch.load('%s/%s_crtic_1_ep%s.pth' % (directory, name, ep), map_location=lambda storage, loc: storage))\n",
    "        self.critic_1_target.load_state_dict(torch.load('%s/%s_critic_1_target_ep%s.pth' % (directory, name, ep), map_location=lambda storage, loc: storage))\n",
    "        \n",
    "        self.critic_2.load_state_dict(torch.load('%s/%s_crtic_2_ep%s.pth' % (directory, name, ep), map_location=lambda storage, loc: storage))\n",
    "        self.critic_2_target.load_state_dict(torch.load('%s/%s_critic_2_target_ep%s.pth' % (directory, name, ep), map_location=lambda storage, loc: storage))\n",
    "        \n",
    "        \n",
    "    def load_actor(self, directory, name, ep):\n",
    "        self.actor.load_state_dict(torch.load('%s/%s_actor_ep%s.pth' % (directory, name, ep), map_location=lambda storage, loc: storage))\n",
    "        self.actor_target.load_state_dict(torch.load('%s/%s_actor_target_ep%s.pth' % (directory, name, ep), map_location=lambda storage, loc: storage))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Define your training process and train your model (5 points)  \n",
    "You must use some data structures to collect the mean reward and mean loss in each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     37,
     44
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8df353dc337d4ff1b1b4729dca7871bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "l:\\anaconda3\\envs\\cs286\\lib\\site-packages\\ipykernel_launcher.py:70: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 298/10000,\tScore: -103.97,\tDistance: -103.97,\tactor_loss: 28.850149154663086,\tc1_loss:1.5559478998184204,\tc2_loss:1.5517163276672363358"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "######### Hyperparameters #########\n",
    "gym.logger.set_level(40)\n",
    "env_name = \"BipedalWalker-v3\"\n",
    "env = gym.make(env_name)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "max_action = float(env.action_space.high[0])\n",
    "\n",
    "log_interval = 1000  # print avg reward after interval\n",
    "random_seed = 0\n",
    "gamma = 0.99  # discount for future rewards\n",
    "batch_size = 100  # num of transitions sampled from replay buffer\n",
    "lr = 0.01 # 0.001\n",
    "exploration_noise = 0.1\n",
    "polyak = 0.995  # target policy update parameter (1-tau)\n",
    "policy_noise = 0.2  # target policy smoothing noise\n",
    "noise_clip = 0.5\n",
    "policy_delay = 2  # delayed policy updates parameter\n",
    "max_episodes = 10000  # max num of episodes\n",
    "max_timesteps = 1000  # max timesteps in one episode\n",
    "directory = \"./preTrained/\"  # save trained models\n",
    "filename = \"TD3_{}_{}\".format(env_name, random_seed)\n",
    "\n",
    "start_episode = 0\n",
    "\n",
    "\n",
    "policy = TD3(lr, state_dim, action_dim, max_action)\n",
    "replay_buffer = ReplayBuffer()\n",
    "\n",
    "if random_seed:\n",
    "    print(\"Random Seed: {}\".format(random_seed))\n",
    "    env.seed(random_seed)\n",
    "    torch.manual_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "LOAD = False\n",
    "if LOAD:\n",
    "    start_episode = 1000\n",
    "    policy.load(directory, filename, str(start_episode))\n",
    "\n",
    "# logging variables:\n",
    "scores = []\n",
    "mean_scores = []\n",
    "last_scores = deque(maxlen=log_interval)\n",
    "distances = []\n",
    "mean_distances = []\n",
    "last_distance = deque(maxlen=log_interval)\n",
    "losses_mean_episode = []\n",
    "hidden_layer=torch.tensor(env.reset()).to(device).float()\n",
    "\n",
    "# training procedure:\n",
    "for ep in tn.tqdm(range(start_episode + 1, max_episodes + 1)):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    total_distance = 0\n",
    "    actor_losses = []\n",
    "    c1_losses = []\n",
    "    c2_losses = []\n",
    "\n",
    "    for t in range(max_timesteps):\n",
    "        # select action and add exploration noise:\n",
    "        hidden_layer = torch.tensor(hidden_layer).to(device).float().unsqueeze(0)\n",
    "        action , hidden_layer = policy.select_action(state,hidden_layer)\n",
    "\n",
    "        action = action + np.random.normal(0, exploration_noise, size=env.action_space.shape[0])\n",
    "        action = action.clip(env.action_space.low, env.action_space.high)\n",
    "\n",
    "        # take action in env:\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "#         print('next_state',state.shape)\n",
    "#         print('hidden_layer',hidden_layer.shape)\n",
    "        replay_buffer.add((state, action, reward, next_state, float(done), hidden_layer))\n",
    "        state = next_state\n",
    "\n",
    "        total_reward += reward\n",
    "        if reward != -100:\n",
    "            total_distance += reward\n",
    "\n",
    "        # if episode is done then update policy:\n",
    "        if done or t == (max_timesteps - 1):\n",
    "            actor_loss, c1_loss, c2_loss = policy.update(replay_buffer, t, batch_size, gamma, polyak, policy_noise, noise_clip, policy_delay)\n",
    "            actor_losses.append(actor_loss)\n",
    "            c1_losses.append((c1_loss))\n",
    "            c2_losses.append(c2_loss)\n",
    "            break\n",
    "    mean_loss_actor = np.mean(actor_losses)\n",
    "    mean_loss_c1 = np.mean(c1_losses)\n",
    "    mean_loss_c2 = np.mean(c2_losses)\n",
    "    losses_mean_episode.append((ep, mean_loss_actor, mean_loss_c1, mean_loss_c2))\n",
    "    print('\\rEpisode: {}/{},\\tScore: {:.2f},\\tDistance: {:.2f},\\tactor_loss: {},\\tc1_loss:{},\\tc2_loss:{}'\n",
    "        .format(ep, max_episodes,total_reward,total_distance,mean_loss_actor,mean_loss_c1, mean_loss_c2),end=\"\")\n",
    "    \n",
    "    \n",
    "    # logging updates:\n",
    "    scores.append(total_reward)\n",
    "    distances.append(total_distance)\n",
    "    last_scores.append(total_reward)\n",
    "    last_distance.append(total_distance)\n",
    "    mean_score = np.mean(last_scores)\n",
    "    mean_distance = np.mean(last_distance)\n",
    "    FILE = 'record.dat'\n",
    "    data = [ep, total_reward, total_distance, mean_loss_actor, mean_loss_c1, mean_loss_c2]\n",
    "    with open(FILE, \"ab\") as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "\n",
    "    # if avg reward > 300 then save and stop traning:\n",
    "    # end = True\n",
    "    # for s in last_scores:\n",
    "    #     if s < 300:\n",
    "    #         end = False\n",
    "    #         break\n",
    "    # if end and len(last_scores) == 100:\n",
    "    #     print(\"########## Solved! ###########\")\n",
    "    #     name = filename + '_solved'\n",
    "    #     policy.save(directory, name, str(ep))\n",
    "    #     break\n",
    "\n",
    "    # print avg reward every log interval:\n",
    "    if ep % log_interval == 0:\n",
    "        policy.save(directory, filename, str(ep))\n",
    "        mean_scores.append(mean_score)\n",
    "        mean_distances.append(mean_distance)\n",
    "        print('\\rEpisode: {}/{},\\tMean Score: {:.2f},\\tMean Distance: {:.2f},\\tactor_loss: {},\\tc1_loss:{},\\tc2_loss:{}'\n",
    "            .format(ep, max_episodes, mean_score, mean_distance, mean_loss_actor, mean_loss_c1, mean_loss_c2))\n",
    "        FILE = 'record_mean.dat'\n",
    "        data = [ep, mean_score, mean_distance, mean_loss_actor, mean_loss_c1, mean_loss_c2]\n",
    "        with open(FILE, \"ab\") as f:\n",
    "            pickle.dump(data, f)\n",
    "        if mean_score >= 300 and len(last_scores) == 100:\n",
    "            print(\"########## Solved! ###########\")\n",
    "            name = filename + '_solved'\n",
    "            policy.save(directory, name, str(ep))\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 2.5 Show your change curves of reward and loss in two sub-pictures (5 points)\n",
    "Your reward and loss must be able to converge; otherwise this part will get 0 point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = []\n",
    "with open('./record.dat', 'rb') as fr:\n",
    "    try:\n",
    "        while True:\n",
    "            data.append(pickle.load(fr))\n",
    "    except EOFError:\n",
    "        pass\n",
    "data = pd.DataFrame(np.array(data))\n",
    "\n",
    "scores = []\n",
    "distances = []\n",
    "actor_losses = []\n",
    "critic1_losses = []\n",
    "critic2_losses = []\n",
    "\n",
    "\n",
    "for r in data[1]:\n",
    "    scores.append(r)\n",
    "\n",
    "for r in data[2]:\n",
    "    distances.append(r)\n",
    "\n",
    "for r in data[3]:\n",
    "    actor_losses.append(r)\n",
    "\n",
    "for r in data[4]:\n",
    "    critic1_losses.append(r)\n",
    "\n",
    "for r in data[4]:\n",
    "    critic2_losses.append(r)\n",
    "\n",
    "data_mean = []\n",
    "with open('./record_mean.dat', 'rb') as record_mean:\n",
    "    try:\n",
    "        while True:\n",
    "            data_mean.append(pickle.load(record_mean))\n",
    "    except EOFError:\n",
    "        pass\n",
    "data_mean = pd.DataFrame(np.array(data_mean))\n",
    "\n",
    "episode = []\n",
    "scores_mean = []\n",
    "distances_mean = []\n",
    "\n",
    "for r in data_mean[0]:\n",
    "    episode.append(r)\n",
    "\n",
    "for r in data_mean[1]:\n",
    "    scores_mean.append(r)\n",
    "\n",
    "for r in data_mean[2]:\n",
    "    distances_mean.append(r)\n",
    "\n",
    "\n",
    "# plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "x = np.arange(1, len(scores) + 1)\n",
    "y = scores\n",
    "plt.plot(x, y)\n",
    "plt.grid()\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.title('Scores on BipedalWalker-v3')\n",
    "plt.show()\n",
    "namefig = \"plot/scores_TD3_episodes\" + str(data.shape[0]) + \".jpg\"\n",
    "fig.savefig(namefig, dpi=300)\n",
    "\n",
    "# plot the distance and mean\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "x = np.arange(1, len(distances) + 1)\n",
    "y = distances\n",
    "plt.plot(x, y)\n",
    "plt.grid()\n",
    "plt.ylabel('Distance')\n",
    "plt.xlabel('Episode #')\n",
    "plt.title('Distances on BipedalWalker-v3')\n",
    "plt.show()\n",
    "namefig = \"plot/distances_TD3_episodes\" + str(data.shape[0]) + \".jpg\"\n",
    "fig.savefig(namefig, dpi=300)\n",
    "\n",
    "\n",
    "\n",
    "# plot the loss\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "x = np.arange(1, len(actor_losses) + 1)\n",
    "y = actor_losses\n",
    "plt.plot(x, y)\n",
    "plt.grid()\n",
    "plt.ylabel('Actor Loss')\n",
    "plt.xlabel('Episode #')\n",
    "plt.title('Actor Loss on BipedalWalker-v3')\n",
    "plt.show()\n",
    "namefig = \"plot/actor_loss_TD3_episodes\" + str(data.shape[0]) + \".jpg\"\n",
    "fig.savefig(namefig, dpi=300)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "x = np.arange(1, len(critic1_losses) + 1)\n",
    "y = critic1_losses\n",
    "plt.plot(x, y)\n",
    "plt.grid()\n",
    "plt.ylabel('Critic1 Loss')\n",
    "plt.xlabel('Episode #')\n",
    "plt.title('Critic Loss on BipedalWalker-v3')\n",
    "plt.show()\n",
    "namefig = \"plot/critic1_loss_TD3_episodes\" + str(data.shape[0]) + \".jpg\"\n",
    "fig.savefig(namefig, dpi=300)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "x = np.arange(1, len(critic2_losses) + 1)\n",
    "y = critic2_losses\n",
    "plt.plot(x, y)\n",
    "plt.grid()\n",
    "plt.ylabel('Critic2 Loss')\n",
    "plt.xlabel('Episode #')\n",
    "plt.title('Critic Loss on BipedalWalker-v3')\n",
    "plt.show()\n",
    "namefig = \"plot/critic2_loss_TD3_episodes\" + str(data.shape[0]) + \".jpg\"\n",
    "fig.savefig(namefig, dpi=300)\n",
    "\n",
    "\n",
    "\n",
    "# plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "x = episode\n",
    "y = scores_mean\n",
    "plt.plot(x, y)\n",
    "plt.grid()\n",
    "plt.ylabel('Mean Scores every 100 episodes')\n",
    "plt.xlabel('Episode #')\n",
    "plt.title('Mean Scores on BipedalWalker-v3')\n",
    "plt.show()\n",
    "namefig = \"plot/mean_scores_TD3_episodes\" + str(data.shape[0]) + \".jpg\"\n",
    "fig.savefig(namefig, dpi=300)\n",
    "\n",
    "# plot the distance and mean\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "x = episode\n",
    "y = distances_mean\n",
    "plt.plot(x, y)\n",
    "plt.grid()\n",
    "plt.ylabel('Mean Distances every 100 episodes')\n",
    "plt.xlabel('Episode #')\n",
    "plt.title('Mean Distances on BipedalWalker-v3')\n",
    "plt.show()\n",
    "namefig = \"plot/mean_distances_TD3_episodes\" + str(data.shape[0]) + \".jpg\"\n",
    "fig.savefig(namefig, dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Test your model 10 times and print the mean reward (5 points) \n",
    "You should load your trained model **best_model.pt**. If your mean reward is >= 200 but < 250, you will get 3 points. If your mean reward is >= 250, you will get 5 points. If your mean reward is < 200, you will get 0 point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "import gym\n",
    "from TD3 import TD3\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "gym.logger.set_level(40)\n",
    "env_name = \"BipedalWalker-v3\"\n",
    "random_seed = 0\n",
    "n_episodes = 100\n",
    "lr = 0.002\n",
    "max_timesteps = 2000\n",
    "render = False\n",
    "save_gif = False\n",
    "\n",
    "filename = \"TD3_{}_{}\".format(env_name, random_seed)\n",
    "filename += '_solved'\n",
    "directory = \"./preTrained/\".format(env_name)\n",
    "episode = 1073\n",
    "\n",
    "env = gym.make(env_name)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "max_action = float(env.action_space.high[0])\n",
    "\n",
    "policy = TD3(lr, state_dim, action_dim, max_action)\n",
    "\n",
    "policy.load_actor(directory, filename, episode)\n",
    "\n",
    "scores = []\n",
    "\n",
    "for ep in range(1, n_episodes+1):\n",
    "    ep_reward = 0\n",
    "    state = env.reset()\n",
    "    for t in range(max_timesteps):\n",
    "        action = policy.select_action(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        ep_reward += reward\n",
    "        if render:\n",
    "            env.render()\n",
    "            if save_gif:\n",
    "                 img = env.render(mode = 'rgb_array')\n",
    "                 img = Image.fromarray(img)\n",
    "                 img.save('./gif/{}.jpg'.format(t))\n",
    "        if done:\n",
    "            break\n",
    "    scores.append(ep_reward)\n",
    "    #print('Episode: {}\\tReward: {}'.format(ep, int(ep_reward)))\n",
    "    env.close()\n",
    "\n",
    "\n",
    "print(\"Score media\", np.mean(scores))\n",
    "    \n",
    "# plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "x = np.arange(1, len(scores) + 1)\n",
    "y = scores\n",
    "plt.plot(x, y)\n",
    "plt.grid()\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.title('Test Scores on BipedalWalker-v3')\n",
    "plt.show()\n",
    "namefig = \"plot/test_scores_TD3_episodes\" + str(episode) + \".jpg\"\n",
    "fig.savefig(namefig, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
